# Generated by Claude Code
"""Filter out low-quality questions and recalculate RAG evaluation metrics."""
import json
from pathlib import Path
import statistics

PROJECT_ROOT = Path(__file__).parent.parent
EVAL_DATA_DIR = PROJECT_ROOT / "data" / "eval"

# Questions to remove (definitely bad + moderate quality)
# Definitely bad: 4, 5, 14, 18, 21, 22, 24, 25, 35, 37, 39 (11 questions)
# Moderate: 15, 22, 24, 28 (4 questions, 2 duplicates)
# Unique indices to remove: 13
REMOVE_INDICES = [4, 5, 14, 15, 18, 21, 22, 24, 25, 28, 35, 37, 39]

print(f"Removing {len(REMOVE_INDICES)} low-quality questions")
print(f"Indices: {REMOVE_INDICES}\n")


def filter_results(mode: str):
    """Filter and recalculate metrics for a given mode."""
    print(f"{'='*80}")
    print(f"Processing mode: {mode.upper()}")
    print(f"{'='*80}")

    # Load original evaluation results
    with_ref_path = EVAL_DATA_DIR / f"eval_results_{mode}_with_reference.json"
    no_ref_path = EVAL_DATA_DIR / f"eval_results_{mode}_no_reference.json"

    with open(with_ref_path, 'r') as f:
        results_with_ref = json.load(f)

    with open(no_ref_path, 'r') as f:
        results_no_ref = json.load(f)

    # Results are stored as dict with string indices as keys
    # Convert to list format for easier manipulation
    def dict_to_list(results_dict):
        """Convert dict with string indices to list of dicts."""
        if not results_dict:
            return []
        # Get first key to determine structure
        first_key = list(results_dict.keys())[0]
        if isinstance(results_dict[first_key], dict):
            # Format: {"metric1": {"0": val, "1": val}, "metric2": {...}}
            num_rows = len(results_dict[first_key])
            return [{k: results_dict[k][str(i)] for k in results_dict.keys()}
                    for i in range(num_rows)]
        else:
            # Already in list format
            return results_dict

    results_with_ref_list = dict_to_list(results_with_ref)
    results_no_ref_list = dict_to_list(results_no_ref)

    print(f"Original with_reference rows: {len(results_with_ref_list)}")
    print(f"Original no_reference rows: {len(results_no_ref_list)}")

    # Filter out bad questions (only for ragas testset, first 41 questions)
    # For with_reference: only ragas questions (41 total)
    results_with_ref_filtered = [results_with_ref_list[i] for i in range(41) if i not in REMOVE_INDICES]

    # For no_reference: first 10 are manual (keep all), next 41 are ragas (filter)
    if mode != "none":
        # With retrieval: 10 manual + 41 ragas = 51 total
        results_no_ref_filtered = (
            results_no_ref_list[:10] +  # Keep all manual questions
            [results_no_ref_list[10 + i] for i in range(41) if i not in REMOVE_INDICES]  # Filter ragas
        )
    else:
        # No retrieval: only 51 questions total
        results_no_ref_filtered = (
            results_no_ref_list[:10] +
            [results_no_ref_list[10 + i] for i in range(41) if i not in REMOVE_INDICES]
        )

    print(f"Filtered with_reference rows: {len(results_with_ref_filtered)}")
    print(f"Filtered no_reference rows: {len(results_no_ref_filtered)}")

    # Recalculate metrics (mean of each metric across filtered results)
    def calculate_means(results_list):
        if not results_list:
            return {}
        # Get all metric keys
        all_keys = set()
        for result in results_list:
            all_keys.update(result.keys())

        metrics = {}
        for key in all_keys:
            values = [r[key] for r in results_list if key in r and r[key] is not None
                     and isinstance(r[key], (int, float))]  # Only numeric values
            if values:
                metrics[key] = statistics.mean(values)
        return metrics

    metrics_with_ref = calculate_means(results_with_ref_filtered)
    metrics_no_ref = calculate_means(results_no_ref_filtered)

    print(f"\nFiltered metrics (with reference):")
    for metric, value in metrics_with_ref.items():
        print(f"  {metric}: {value:.4f}")

    print(f"\nFiltered metrics (no reference):")
    for metric, value in metrics_no_ref.items():
        print(f"  {metric}: {value:.4f}")

    # Save filtered results
    filtered_with_ref_path = EVAL_DATA_DIR / f"eval_results_{mode}_with_reference_filtered.json"
    filtered_no_ref_path = EVAL_DATA_DIR / f"eval_results_{mode}_no_reference_filtered.json"

    with open(filtered_with_ref_path, 'w') as f:
        json.dump(results_with_ref_filtered, f, indent=2)

    with open(filtered_no_ref_path, 'w') as f:
        json.dump(results_no_ref_filtered, f, indent=2)

    print(f"\nSaved filtered results to:")
    print(f"  {filtered_with_ref_path}")
    print(f"  {filtered_no_ref_path}\n")

    return metrics_with_ref, metrics_no_ref


def main():
    """Process all modes and create summary table."""
    modes = ["hybrid", "dense", "sparse", "none"]

    all_metrics = {}

    for mode in modes:
        metrics_with_ref, metrics_no_ref = filter_results(mode)
        all_metrics[mode] = {**metrics_with_ref, **metrics_no_ref}

    # Create summary table
    print("\n" + "="*80)
    print("SUMMARY: Filtered Metrics (28 clean questions from 41 ragas)")
    print("="*80)

    metric_order = ["answer_correctness", "context_precision", "context_recall",
                    "answer_relevancy", "faithfulness"]

    # Print header
    print(f"\n{'Mode':<10}", end="")
    for metric in metric_order:
        print(f"{metric:<20}", end="")
    print()
    print("-" * 110)

    # Print rows
    for mode in modes:
        print(f"{mode.upper():<10}", end="")
        for metric in metric_order:
            value = all_metrics[mode].get(metric, None)
            if value is not None:
                print(f"{value:<20.4f}", end="")
            else:
                print(f"{'N/A':<20}", end="")
        print()

    # Save summary as JSON
    summary_path = EVAL_DATA_DIR / "filtered_metrics_summary.json"
    with open(summary_path, 'w') as f:
        json.dump(all_metrics, f, indent=2)
    print(f"\nSummary saved to: {summary_path}")

    # Print comparison table
    print("\n" + "="*80)
    print("COMPARISON: Original vs Filtered (answer_correctness improvement)")
    print("="*80)

    original_metrics = {
        "HYBRID": {"answer_correctness": 0.5694, "context_precision": 0.7316, "context_recall": 0.7907, "answer_relevancy": 0.8272, "faithfulness": 0.7677},
        "DENSE": {"answer_correctness": 0.4648, "context_precision": 0.5932, "context_recall": 0.6240, "answer_relevancy": 0.6972, "faithfulness": 0.7144},
        "SPARSE": {"answer_correctness": 0.5826, "context_precision": 0.6984, "context_recall": 0.7907, "answer_relevancy": 0.8147, "faithfulness": 0.7634},
        "NONE": {"answer_correctness": 0.3888, "answer_relevancy": 0.8833},
    }

    for mode in ["HYBRID", "DENSE", "SPARSE", "NONE"]:
        print(f"\n{mode}:")
        for metric in ["answer_correctness", "context_precision", "context_recall", "answer_relevancy", "faithfulness"]:
            if metric in original_metrics[mode] and metric in all_metrics[mode.lower()]:
                orig = original_metrics[mode][metric]
                filt = all_metrics[mode.lower()][metric]
                diff = filt - orig
                print(f"  {metric:20s}: {orig:.4f} → {filt:.4f} ({diff:+.4f})")
            elif metric in all_metrics[mode.lower()]:
                filt = all_metrics[mode.lower()][metric]
                print(f"  {metric:20s}: N/A    → {filt:.4f}")


if __name__ == "__main__":
    main()
