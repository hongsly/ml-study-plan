# Generated by Claude Code
"""Analyze ragas_testset.jsonl for low-quality questions."""
import json
import re
from pathlib import Path

RAGAS_TESTSET_PATH = Path(__file__).parent.parent / "data" / "eval" / "ragas_testset.jsonl"


def is_reference_section(text: str) -> bool:
    """Check if text is primarily from a reference/citation section."""
    # Look for citation patterns
    citation_patterns = [
        r'\d{4}[a-z]?\.',  # Year pattern: "2019."
        r'et al\.',  # "et al."
        r'In _[^_]+_\.',  # Venue pattern: "In _EMNLP_."
        r'_[A-Z][^_]+_,',  # Journal pattern: "_ArXiv_,"
    ]

    # Count citation patterns
    citation_count = sum(len(re.findall(pattern, text)) for pattern in citation_patterns)

    # Heuristic: >3 citations per 100 words suggests reference section
    word_count = len(text.split())
    if word_count < 50:
        return False

    citation_density = citation_count / (word_count / 100)
    return citation_density > 3


def check_context_quality(contexts: list[str]) -> dict:
    """Analyze context quality."""
    issues = []

    for i, ctx in enumerate(contexts):
        if is_reference_section(ctx):
            issues.append(f"Context {i}: Appears to be from reference section")

        if len(ctx.strip()) < 100:
            issues.append(f"Context {i}: Very short ({len(ctx)} chars)")

        # Check if context is mostly formatting/whitespace
        if len(ctx.strip()) < len(ctx) * 0.5:
            issues.append(f"Context {i}: Excessive whitespace")

    return {"issues": issues, "has_issues": len(issues) > 0}


def check_question_quality(question: str) -> dict:
    """Analyze question quality."""
    issues = []

    # Check for typos/misspellings (basic heuristic)
    if re.search(r"[a-z]{2,}'[a-z]", question):  # e.g., "Wha's"
        issues.append("Contains unusual contractions/typos")

    # Check for very short questions
    if len(question.split()) < 5:
        issues.append("Very short question")

    # Check for incomplete questions
    if not question.strip().endswith('?'):
        issues.append("Doesn't end with question mark")

    return {"issues": issues, "has_issues": len(issues) > 0}


def check_reference_quality(reference: str, contexts: list[str]) -> dict:
    """Check if reference answer is supported by contexts."""
    issues = []

    # Check if reference is too short
    if len(reference.strip()) < 20:
        issues.append("Very short reference answer")

    # Check if reference contains specific claims
    # (We can't fully validate without LLM, but can flag suspicious patterns)

    return {"issues": issues, "has_issues": len(issues) > 0}


def analyze_testset():
    """Analyze all questions in testset."""
    with open(RAGAS_TESTSET_PATH, 'r') as f:
        questions = [json.loads(line) for line in f]

    print(f"Analyzing {len(questions)} questions...\n")

    suspicious_questions = []

    for idx, item in enumerate(questions):
        question = item.get("user_input", "")
        reference = item.get("reference", "")
        contexts = item.get("reference_contexts", [])

        # Analyze each component
        context_analysis = check_context_quality(contexts)
        question_analysis = check_question_quality(question)
        reference_analysis = check_reference_quality(reference, contexts)

        # Collect all issues
        all_issues = []
        if context_analysis["has_issues"]:
            all_issues.extend(context_analysis["issues"])
        if question_analysis["has_issues"]:
            all_issues.extend(question_analysis["issues"])
        if reference_analysis["has_issues"]:
            all_issues.extend(reference_analysis["issues"])

        if all_issues:
            suspicious_questions.append({
                "index": idx,
                "question": question,
                "reference": reference,
                "issues": all_issues,
                "query_style": item.get("query_style", ""),
                "query_length": item.get("query_length", ""),
            })

    # Print results
    print(f"Found {len(suspicious_questions)} suspicious questions:\n")
    print("=" * 100)

    for item in suspicious_questions:
        print(f"\n[Question {item['index']}] ({item['query_style']}, {item['query_length']})")
        print(f"Q: {item['question']}")
        print(f"Ref: {item['reference'][:150]}..." if len(item['reference']) > 150 else f"Ref: {item['reference']}")
        print(f"Issues:")
        for issue in item['issues']:
            print(f"  - {issue}")
        print("-" * 100)

    print(f"\n\nSummary:")
    print(f"Total questions: {len(questions)}")
    print(f"Suspicious questions: {len(suspicious_questions)} ({len(suspicious_questions)/len(questions)*100:.1f}%)")
    print(f"Clean questions: {len(questions) - len(suspicious_questions)} ({(len(questions)-len(suspicious_questions))/len(questions)*100:.1f}%)")

    # Save suspicious question indices
    output_path = RAGAS_TESTSET_PATH.parent / "suspicious_question_indices.json"
    with open(output_path, 'w') as f:
        json.dump([item['index'] for item in suspicious_questions], f, indent=2)
    print(f"\nSuspicious question indices saved to: {output_path}")


if __name__ == "__main__":
    analyze_testset()
