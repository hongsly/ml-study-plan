# RAG Q&A System - Project Plan

**Created**: 2025-11-22 (Day 26, Week 4 Day 5)
**Timeline**: Option B - Full Project 4 (~12 hours over 2 weeks)
**Data Source**: ArXiv Papers (RAG/LLM domain)

---

## Problem Statement

Build a production-quality RAG system for question-answering over recent ArXiv papers on RAG and LLM techniques. Demonstrate:
- Hybrid retrieval (dense + sparse + RRF fusion)
- Automated evaluation with Ragas framework
- Docker deployment to cloud
- Complete senior MLE portfolio piece

---

## Architecture Overview

```
ArXiv Papers (PDF) â†’ Chunking (500 tokens, 50 overlap)
                           â†“
              Sentence-BERT Embeddings (384-dim)
                           â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â†“             â†“
              FAISS Index      BM25 Index
             (Dense retrieval) (Sparse retrieval)
                    â†“             â†“
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                  RRF Fusion (k=60)
               Score = Î£ 1/(k + rank_i)
                           â†“
                    Top-K Documents
                           â†“
              GPT-3.5-turbo + Context
                           â†“
                  Generated Answer + Citations
```

---

## Tech Stack Decisions

### Core Components
- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2)
  - Why: Free, fast, 384-dim works well, proven for RAG
  - Alternative considered: OpenAI embeddings (too expensive for portfolio)

- **Dense Retrieval**: FAISS (local index)
  - Why: Fast, battle-tested, good for <100K docs
  - Alternative: Chroma (heavier dependency)

- **Sparse Retrieval**: rank-bm25 library
  - Why: Pure Python, no server needed, sufficient for portfolio
  - Alternative: Elasticsearch (overkill for 20-30 papers)

- **Fusion**: Hand-coded RRF
  - Why: Simple (5 lines), demonstrates understanding
  - Formula: Score = Î£ 1/(60 + rank_i)

- **LLM**: OpenAI API (gpt-3.5-turbo)
  - Why: Reliable, fast, cheap ($0.50 for 5K queries)
  - Alternative: Ollama (slower, local hassle for portfolio)

- **Evaluation**: Ragas + manual metrics
  - Ragas: Context precision, recall, faithfulness, answer relevance
  - Manual: Recall@K, MRR, NDCG for retrieval

- **Deployment**: Docker + Streamlit Cloud
  - Why: Free hosting, easy to share, professional
  - Alternative: AWS Lambda (more complex)

### Development Tools
- **Version Control**: Git + GitHub
- **Environment**: Python 3.10+ with venv
- **CI/CD**: GitHub Actions (linting + tests)
- **Monitoring**: Simple logging to file

---

## Data Source: ArXiv Papers

### Target Papers (20-30 papers on RAG/LLMs)

**Search queries on arxiv.org:**
1. "Retrieval Augmented Generation" (2023-2024)
2. "RAG evaluation" OR "RAG metrics"
3. "Hybrid retrieval" OR "dense sparse retrieval"
4. "Query rewriting" OR "Query decomposition"
5. "LLM hallucination" OR "Faithfulness"

**Recommended papers to download** (pick 20-30):
- FiD (Fusion-in-Decoder) - Izacard et al.
- Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020)
- Lost in the Middle (Liu et al., 2023)
- RAFT (Gorilla paper, 2024)
- ColBERT: Efficient and Effective Passage Search (Khattab & Zaharia, 2020)
- Ragas: Automated Evaluation of RAG (2023)
- Self-RAG (Asai et al., 2023)
- RAPTOR: Recursive Abstractive Processing (2024)
- GraphRAG (Microsoft, 2024)
- Recent survey papers on RAG (2024)
- Papers on query rewriting/decomposition
- Papers on reranking strategies
- Papers on long-context vs. RAG

**Download strategy**:
- Use arxiv.org search + filter by date (2023-2024)
- Download PDFs to `data/raw/`
- Total size: ~50-100 MB (acceptable)

---

## Implementation Timeline

### Weekend (Light Sessions)

**Day 26 (Sat, Nov 23) - 30 min** âœ…
- [x] Create project structure
- [x] Write this project-plan.md
- [x] Decision: Option B confirmed

**Day 27 (Sun, Nov 24) - 30 min**
- [ ] Download 20-30 ArXiv papers (PDFs to `data/raw/`)
- [ ] Create folder structure:
  ```
  rag-qa-system/
  â”œâ”€â”€ data/
  â”‚   â”œâ”€â”€ raw/              # PDFs
  â”‚   â”œâ”€â”€ processed/        # Chunks (JSON)
  â”‚   â””â”€â”€ eval/             # Test questions
  â”œâ”€â”€ src/
  â”œâ”€â”€ evaluation/
  â”œâ”€â”€ tests/
  â””â”€â”€ outputs/
  ```
- [ ] Create `requirements.txt` stub (list libraries, don't install yet)

### Week 5 (Main Implementation - 12 hours)

**Day 1 (Mon, Nov 25) - 2 hours**
- [ ] `src/data_loader.py`: Parse PDFs, chunk by 500 tokens with 50 overlap
- [ ] `src/embeddings.py`: Load sentence-transformers, generate embeddings
- [ ] Build FAISS index, save to disk
- [ ] Test: Search for 1 query, verify top-5 results
- [ ] Commit: "Add data loading and embedding generation"

**Day 2 (Tue, Nov 26) - 2 hours**
- [ ] `src/retriever.py`: Implement hybrid retrieval
  - [ ] FAISS dense search (top-20)
  - [ ] BM25 sparse search (top-20)
  - [ ] RRF fusion: Score = Î£ 1/(60 + rank_i)
  - [ ] Return top-K docs with scores
- [ ] Test: Compare dense-only vs. BM25-only vs. hybrid+RRF
- [ ] Commit: "Add hybrid retrieval with RRF fusion"

**Day 3 (Wed, Nov 27) - 2 hours**
- [ ] `src/generator.py`: OpenAI API wrapper
- [ ] `src/rag_pipeline.py`: End-to-end pipeline
- [ ] Create test question set (10 questions in `data/eval/test_questions.json`)
  - 3 simple factual
  - 3 complex reasoning
  - 2 multi-hop
  - 2 negative (not in corpus)
- [ ] Run end-to-end test on all 10 questions
- [ ] Commit: "Add generation and end-to-end pipeline"

**Day 4 (Thu, Nov 28) - 3 hours**
- [ ] `evaluation/evaluate_retrieval.py`: Calculate Recall@K, MRR, NDCG
- [ ] `evaluation/evaluate_rag.py`: Ragas integration
  - Context precision, context recall, faithfulness, answer relevance
- [ ] `evaluation/error_analysis.py`: Categorize failures
- [ ] `evaluation/cost_analysis.py`: Track API costs
- [ ] Run full evaluation, generate reports
- [ ] Commit: "Add Ragas evaluation and error analysis"

**Day 5 (Fri, Nov 29) - 3 hours**
- [ ] `Dockerfile`: Containerize application
- [ ] `docker-compose.yml`: Multi-service setup (optional)
- [ ] `app.py`: Streamlit UI with citations
- [ ] `.github/workflows/ci.yml`: Basic CI (linting, tests)
- [ ] Deploy to Streamlit Cloud or Hugging Face Spaces
- [ ] Write comprehensive `README.md`:
  - Problem, architecture, tech stack
  - Results (retrieval metrics, Ragas scores)
  - How to run locally and in Docker
  - Future improvements
- [ ] Push to GitHub
- [ ] Commit: "Add deployment and documentation"

---

## Code Structure (Full Project 4)

```
rag-qa-system/
â”œâ”€â”€ README.md                      # Comprehensive documentation
â”œâ”€â”€ requirements.txt              # All dependencies with versions
â”œâ”€â”€ Dockerfile                    # Container definition
â”œâ”€â”€ docker-compose.yml            # Optional: multi-service
â”œâ”€â”€ .env.example                  # API keys template
â”œâ”€â”€ .gitignore                    # Don't commit data, .env
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                # GitHub Actions (lint + test)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # 20-30 ArXiv PDFs
â”‚   â”œâ”€â”€ processed/                # Chunked docs (JSON lines)
â”‚   â””â”€â”€ eval/                     # Test question sets
â”‚       â””â”€â”€ test_questions.json   # 10 test questions with ground truth
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py            # PDF parsing, chunking
â”‚   â”œâ”€â”€ embeddings.py             # Sentence-BERT wrapper
â”‚   â”œâ”€â”€ vector_store.py           # FAISS operations
â”‚   â”œâ”€â”€ retriever.py              # Dense + BM25 + RRF fusion
â”‚   â”œâ”€â”€ generator.py              # OpenAI API wrapper
â”‚   â”œâ”€â”€ rag_pipeline.py           # End-to-end pipeline
â”‚   â””â”€â”€ api.py                    # FastAPI endpoint (optional)
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluate_retrieval.py    # Recall@K, MRR, NDCG
â”‚   â”œâ”€â”€ evaluate_rag.py           # Ragas integration
â”‚   â”œâ”€â”€ error_analysis.py         # Failure categorization
â”‚   â””â”€â”€ cost_analysis.py          # API cost tracking
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_pipeline.py          # Unit tests
â”‚   â””â”€â”€ test_api.py               # API tests
â”œâ”€â”€ app.py                        # Streamlit UI
â”œâ”€â”€ notebooks/                    # Optional
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_embedding_comparison.ipynb
â”‚   â””â”€â”€ 03_retrieval_tuning.ipynb
â””â”€â”€ outputs/
    â”œâ”€â”€ eval_results/             # Evaluation metrics and reports
    â”‚   â”œâ”€â”€ retrieval_metrics.json
    â”‚   â”œâ”€â”€ ragas_scores.json
    â”‚   â””â”€â”€ error_analysis.json
    â””â”€â”€ logs/                     # Query logs and monitoring
```

**Estimated lines of code**: ~800 lines (excluding notebooks)

---

## Requirements (Libraries to Install)

```txt
# Core dependencies
sentence-transformers==2.2.2
faiss-cpu==1.7.4
rank-bm25==0.2.2
openai==1.3.0
python-dotenv==1.0.0

# PDF parsing
PyPDF2==3.0.1
pdfplumber==0.10.3

# Evaluation
ragas==0.1.0
datasets==2.14.0

# API and UI
fastapi==0.104.1
uvicorn==0.24.0
streamlit==1.28.0

# Development
pytest==7.4.3
black==23.11.0
flake8==6.1.0

# Utilities
pandas==2.1.3
numpy==1.26.2
tqdm==4.66.1
```

---

## Evaluation Plan

### Test Question Set (10 questions)

**Simple Factual (3 questions)**:
1. "What is Retrieval-Augmented Generation?"
2. "Who proposed the FiD architecture?"
3. "What does RAFT stand for?"

**Complex Reasoning (3 questions)**:
4. "Why does hybrid retrieval (dense + sparse) outperform either approach alone?"
5. "How does ColBERT differ from traditional dense retrieval?"
6. "What are the trade-offs between long-context LLMs and RAG systems?"

**Multi-hop (2 questions)**:
7. "How do GraphRAG and FiD differ in their approach to multi-document reasoning?"
8. "What evaluation metrics are recommended for both retrieval and generation in RAG?"

**Negative (2 questions)**:
9. "What is the capital of France?" (not in corpus)
10. "How do you train a neural network?" (not in corpus)

### Metrics to Track

**Retrieval Metrics** (compare dense, BM25, hybrid):
- Recall@K (K=1,3,5,10): % of questions with correct doc in top-K
- MRR (Mean Reciprocal Rank): 1/rank of first correct doc
- NDCG: Normalized Discounted Cumulative Gain
- Precision@K: % of relevant docs in top-K

**Ragas Metrics** (automated LLM-as-judge):
- Context Precision: Are retrieved contexts relevant to question?
- Context Recall: Does retrieved context contain answer?
- Faithfulness: Is answer grounded in context (no hallucination)?
- Answer Relevance: Does answer address the question?
- Answer Correctness: Semantic similarity with ground truth

**Cost Metrics**:
- Total API calls (embeddings + generation + evaluation)
- Tokens used per query
- Cost per query, cost per 1K queries

### Expected Results

**Retrieval** (based on 99.2% RAG mastery):
- Dense-only: Recall@5 â‰ˆ 70-80%
- BM25-only: Recall@5 â‰ˆ 60-70%
- Hybrid+RRF: Recall@5 â‰ˆ 85-95% â­ (best)

**Ragas Scores** (target):
- Context Precision: >0.85
- Context Recall: >0.90
- Faithfulness: >0.90
- Answer Relevance: >0.85

---

## Interview Talking Points

After building this, you can say:

**"I built a hybrid RAG system with automated evaluation for Q&A over recent research papers."**

**Architecture**:
- Problem: Answer questions over 20-30 ArXiv papers on RAG/LLMs with accurate citations
- Retrieval: Dense (sentence-BERT in FAISS) + sparse (BM25) + RRF fusion
- Key insight: Hybrid retrieval captures both semantic similarity (dense) and exact keyword matches (sparse)
- RRF formula: Score = Î£ 1/(k + rank_i) with k=60 - principled fusion without learned weights
- Generation: GPT-3.5-turbo with retrieved context + prompt engineering for citations

**Evaluation rigor**:
- Automated: Ragas framework (LLM-as-judge) for context precision, faithfulness, answer relevance
- Manual: Recall@K, MRR, NDCG for retrieval quality
- Error analysis: Categorized failures (retrieval miss, poor generation, hallucination)
- Result: Hybrid+RRF achieved 90% Recall@5 (10-15% better than dense-only)

**Production readiness**:
- Docker containerization for reproducibility
- Deployed to Streamlit Cloud for demo
- CI/CD with GitHub Actions (linting, unit tests)
- Cost analysis: ~$0.02 per query (mostly generation, not retrieval)

**What I'd improve next**:
1. Add reranking with cross-encoder (retrieve 20 â†’ rerank â†’ top 5)
2. Multi-hop query decomposition for complex questions
3. Implement caching for repeated queries (reduce cost)
4. A/B test different embedding models (compare to OpenAI embeddings)

---

## Success Criteria

**Technical**:
- âœ… End-to-end RAG pipeline working
- âœ… Hybrid retrieval (dense + BM25 + RRF) implemented correctly
- âœ… Ragas evaluation framework integrated
- âœ… Recall@5 â‰¥ 85% on test questions
- âœ… Faithfulness score â‰¥ 0.90 (no hallucinations)
- âœ… Dockerized and deployed to cloud
- âœ… Clean GitHub repo with comprehensive README

**Portfolio**:
- âœ… Demonstrates senior MLE skills (evaluation rigor, production deployment)
- âœ… Shows RAG mastery (99.2% from Week 4 studies)
- âœ… Interview-ready: Can explain architecture, trade-offs, evaluation in 5 min
- âœ… Meets all updated Project 4 requirements from Project-Ideas.md

**Timeline**:
- âœ… Weekend: Documentation + data prep (~1 hour total)
- âœ… Week 5: Implementation + evaluation + deployment (~12 hours)
- âœ… End of Week 5: Complete portfolio piece on GitHub

---

## Notes

- This is Option B: Full Project 4 with all mandatory components
- Focus on quality over features - better to have working eval than broken reranking
- Document everything for interview storytelling
- If Week 5 runs long, cut optional notebooks - code + README are essential
- Cost estimate: ~$5-10 for OpenAI API during development (acceptable for portfolio)

---

**Next Steps**:
- **Tomorrow (Day 27)**: Download ArXiv papers, create folder structure (30 min)
- **Week 5 Day 1**: Start implementation with data loading (2 hours)

**Status**: Ready to begin! ğŸš€
