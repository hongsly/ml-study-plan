# RAG Q&A System - Project Plan

**Created**: 2025-11-22 (Day 26, Week 4 Day 5)
**Timeline**: Option B - Full Project 4 (~12 hours over 2 weeks)
**Data Source**: ArXiv Papers (RAG/LLM domain)

---

## Problem Statement

Build a production-quality RAG system for question-answering over recent ArXiv papers on RAG and LLM techniques. Demonstrate:
- Hybrid retrieval (dense + sparse + RRF fusion)
- Automated evaluation with Ragas framework
- Docker deployment to cloud
- Complete senior MLE portfolio piece

---

## Architecture Overview

```
ArXiv Papers (PDF) â†’ Chunking (500 tokens, 50 overlap)
                           â†“
              Sentence-BERT Embeddings (384-dim)
                           â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â†“             â†“
              FAISS Index      BM25 Index
             (Dense retrieval) (Sparse retrieval)
                    â†“             â†“
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                  RRF Fusion (k=60)
               Score = Î£ 1/(k + rank_i)
                           â†“
                    Top-K Documents
                           â†“
              GPT-3.5-turbo + Context
                           â†“
                  Generated Answer + Citations
```

---

## Tech Stack Decisions

### Core Components
- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2)
  - Why: Free, fast, 384-dim works well, proven for RAG
  - Alternative considered: OpenAI embeddings (too expensive for portfolio)

- **Dense Retrieval**: FAISS (local index)
  - Why: Fast, battle-tested, good for <100K docs
  - Alternative: Chroma (heavier dependency)

- **Sparse Retrieval**: rank-bm25 library
  - Why: Pure Python, no server needed, sufficient for portfolio
  - Alternative: Elasticsearch (overkill for 20-30 papers)

- **Fusion**: Hand-coded RRF
  - Why: Simple (5 lines), demonstrates understanding
  - Formula: Score = Î£ 1/(60 + rank_i)

- **LLM**: OpenAI API (gpt-3.5-turbo)
  - Why: Reliable, fast, cheap ($0.50 for 5K queries)
  - Alternative: Ollama (slower, local hassle for portfolio)

- **Evaluation**: Ragas + manual metrics
  - Ragas: Context precision, recall, faithfulness, answer relevance
  - Manual: Recall@K, MRR, NDCG for retrieval

- **Deployment**: Docker + Streamlit Cloud
  - Why: Free hosting, easy to share, professional
  - Alternative: AWS Lambda (more complex)

### Development Tools
- **Version Control**: Git + GitHub
- **Environment**: Python 3.10+ with venv
- **CI/CD**: GitHub Actions (linting + tests)
- **Monitoring**: Simple logging to file

---

## Data Source: ArXiv Papers

### Target Papers (20-30 papers on RAG/LLMs)

**Search queries on arxiv.org:**
1. "Retrieval Augmented Generation" (2023-2024)
2. "RAG evaluation" OR "RAG metrics"
3. "Hybrid retrieval" OR "dense sparse retrieval"
4. "Query rewriting" OR "Query decomposition"
5. "LLM hallucination" OR "Faithfulness"

**Recommended papers to download** (pick 20-30):
- FiD (Fusion-in-Decoder) - Izacard et al.
- Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020)
- Lost in the Middle (Liu et al., 2023)
- RAFT (Gorilla paper, 2024)
- ColBERT: Efficient and Effective Passage Search (Khattab & Zaharia, 2020)
- Ragas: Automated Evaluation of RAG (2023)
- Self-RAG (Asai et al., 2023)
- RAPTOR: Recursive Abstractive Processing (2024)
- GraphRAG (Microsoft, 2024)
- Recent survey papers on RAG (2024)
- Papers on query rewriting/decomposition
- Papers on reranking strategies
- Papers on long-context vs. RAG

**Download strategy**:
- Use arxiv.org search + filter by date (2023-2024)
- Download PDFs to `data/raw/`
- Total size: ~50-100 MB (acceptable)

---

## Implementation Timeline

### Weekend (Light Sessions)

**Day 26 (Sat, Nov 23) - 30 min** âœ…
- [x] Create project structure
- [x] Write this project-plan.md
- [x] Decision: Option B confirmed

**Day 27 (Sun, Nov 24) - 30 min**
- [x] Download 20-30 ArXiv papers (PDFs to `data/raw/`)
- [x] Create folder structure:
  ```
  rag-qa-system/
  â”œâ”€â”€ data/
  â”‚   â”œâ”€â”€ raw/              # PDFs
  â”‚   â”œâ”€â”€ processed/        # Chunks (JSON)
  â”‚   â””â”€â”€ eval/             # Test questions
  â”œâ”€â”€ src/
  â”œâ”€â”€ evaluation/
  â”œâ”€â”€ tests/
  â””â”€â”€ outputs/
  ```
- [x] Create `requirements.txt` stub (list libraries, don't install yet)

### Week 5 (Main Implementation - 12 hours)

**Day 1 (Mon, Nov 24, Week 4 Day 7) - 2 hours**
- [x] `src/data_loader.py`: Parse PDFs, chunk by 500 tokens with 50 overlap
- [x] `src/vector_store.py`: Generate embeddings, build FAISS index, save to disk
- [x] Test: Search for 1 query, verify top-5 results
- [x] Commit: "Add data loading and embedding generation"

**Day 2 (Mon, Nov 25, Week 5 Day 1 / Day 29) - 2.5 hours** âœ… **COMPLETE**
- [x] `src/sparse_retrieval.py`: BM25 with NLTK tokenization
- [x] `src/hybrid_search.py`: RRF fusion (k=60, retrieve 4Ã—k candidates)
- [x] Evaluation framework: Precision@5, MRR
- [x] Test: Compare dense vs BM25 vs hybrid on 2 query sets
  - General NLP queries: Hybrid 40% < Dense 60% (query-corpus mismatch)
  - RAG-focused queries: Hybrid 80% > Dense 67% âœ… (aligned queries)
- [x] Key finding: Query-corpus alignment is critical for BM25 performance
- [x] Folder reorganization: Created `experiments/` for analysis scripts
- [x] Commit: "Add hybrid retrieval with RRF fusion"

**Decision**: Use **Hybrid (RRF)** for production - performs better (80% vs 67%) with RAG-focused queries
**Rationale**: User queries will be RAG-related (e.g., "How does ColBERT work?"), not general NLP
**See**: `references/day29-hybrid-retrieval-findings.md` for details

**Day 3 (Wed, Nov 26, Week 5 Day 2) - 2 hours** âœ… **COMPLETE**
- [x] `src/generator.py`: OpenAI Responses API wrapper (gpt-4o-mini, prompt engineering)
- [x] `src/rag_pipeline.py`: End-to-end pipeline (RagAssistant with 4 modes)
- [x] Create test question set (10 questions in `data/eval/test_questions.json`)
  - 3 simple factual âœ…
  - 3 complex reasoning âœ…
  - 2 multi-hop âœ…
  - 2 negative (not in corpus) âœ…
- [x] Smoke test: 5 questions Ã— 4 modes = 20 tests
  - Results: Citations excellent, token usage validated (2700 vs 50)
  - Issue discovered: Negative question handling (retrieval contamination)
- [x] Commit: "Add generation and end-to-end pipeline"

**Key Decision**: Used gpt-4o-mini ($0.15 input / $0.60 output) instead of gpt-3.5-turbo - 3Ã— cheaper + better quality

**Day 4 (Thu, Nov 27, Week 5 Day 3) - 1 hour** âœ… **PLANNING & COST ANALYSIS**
- [x] Add ArXiv metadata to chunks (title, authors, year, URL) - 30 min
- [x] Regenerate chunks with metadata (`scripts/build_index.py`)
- [x] Researched Ragas 0.3.9 API (`generate_with_langchain_docs`, gpt-4o-mini setup)
- [x] Investigated Ollama support (not reliable - missing `agenerate_prompt`)
- [x] Ragas cost underestimate
  - Test run: 200 chunks â†’ 200K tokens â†’ $0.70 (SummaryExtractor phase only) (but was using gpt-4o instead of gpt-4o-mini)
- [x] Analyzed manual vs Ragas test format differences
- [x] Discussed ground truth requirements for metrics
- [x] **Daily knowledge check**: 94% (A) - Excellent overdue item retention
- Implementation deferred to Day 5 (evaluation code, run metrics)

**Key Decision**: Sample 250 representative chunks instead of all 1500 â†’ 8Ã— cost savings ($1.25 vs $10-15)

**Day 5 (Fri, Nov 28, Week 5 Day 4) - 2 hours** âœ… **RETRIEVAL EVALUATION**
- [x] Add reference filtering to `CorpusLoader.filter_reference_chunks()` (Ollama-based)
- [x] Rebuild index with filtered chunks (1395 remaining, 9.5% references removed)
- [x] Implement sampling: `_sample_chunks()` in generate_testset.py
- [x] Generate 42 Ragas questions with Ollama (free, exceeded target of 40)
- [x] Create `evaluation/evaluate_retrieval.py`: Recall@K, MRR, NDCG
- [x] Run retrieval evaluation on 41 questions (3 modes: sparse, dense, hybrid)
- [x] **Critical insight**: Sampled testset â†’ incomplete ground truth (metrics are lower bounds)
- [ ] RAG evaluation deferred to Day 6 (use LLM-based context_recall)
- [ ] Error analysis deferred to Day 6

**Total cost**: $0 (Ollama for filtering + generation)

**Day 6 (Sat, Nov 29, Week 5 Day 5) - 3 hours** - **RAG EVALUATION & DEPLOYMENT**
- [ ] Create `evaluation/evaluate_rag.py`: Ragas metrics (LLM-based, avoids incomplete ground truth issue)
  - Context Precision, Context Recall, Faithfulness, Answer Relevance
- [ ] Run RAG evaluation on 52 questions (10 manual + 42 Ragas)
- [ ] Error analysis: Categorize failure modes
- [ ] Decision: Use SPARSE only or keep HYBRID with documented findings
- [ ] `app.py`: Streamlit UI with citations (if time)
- [ ] Write comprehensive `README.md`:
  - Problem, architecture, tech stack
  - Results (retrieval metrics, Ragas scores, key finding: BM25 > Hybrid)
  - Evaluation limitations (sampled ground truth)
  - Future improvements (fine-tuned embeddings, learned fusion)
- [ ] Push to GitHub
- [ ] Commit: "Complete RAG evaluation and documentation"

---

## Code Structure (Full Project 4)

```
rag-qa-system/
â”œâ”€â”€ README.md                      # Comprehensive documentation
â”œâ”€â”€ requirements.txt              # All dependencies with versions
â”œâ”€â”€ Dockerfile                    # Container definition
â”œâ”€â”€ docker-compose.yml            # Optional: multi-service
â”œâ”€â”€ .env.example                  # API keys template
â”œâ”€â”€ .gitignore                    # Don't commit data, .env
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                # GitHub Actions (lint + test)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # 20-30 ArXiv PDFs
â”‚   â”œâ”€â”€ processed/                # Chunked docs (JSON lines)
â”‚   â””â”€â”€ eval/                     # Test question sets
â”‚       â””â”€â”€ test_questions.json   # 10 test questions with ground truth
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py            # PDF parsing, chunking
â”‚   â”œâ”€â”€ embeddings.py             # Sentence-BERT wrapper
â”‚   â”œâ”€â”€ vector_store.py           # FAISS operations
â”‚   â”œâ”€â”€ retriever.py              # Dense + BM25 + RRF fusion
â”‚   â”œâ”€â”€ generator.py              # OpenAI API wrapper
â”‚   â”œâ”€â”€ rag_pipeline.py           # End-to-end pipeline
â”‚   â””â”€â”€ api.py                    # FastAPI endpoint (optional)
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluate_retrieval.py    # Recall@K, MRR, NDCG
â”‚   â”œâ”€â”€ evaluate_rag.py           # Ragas integration
â”‚   â”œâ”€â”€ error_analysis.py         # Failure categorization
â”‚   â””â”€â”€ cost_analysis.py          # API cost tracking
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_pipeline.py          # Unit tests
â”‚   â””â”€â”€ test_api.py               # API tests
â”œâ”€â”€ app.py                        # Streamlit UI
â”œâ”€â”€ notebooks/                    # Optional
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_embedding_comparison.ipynb
â”‚   â””â”€â”€ 03_retrieval_tuning.ipynb
â””â”€â”€ outputs/
    â”œâ”€â”€ eval_results/             # Evaluation metrics and reports
    â”‚   â”œâ”€â”€ retrieval_metrics.json
    â”‚   â”œâ”€â”€ ragas_scores.json
    â”‚   â””â”€â”€ error_analysis.json
    â””â”€â”€ logs/                     # Query logs and monitoring
```

**Estimated lines of code**: ~800 lines (excluding notebooks)

---

## Requirements (Libraries to Install)

```txt
# Core dependencies
sentence-transformers==2.2.2
faiss-cpu==1.7.4
rank-bm25==0.2.2
openai==1.3.0
python-dotenv==1.0.0

# PDF parsing
PyPDF2==3.0.1
pdfplumber==0.10.3

# Evaluation
ragas==0.1.0
datasets==2.14.0

# API and UI
fastapi==0.104.1
uvicorn==0.24.0
streamlit==1.28.0

# Development
pytest==7.4.3
black==23.11.0
flake8==6.1.0

# Utilities
pandas==2.1.3
numpy==1.26.2
tqdm==4.66.1
```

---

## Evaluation Plan

### Test Question Set (10 questions)

**Simple Factual (3 questions)**:
1. "What is Retrieval-Augmented Generation?"
2. "Who proposed the FiD architecture?"
3. "What does RAFT stand for?"

**Complex Reasoning (3 questions)**:
4. "Why does hybrid retrieval (dense + sparse) outperform either approach alone?"
5. "How does ColBERT differ from traditional dense retrieval?"
6. "What are the trade-offs between long-context LLMs and RAG systems?"

**Multi-hop (2 questions)**:
7. "How do GraphRAG and FiD differ in their approach to multi-document reasoning?"
8. "What evaluation metrics are recommended for both retrieval and generation in RAG?"

**Negative (2 questions)**:
9. "What is the capital of France?" (not in corpus)
10. "How do you train a neural network?" (not in corpus)

### Metrics to Track

**Retrieval Metrics** (compare dense, BM25, hybrid):
- Recall@K (K=1,3,5,10): % of questions with correct doc in top-K
- MRR (Mean Reciprocal Rank): 1/rank of first correct doc
- NDCG: Normalized Discounted Cumulative Gain
- Precision@K: % of relevant docs in top-K

**Ragas Metrics** (automated LLM-as-judge):
- Context Precision: Are retrieved contexts relevant to question?
- Context Recall: Does retrieved context contain answer?
- Faithfulness: Is answer grounded in context (no hallucination)?
- Answer Relevance: Does answer address the question?
- Answer Correctness: Semantic similarity with ground truth

**Cost Metrics**:
- Total API calls (embeddings + generation + evaluation)
- Tokens used per query
- Cost per query, cost per 1K queries

### Expected Results

**Retrieval** (based on 99.2% RAG mastery):
- Dense-only: Recall@5 â‰ˆ 70-80%
- BM25-only: Recall@5 â‰ˆ 60-70%
- Hybrid+RRF: Recall@5 â‰ˆ 85-95% â­ (best)

**Ragas Scores** (target):
- Context Precision: >0.85
- Context Recall: >0.90
- Faithfulness: >0.90
- Answer Relevance: >0.85

---

## Interview Talking Points

After building this, you can say:

**"I built a hybrid RAG system with automated evaluation for Q&A over recent research papers."**

**Architecture**:
- Problem: Answer questions over 20-30 ArXiv papers on RAG/LLMs with accurate citations
- Retrieval: Dense (sentence-BERT in FAISS) + sparse (BM25) + RRF fusion
- Key insight: Hybrid retrieval captures both semantic similarity (dense) and exact keyword matches (sparse)
- RRF formula: Score = Î£ 1/(k + rank_i) with k=60 - principled fusion without learned weights
- Generation: GPT-3.5-turbo with retrieved context + prompt engineering for citations

**Evaluation rigor**:
- Automated: Ragas framework (LLM-as-judge) for context precision, faithfulness, answer relevance
- Manual: Recall@K, MRR, NDCG for retrieval quality
- Error analysis: Categorized failures (retrieval miss, poor generation, hallucination)
- Result: Hybrid+RRF achieved 90% Recall@5 (10-15% better than dense-only)

**Production readiness**:
- Docker containerization for reproducibility
- Deployed to Streamlit Cloud for demo
- CI/CD with GitHub Actions (linting, unit tests)
- Cost analysis: ~$0.02 per query (mostly generation, not retrieval)

**What I'd improve next**:
1. Add reranking with cross-encoder (retrieve 20 â†’ rerank â†’ top 5)
2. Multi-hop query decomposition for complex questions
3. Implement caching for repeated queries (reduce cost)
4. A/B test different embedding models (compare to OpenAI embeddings)

---

## Success Criteria

**Technical**:
- âœ… End-to-end RAG pipeline working
- âœ… Hybrid retrieval (dense + BM25 + RRF) implemented correctly
- âœ… Ragas evaluation framework integrated
- âœ… Recall@5 â‰¥ 85% on test questions
- âœ… Faithfulness score â‰¥ 0.90 (no hallucinations)
- âœ… Dockerized and deployed to cloud
- âœ… Clean GitHub repo with comprehensive README

**Portfolio**:
- âœ… Demonstrates senior MLE skills (evaluation rigor, production deployment)
- âœ… Shows RAG mastery (99.2% from Week 4 studies)
- âœ… Interview-ready: Can explain architecture, trade-offs, evaluation in 5 min
- âœ… Meets all updated Project 4 requirements from Project-Ideas.md

**Timeline**:
- âœ… Weekend: Documentation + data prep (~1 hour total)
- âœ… Week 5: Implementation + evaluation + deployment (~12 hours)
- âœ… End of Week 5: Complete portfolio piece on GitHub

---

## Notes

- This is Option B: Full Project 4 with all mandatory components
- Focus on quality over features - better to have working eval than broken reranking
- Document everything for interview storytelling
- If Week 5 runs long, cut optional notebooks - code + README are essential
- Cost estimate: ~$5-10 for OpenAI API during development (acceptable for portfolio)

---

**Next Steps**:
- **Tomorrow (Day 27)**: Download ArXiv papers, create folder structure (30 min)
- **Week 5 Day 1**: Start implementation with data loading (2 hours)

**Status**: Ready to begin! ğŸš€
