{"user_input":{"0":"Who are the authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering'?","1":"What is the margin value used in triplet loss?","2":"What are the potential reasons for lower performance on SQuAD?","3":"What is a corpus in the context of information retrieval?","4":"Wha's Yoav Goldberg's contributiin to informatio retrieval?","5":"What is the purpose of Antoine Bordes' work in machine learning?","6":"What is the main contribution of Facebook AI's research paper at ICLR 2021?","7":"What are the performance comparisons between BERT-wwm reader and ELECTRA reader on HotpotQA dev set?","8":"What is the purpose of HotpotQA in information retrieval?","9":"What is the birthday of the author of 'She Walks These Hills'?","10":"What is the role of downstream models in the retrieval process described?","11":"What conference paper introduced WebQuestions?","12":"What are structured state space models (SSMs) and how do they address the limitations of Transformers?","13":"What is the ability of Mamba to solve the induction heads task?","14":"What is the difference between ARES and other retrieval-augmented generation evaluation systems like Data ChatGPT LLaMA Expansion SFT?","15":"What is the purpose of Temporalwiki and how does it relate to the development of language models like LLaMA?","16":"What were the speed and memory benchmarks for the SSM scan operation in the context of the Mamba model?","17":"What is the role of FreshQA in evaluating time-sensitive information and how does it compare to HotPotQA and Musique?","18":"What is the role of dialogue generation in expanding and improving language models like LLaMA?","19":"What open-domain QA models use dense retrieval techniques and how do they compare to traditional IR methods?","20":"What is the relationship between Selective State Space Models and linear attention approximations like SSMs?","21":"What is the generate-and-retrieve method and how does it improve information retrieval for semantic parsing?","22":"What is the fine-tuning approach used in Temporalwiki?","23":"What were the speed and memory benchmarks for the Mamba model in comparison to other models?","24":"What is the process of Data ChatGPT LLaMA Expansion SFT and how does it contribute to the democratization of AI?","25":"What are some examples of downstream models that have been expanded and fine-tuned using the Data ChatGPT LLaMA Expansion SFT method?","26":"What is RAFT and how does it improve the performance of pre-trained Large Language Models on domain-specific RAG tasks?","27":"How does the Flare framework utilize recursive retrieval to enhance information retrieval?","28":"What challenges did Anil et al. face when developing their model and how did they overcome them?","29":"What is an example of a multi-hop reasoning task in MMLU high-school Physics?","30":"What are retrieval units in the context of open-domain QA?","31":"How do different contextual inputs affect the embeddings generated for documents in the TREC-Covid dataset?","32":"What method did Wu et al. propose to enhance the diversity and coverage of instruction-following knowledge extracted from teacher LLMs?","33":"What specific research approach did Xu et al. propose to improve the performance of LLMs in handling factual errors?","34":"What datasets did the researchers use to evaluate the performance of their models for Musique?","35":"What specific research did Anil et al. contribute to in the field of large language models?","36":"What is the qrel matrix used in Yang et al.'s work for instantiating attributes?","37":"What contributions did Chunting Zhou make to the field of information retrieval?","38":"How does MiniCPM-V 2.6 compare to VisRAG-Ret in terms of retrieval efficiency and performance on text retrieval benchmarks?","39":"What is the significance of LLaMA in the field of advanced models for information retrieval and understanding?","40":"How does the use of DeBERTa-v3-Large in evaluating RAG systems contribute to their accuracy?"},"retrieved_contexts":{"0":["## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice ["," shown effective in passage\n\n- r dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates\nthe feasibility of full dense retrieval in IR tasks.\nInstead of employing the dual-encoder framework,\nthey introduced a late-interaction operator on top\n\n- f the BERT encoders.\n\n\nDense retrieval for open-domain QA has been\nexplored by Das et al. (2019), who propose to retrieve relevant passages iteratively using reformulated question vectors. As an alternative approach\nthat skips passage retrieval, Seo et al. (2019) propose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input questions efficiently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al. (2019) jointly train\nthe question encoder and reader. Their approach\n\n- utperforms the BM25 plus reader paradigm on\nmultiple open-domain QA datasets in QA accuracy,\nand is further extended by REALM (Guu et al.,\n2020), which includes tuning the passage encoder\nasynchronously by re-indexing the passages during training. The pretraining objective has also\nrecently been improved by Xiong et al. (2020b).\nIn contrast, our model provides a simple and yet\n\n\n\neffective solution that shows stronger empirical performance, without relying on additional pretraining\n\n- r complex joint training schemes.\nDPR has also been used as an important module in very recent work. For instance, extending\nthe idea of leveraging hard negatives, Xiong et al.\n(2020a) use the retrieval model trained in the previous iteration to discover new negatives and construct a different set of examples in each training\niteration. Starting from our trained DPR model,\nthey show that the retrieval performance can be\nfurther improved. Recent work (Izacard and Grave,\n2020; Lewis et al., 2020b) have also shown that\nDPR can be combined with generation models\nsuch as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2019), achieving good performance on\n\n- pen-domain QA and other knowledge-intensive\ntasks.\n\n\n**8** **Conclusion**\n\n\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the","QA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20\nparagraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these\ndatasets into an open-domain setting, we make two\ncorpora, one for each dataset, by combining all\nsupporting and non-supporting paragraphs for all\nits questions in the train, development, and test\nsets. IIRC is originally a mix between reading\ncomprehension and an open-domain setting. Each\nquestion is grounded in one main paragraph, which\ncontains links to multiple Wikipedia pages with\nseveral paragraphs each. We create a corpus out\n\n- f all the paragraphs from all the Wikipedia pages\npresent in the dataset. [18] We do assume the availability of the main passage which doesn\u2019t need\nto be retrieved and is always present. We don\u2019t\nassume the availability of Wikipedia links in the\nmain passage, however, to keep the retrieval problem challenging. [19]\n\n\n**B** **Special Handling of Models for IIRC**\n\n\nIIRC is slightly different from the other datasets,\nin that the question is grounded in the main passage and other supporting paragraphs come from\nthe Wikipedia pages of entities mentioned in this\npassage. We modify the retrievers and readers to\naccount for this difference: (i) We always keep the\nmain passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all\nthe retrieval methods, we first prompt the model to\ngenerate a list of Wikipedia page titles using the\nmain passage and the question. We map these generated titles to the nearest Wikipedia page titles in\nthe corpus (found using BM25), and then the rest\n\n- f the paragraph retrieval queries are scoped within\n\n- nly those Wikipedia pages.\nTo prompt the model to generate Wikipedia page\ntitles using the main passage and the question for\n\n\n18Following are the corpus sizes for the datasets: HotpotQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e",":2002.08909 [cs.CL]](https:\/\/arxiv.org\/abs\/2002.08909)\n\n[11] Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, and\nAllan Hanbury. 2020. Improving Efficient Neural Ranking Models with Cross[Architecture Knowledge Distillation. arXiv:2010.02666 [cs.IR]](https:\/\/arxiv.org\/abs\/2010.02666)\n\n[12] Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen tau Yih. 2020. Dense Passage Retrieval for Open[Domain Question Answering. arXiv:2004.04906 [cs.CL]](https:\/\/arxiv.org\/abs\/2004.04906)\n\n[13] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\nSearch via Contextualized Late Interaction over BERT. In _Proceedings of the 43rd_\n_International ACM SIGIR Conference on Research and Development in Information_\n_Retrieval_ (Virtual Event, China) _(SIGIR \u201920)_ . Association for Computing Machinery,\n[New York, NY, USA, 39\u201348. https:\/\/doi.org\/10.1145\/3397271.3401075](https:\/\/doi.org\/10.1145\/3397271.3401075)\n\n[14] Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. 2015. Numba: A llvm-based\npython jit compiler. In _Proceedings of the Second Workshop on the LLVM Compiler_\n_Infrastructure in HPC_ . 1\u20136.\n\n[15] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Repre[sentations for Ranking using Tightly-Coupled Teachers. arXiv:2010.11386 [cs.IR]](https:\/\/arxiv.org\/abs\/2010.11386)\n\n[16] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\nGoharian, and Ophir Frieder. 2020. Expansion via Prediction"],"1":[" top scoring passage from BM25 is irrelevant, even though\nkeywords such as _England_ and _Ireland_ appear multiple times. In comparison, DPR is able to return\n\n\n\n**Top-1** **Top-5** **Top-20** **Top-100**\n\n\nGold 44.9 66.8 78.1 85.0\n\nDist. Sup. 43.9 65.3 77.1 84.4\n\n\nTable 5: Retrieval accuracy on the development set of\nNatural Questions, trained on passages that match the\ngold context (Gold) or the top BM25 passage that contains the answer (Dist. Sup.).\n\n\n**Sim Loss** **Retrieval Accuracy**\nTop-1 Top-5 Top-20 Top-100\n\n\nNLL **44.9** **66.8** **78.1** **85.0**\nDP\nTriplet 41.6 65.0 77.2 84.5\n\n\nNLL 43.5 64.7 76.1 83.1\nL2\nTriplet 42.2 66.0 **78.1** 84.9\n\n\nTable 6: Retrieval Top- _k_ accuracy on the development\nset of Natural Questions using different similarity and\nloss functions.\n\n\nthe correct answer, presumably by matching _\u201cbody_\n\n_of water\u201d_ with semantic neighbors such as _sea_ and\n_channel_, even though no lexical overlap exists. The\nsecond example is one where BM25 does better.\nThe salient phrase _\u201cThoros of Myr\u201d_ is critical, and\nDPR is unable to capture it.\n\n\n**D** **Joint Training of Retriever and**\n**Reader**\n\n\nWe fix the passage encoder in our joint-training\nscheme while allowing only the question encoder\nto receive backpropagation signal from the combined (retriever + reader) loss function. This allows\nus to leverage the HNSW-based FAISS index for\nefficient low-latency retrieving, without reindexing\nthe passages during model updates. Our loss function largely follows ORQA\u2019s approach, which uses\nlog probabilities of positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically","A** **Distant Supervision**\n\n\nWhen training our final DPR model using Natural\nQuestions, we use the passages in our collection\nthat best match the gold context as the positive\npassages. As some QA datasets contain only the\nquestion and answer pairs, it is thus interesting\nto see when using the passages that contain the\nanswers as positives (i.e., the distant supervision\nsetting), whether there is a significant performance\ndegradation. Using the question and answer together as the query, we run Lucene-BM25 and pick\nthe top passage that contains the answer as the positive passage. Table 5 shows the performance of\nDPR when trained using the original setting and\nthe distant supervision setting.\n\n\n**B** **Alternative Similarity Functions &**\n**Triplet Loss**\n\n\nIn addition to dot product (DP) and negative loglikelihood based on softmax (NLL), we also experiment with Euclidean distance (L2) and the triplet\nloss. We negate L2 similarity scores before applying softmax and change signs of question-topositive and question-to-negative similarities when\napplying the triplet loss on dot product scores. The\nmargin value of the triplet loss is set to 1. Table 6 summarizes the results. All these additional\n\nexperiments are conducted using the same hyperparameters tuned for the baseline (DP, NLL).\nNote that the retrieval accuracy for our \u201cbaseline\u201d\nsettings reported in Table 5 (Gold) and Table 6\n(DP, NLL) is slightly better than those reported in\nTable 3. This is due to a better hyper-parameter\nsetting used in these analysis experiments, which\nis documented in our code release.\n\n\n**C** **Qualitative Analysis**\n\n\nAlthough DPR performs better than BM25 in general, the retrieved passages of these two retrievers\nactually differ qualitatively. Methods like BM25\nare sensitive to highly selective keywords and\nphrases, but cannot capture lexical variations or semantic relationships well. In contrast, DPR excels\nat semantic representation, but might lack sufficient\ncapacity to represent salient phrases which appear\nrarely. Table 7 illustrates this phenomenon with\ntwo examples. In the first example, the top scoring passage from BM25 is irrelevant, even though\nkeywords such as _England_ and _Ireland_ appear multiple times. In comparison, DPR is able to return\n\n\n\n**Top-1** **Top-5** **Top-20","_ [) =]\n\n\n\n_\u221e_ (3)\n\n- _\u03bb_ _[t][\u2032]_ _\u03b4t_ + _t\u2032,_\n\n_t_ _[\u2032]_ =0\n\n\n\nwhere _\u03bb_ is the bias-variance trade-off parameter.\nThe reward function _R_ reflects the quality of the\ngenerated queries, which needs to be consistent\nwith the final evaluation of the task. _x_ \u02dc [\u02c6] is fed to the\nretriever and the reader for a final prediction \u02c6 _y_ . A\npart of the reward function is the measures of \u02c6 _y_\ncompared to the golden label _y_ (e.g., exact match\nand F1 of the predicted answers), denoted as _Rlm_ .\nBesides, a KL-divergence regularization is added\nto prevent the model from deviating too far from\nthe initialization (Ramamurthy et al., 2022; Ziegler\net al., 2019).\n\n\n\u02dc\n_R_ ( _st, at_ ) = _Rlm_ ( _x, y_ [\u02c6] ) _\u2212_ _\u03b2_ KL ( _\u03c0\u03b8\u2225\u03c0_ 0) _._ (4)\n\n\nThe final loss function is composed of policy loss\nand value loss.\n\n\n\n1\n_L\u03b8_ = _\u2212_\n_|S| T_\n\n\n\n\n\n_\u03c4_ _\u2208S_\n\n\n\n_T_\n\n- min( _kt,\u03b8A_ _[\u03b8][\u2032]_ _,_ clip _A_ _[\u03b8][\u2032]_ ) _,_\n\n\n_t_ =0\n\n\n\n1\n_L\u03d5_ =\n_|S| T_\n\n\n\n\n\n_\u03c4_ _\u2208S_\n\n\n\n_T_\n\n- ( _V\u03d5_ ( _st_ ) _\u2212_ _Rt_ ) [2] _,_\n\n\n_t_ =0\n\n\n\n_Lppo_ = _L\u03b8_ + _\u03bbvL\u03d5._\n(5)\n\n\nHere, _S_ denotes the sampled set, and _T_ is for step\nnumbers.\n\n\n**4** **Implementation**\n\n\n**Rewriter** For the frozen pipeline in \u00a73.1, we\nprompt an LLM to rewrite the query with few-shot\n\n\n**Direct prompt**\n\n\nAnswer the question in the following format, end the answer with \u2019**\u2019. {demonstration} Question: { _","N: number of negative\nexamples, IB: in-batch training. G.+BM25 [(1)] and\nG.+BM25 [(2)] denote in-batch training with 1 or 2 additional BM25 negatives, which serve as negative passages for all questions in the batch.\n\n\nOur experiments on Natural Questions show that\nswitching to distantly-supervised passages (using\nthe highest-ranked BM25 passage that contains the\nanswer), has only a small impact: 1 point lower\ntop- _k_ accuracy for retrieval. Appendix A contains\nmore details.\n\n\n**Similarity and loss** Besides dot product, cosine\nand Euclidean L2 distance are also commonly used\nas decomposable similarity functions. We test these\nalternatives and find that L2 performs comparable to dot product, and both of them are superior\nto cosine. Similarly, in addition to negative loglikelihood, a popular option for ranking is triplet\nloss, which compares a positive passage and a negative one directly with respect to a question (Burges\net al., 2005). Our experiments show that using\ntriplet loss does not affect the results much. More\ndetails can be found in Appendix B.\n\n\n**Cross-dataset generalization** One interesting\nquestion regarding DPR\u2019s discriminative training\nis how much performance degradation it may suffer from a non-iid setting. In other words, can\nit still generalize well when directly applied to\na different dataset without additional fine-tuning?\nTo test the cross-dataset generalization, we train\nDPR on Natural Questions only and test it directly\n\n- n the smaller WebQuestions and CuratedTREC\ndatasets. We find that DPR generalizes well, with\n3-5 points loss from the best performing fine-tuned\nmodel in top-20 retrieval accuracy (69.9\/86.3 vs.\n75.0\/89.1 for WebQuestions and TREC, respectively), while still greatly outperforming the BM25\nbaseline (55.0\/70.9).\n\n\n**5.3** **Qualitative Analysis**\n\n\nAlthough DPR performs better than BM25 in general, passages retrieved by these two methods differ qualitatively. Term-matching methods like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason"," 1 when we jointly train with the identity loss _\u03bbY_ = 1, and\nchoose a larger value _\u03bbX_ = 10 without the identity loss ( _\u03bbY_ = 0) to compensate\nfor the absence of the regularizer. We find that the former configuration, named\n_Contrastive Unpaired Translation (CUT)_ hereafter, achieves superior performance\nto existing methods, whereas the latter, named _FastCUT_, can be thought as a\nfaster and lighter version of CycleGAN. Our model is relatively simple compared\nto recent methods that often use 5-10 losses and hyper-parameters.\n\n\n**Discussion.** Li et al. [42] has shown that cycle-consistency loss is the upper\nbound of conditional entropy H( _X|Y_ ) (and H( _Y |X_ )). Therefore, minimizing\ncycle-consistency loss encourages the output \u02c6 _**y**_ to be more dependent on input _**x**_ .\nThis is related to our objective of maximizing the mutual information I( _X, Y_ ), as\nI( _X, Y_ ) = H( _X_ ) _\u2212_ H( _X|Y_ ). As entropy H( _X_ ) is a constant and independent of\nthe generator _G_, maximizing mutual information is equivalent to minimizing the\nconditional entropy. Notably, using contrastive learning, we can achieve a similar\ngoal without introducing inverse mapping networks and additional discriminators.\nIn the unconditional modeling scenario, InfoGAN [7] shows that simple losses\n(e.g., L2 or cross-entropy) can serve as a lower bound for maximizing mutual\ninformation between an image and a low-dimensional code. In our setting, we\nmaximize the mutual information between two high-dimensional image spaces,\nwhere simple losses are no longer effective. Liang et al. [43] proposes an adversarial\nloss based on Siamese networks that encourages the output to be closer to the\ntarget domain than to its source domain. The above method still builds on\ncycle-consistency and two-way translations. Different from the above work, we\nuse contrastive learning to enforce content consistency, rather than to improve\nthe adversarial loss itself. To measure the similarity between two distributions,\nthe Contextual Loss [52] used softmax over cosine disntances of features extracted\nfrom pre-trained networks. In contrast"],"2":[" BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25( _q_, _p_ ) + _\u03bb \u00b7_ sim( _q, p_ ) as the ranking function.\nWe used _\u03bb_ = 1 _._ 1 based on the retrieval accuracy in\nthe development set.\n\n\n**5.1** **Main Results**\n\n\nTable 2 compares different passage retrieval systems on five QA datasets, using the top- _k_ accuracy\n( _k \u2208{_ 20 _,_ 100 _}_ ). With the exception of SQuAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when _k_ is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\n\n- n Natural Questions). When training with mul\n\n8SQuAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\n\nmore in Section 5.1.\n\n9\n[Lucene implementation. BM25 parameters](https:\/\/lucene.apache.org\/) _b_ = 0 _._ 4 (document length normalization) and _k_ 1 = 0 _._ 9 (term frequency\nscaling) are tuned using development sets.\n\n\n\nFigure 1: Retriever top- _k_ accuracy with different numbers of training examples used in our dense passage retriever vs BM25. The results are measured on the de\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n\n\ntiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and",".05)  - 90.1  - 95.0 [\u2021]\n\nHybrid (\u03b1 = 0.7)  - 89.6  - 94.6 [\u2021]\n\n\nSQuAD\n\nDPR 51.6 52.0 67.6 67.7\nBM25 68.8 71.1 [\u2020] 80.0 81.8 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 66.2  - 78.6  Hybridnorm (\u03b1 = 2.00)  - 75.1 [\u2021]  - 84.4 [\u2021]\n\nHybrid (\u03b1 = 28)  - 75.0 [\u2021]  - 84.0 [\u2021]\n\n\nTable 1: Retrieval effectiveness comparing results from\nthe original DPR paper (\u201corig\u201d) and our replication attempt (\u201crepl\u201d). The symbol [\u2020] - n a BM25 result indicates effectiveness that is significantly different from\nDPR. The symbol [\u2021] indicates that the hybrid technique\nis significantly better than BM25 (for SQuAD) or DPR\n(for all remaining collections).\n\n\nthat the effectiveness differences between DPR\n\nand BM25 in our replication results are statistically significant. In all datasets except for SQuAD,\nDPR outperforms BM25; this is consistent with\nthe original paper. We further confirmed that for\nSQuAD, DPR is significantly worse than BM25.\nAs Karpukhin et al. (2020) noted, RetrieverMulti\nwas trained by combining training data from all\ndatasets but excluding SQuAD; these poor results\nare expected, since SQuAD draws from a very\nsmall set of Wikipedia articles.\nThird, the effectiveness of hybrid dense\u2013sparse\nfusion appears to be understated in the original\nDPR paper. Karpukhin et al. (2020) found that\n\n\nCondition k = 20 100 500 1000\n\n\nNQ 6.1 5.2 4.4 4.2\nTriviaQA 9.2 6.6 5.0 4.6\nWQ 5.9 5.9 5.8 5.7\n\nCuratedTrec 6.9 7.2 6.3 5.9\n\n","ING on challenging Multi-Hop reasoning benchmark MuSiQue\n(Trivedi et al., 2022) and StrategyQA (Geva et al., 2021). We follow the same protocol as Section 5\nto implement STEP-BACK PROMPTING.\n\n\nTable 3 shows performance of various baselines on the dev set of MuSiQue and StrategyQA.\nBaseline performance of PaLM-2L and GPT-4 are low (35 _._ 5% and 38 _._ 5% for PaLM-2L and GPT-4\nrespectively) in MuSiQue since it is a hard multihop reasoning benchmark. In contrast, StrategyQA\nhas stronger baselines (82 _._ 8% and 78 _._ 3% for PaLM-2L and GPT-4 respectively) probably because\nit is a binary classification task. CoT and TDB improve model performance a bit in the case of\n\n_\u223c_\nMuSiQue ( 3% and 3.5% respectively) which can be attributed to the inherent reasoning nature\n\n- f this task where these methods are shown to be helpful. In the case of StrategyQA, there is no\nsignificant performance gain with CoT and TDB which could be due to the high baseline performance\nin this task, with limited scope for these prompting methods to improve performance. Often, 1-shot\nperformance is significantly lower than their zero-shot methods, which could be attributed to potential\nexample bias (Zhao et al., 2021; Parmar et al., 2023). RAG improves model performance ( _\u223c_ 4%\nand 2% for MuSiQue and StrategyQA respectively.) STEP-BACK PROMPTING with the power\n\n- f abstraction produces the best performance of all methods: 42 _._ 8% in MuSiQue and 86 _._ 4% in\nStrategyQA, significantly outperforming GPT-4 on both tasks. We present a detailed error analysis\n\n- n StrategyQA in Appendix A.3.\n\n\n7 DISCUSSION\n\n\nAbstraction helps humans to solve complex tasks by removing irrelevant details and distilling highlevel concepts and principles to guide the problem-solving process. STEP-BACK PROMPTING breaks\n\n\n8\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ncomplex tasks such as knowledge-intensive QA, multi-hop reasoning,","Ab|rrect<br>Ab|sta|in<br>Hal|in<br>Hal|rect<br>Abst|rect<br>Abst|ain<br>Hall|\n\n\n\nFigure 3: **Model Performance on Datasets Stratified by Sufficient Context.** Given sufficient\ncontext, models have a higher correct percentage on these challenging datasets. Performance drops,\nbut the models are still able to answer a large portion of questions correct without sufficient context.\nOne prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient\ncontext. The smallest model Gemma 27B struggles to avoid hallucinations given insufficient context.\n\n\nappropriate. Without RAG, Claude 3.5 Sonnet abstains on 84.1% questions, while with RAG, the\nfraction of abstentions drops to 52%. Similarly, GPT 4o\u2019s abstention fraction moves from 34.4%\nto 31.2% and Gemini 1.5 Pro\u2019s drops from 100% to 18.6%. This phenomenon may arise from the\nmodel\u2019s increased confidence in the presence of any contextual information, leading to a higher\npropensity for hallucination rather than abstention.\n\n\n**Models Hallucinate with Both Sufficient and Insufficient Context.** Considering Figure 3, models\ngenerally achieve higher accuracy with sufficient context (higher **green bars**, top row) than without\nsufficient context (lower **green bars**, bottom row). However, looking at each row separately, we\ndiscover several findings. First, in the sufficient context case (top row), we see that models hallucinate\nmore than they abstain ( **red bars** are higher than **blue bars**, usually). The trend holds across all\nthree datasets. Moving to insufficient context (bottom row), we find a different distribution of model\nresponses, with more abstentions and hallucinations. This tendency varies notably across different\nmodels. For instance, Claude abstains more (higher **blue bars** ) with insufficient context, but answers\nfewer questions correctly (lower **green bars** ) than Gemini and GPT. These differences underscore\nthe potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\nmuch more hallucinations (higher **red bars** ) than the other models, except for HotPotQA, where we\nattribute the higher accuracy to the smaller retrieved contexts.\n\n\n4.3 QUALITATIVELY ANALYZING","-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\n\n- nly 500+ Wikipedia articles and thus the distribution of training examples is extremely biased, as\nargued previously by Lee et al. (2019).\n\n\n**5.2** **Ablation Study on Model Training**\n\n\nTo understand further how different model training\n\n- ptions affect the results, we conduct several additional experiments and discuss our findings below.\n\n\n\n\n\n\n**Sample efficiency** We explore how many training examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top- _k_\nretrieval accuracy with respect to different numbers of training examples, measured on the devel\n- pment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 examples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question\u2013passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently.\n\n\n**In-batch negative training** We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3.\nThe top block is the standard 1-of- _N_ training setting, where each question in the batch is paired\nwith a positive passage and its own set of _n_ negative passages (Eq. (2)). We find that the choice\n\n- f negatives \u2014 random, BM25 or gold passages\n(positive passages from other questions) \u2014 does\nnot impact the top- _k_ accuracy much in this setting\nwhen _k \u2265_ 20.\n\nThe middle bock is the in-batch negative training\n(Section 3.2) setting. We find that using a similar\nconfiguration (7 gold negative passages), in-batch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\n\n- r from the whole training set. Effectively, in-batch\nnegative training is an easy and memory"],"3":[") in a single context window.\n\n\n    - _Map community answers_ . Intermediate answers are generated in parallel. The LLM is also\nasked to generate a score between 0-100 indicating how helpful the generated answer is in\nanswering the target question. Answers with score 0 are filtered out.\n\n\n    - _Reduce to global answer_ . Intermediate community answers are sorted in descending order\n\n     - f helpfulness score and iteratively added into a new context window until the token limit\nis reached. This final context is used to generate the global answer returned to the user.\n\n\n**3.2** **Global Sensemaking Question Generation**\n\n\nTo evaluate the effectiveness of RAG systems for global sensemaking tasks, we use an LLM to\ngenerate a set of corpus-specific questions designed to asses high-level understanding of a given\ncorpus, without requiring retrieval of specific low-level facts. Instead, given a high-level description\n\n- f a corpus and its purposes, the LLM is prompted to generate personas of hypothetical users of\nthe RAG system. For each hypothetical user, the LLM is then prompted to specify tasks that this\nuser would use the RAG system to complete. Finally, for each combination of user and task, the\nLLM is prompted to generate questions that require understanding of the entire corpus. Algorithm\n1 describes the approach.\n\n\n6\n\n\n**Algorithm 1: Prompting Procedure for Question Generation**\n\n\n1: **Input:** Description of a corpus, number of users _K_, number of tasks per user _N_, number of\nquestions per (user, task) combination _M_ .\n2: **Output:** A set of _K \u2217_ _N \u2217_ _M_ high-level questions requiring global understanding of the corpus.\n3: **procedure** GENERATEQUESTIONS\n4: Based on the corpus description, prompt the LLM to:\n\n1. Describe personas of _K_ potential users of the dataset.\n\n2. For each user, identify _N_ tasks relevant to the user.\n\n3. Specific to each user & task pair, generate _M_ high-level questions that:\n\n\n      - Require understanding of the entire corpus.\n\n      - Do not require retrieval of specific low-level facts.\n\n\n5: Collect the generated questions to produce _K \u2217_ _N \u2217_ _M_ test questions for the dataset.\n6: **end procedure**\n\n\nFor our evaluation, we set _K_","**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r]_ = _\u2205_ when unsupervised retrieval\nsources are used. In general, the retrieval mem\n- ry can be retrieved from three kinds of sources:\nthe training corpus, external datasets in the same\nformat with the training corpus, and large-scale\nunsupervised corpus (\u00a72.2). Metrics that evaluate\nthe relevance between text are varied as well, in\n\u00a72.3 we divided them into three categories: sparsevector retrieval, dense-vector retrieval, and trainingbased retrieval. Finally, how to integrate the retrieval memory to the generation model is also significant, we also introduce some popular integration approaches in \u00a72.4.\n\n\n\n**2.2** **Retrieval Sources**\n\n\n**Training Corpus** Most previous studies search\nthe external memory from its _training corpus_ (Song\net al., 2016; Gu et al., 2018; Weston et al., 2018).\nIn the inference time, retrieved examples with high\nrelevant scores could be regarded as extra references and reduce model\u2019s uncertainty in generation.\nThe main motivation of those works is to to store\n\nknowledge not only in the model parameters but\nalso in an explicit and accessible form, making the\nmodel be able to re-access it during inference.\n\n\n**External Data** Some researchers also propose to\nretrieval relevant samples from _external datasets_\n(Su et al., 2021c; Xiao et al., 2021). In these studies, the retrieval pool is different with the training\ncorpus, which can further provide additional information that are not contained in the training corpus.\nThis is especially beneficial for applications such\nas domain adaptation and knowledge update. For\nexample, Khandelwal et al. (2020a); Zheng et al.\n(2021a) employ the in-domain dataset as the external memory to achieve fast domain adaptation for\nmachine translation.\n\n\n**Unsupervised Data** One limitation for previous\ntwo sources is that the datasets have to be supervised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) propose a cross-lingual retriever to directly retrieve target sentence from _unsupervised corpus_ (i.e"," Transformers for longer sequences. In\n_Proc. of NeurIPS_ .\n\n\n**A** **Ambiguity in Multi-Document QA**\n**Distractor Documents**\n\n\nFollowing past work on NaturalQuestions-Open\n(Izacard et al., 2021; Izacard and Grave, 2021, _inter_\n_alia_ ), we use a Wikipedia dump from late 2018\nas our retrieval corpus. However, this standard\nWikipedia dump has a small amount of temporal\nmismatch with the NaturalQuestions annotations.\n\nFor example, consider the question \u201cwhat nfl\nteam does robert griffin iii play for\u201d. The NaturalQuestions annotated answer is \u201ccurrently a free\nagent\u201d. However, the Wikipedia retrieval corpus\ncontains the information that he plays for the \u201cBaltimore Ravens\u201d, since he was released from the team\nbetween the Wikipedia dump\u2019s timestamp and the\nNaturalQuestions annotation process.\nWe use the ambiguity annotations of Min et al.\n(2020) to create a subset unambiguous questions.\nExperiments on this unambiguous subset of the\ndata show similar results and conclusions as the\n\nexperiments on the full questions collection (Figure 12).\n\n\n|(~4K tokens unambi<br>,|Col2|Col3|Col4|guousques|Col6|tions)|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\nFigure 12: Language model performance on a unambiguous subset of questions.\n\n\n**B** **Random Distractors in**\n\n**Multi-Document QA**\n\n\nWe also run multi-document question answering\nexperiments with random Wikipedia documents as\ndistractors, which allows us to ablate the impact\n\n- f retrieved distractors (hard negatives). Note that\nin this setting, the the document containing the answer can often be identified with simple heuristics\n(e.g., lexical overlap with the query). Figure 13\npresents the results of this experiment. Although\nall models have higher absolute accuracy in this\nsetting, they surprisingly still struggle to reason\n\n- ver their entire input context, indicating that their\nperformance degradation is not solely due to an\ninability to identify relevant documents.\n\n\n**C** **Randomizing Distractor Order in**\n**Multi-Document QA**\n\n\nOur prompt instructs the language model to use\nthe provided search results to answer the question.\nThere may","corpus-aware architecture for retrieval and allows us to train a state-of-the-art text embedding model.\n\n\n9 ACKNOWLEDGEMENTS\n\n\nThanks to Orion Weller, Vin Sachidananda, and Zach Nussbaum for valuable feedback on this\nresearch. We would also like to acknowledge to Nomic and Hyperbolic for providing the compute\nnecessary to conduct this research. This work was partially supported by Intelligence Advanced\nResearch Projects Activity (IARPA), via the HIATUS Program #2022-22072200003. JM is supported\nby an NSF GFRP fellowship.\n\n\n11\n\n\nREFERENCES\n\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica,\nSaurabh Tiwary, and Tong Wang. Ms marco: A human generated machine reading comprehension\n[dataset, 2018. URL https:\/\/arxiv.org\/abs\/1611.09268.](https:\/\/arxiv.org\/abs\/1611.09268)\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego\nde Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren\nMaggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol\nVinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. Improving\nlanguage models by retrieving from trillions of tokens, 2022.\n\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations, 2020.\n\n\nWilliam Coster and David Kauchak. Simple English Wikipedia: A new text simplification task.\nIn Dekang Lin, Yuji Matsumoto, and Rada Mihalcea (eds.), _Proceedings of the 49th Annual_\n_Meeting of the Association for Computational Linguistics: Human Language Technologies"," time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine"],"4":["ay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language models. _Transactions of the Association_\n_for Computational Linguistics_ [, 2023. URL https:\/\/arxiv.org\/abs\/2302.00083.](https:\/\/arxiv.org\/abs\/2302.00083)\n\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,\nStella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training\nenables zero-shot task generalization. In _International Conference on Learning Representations_,\n[2022. URL https:\/\/openreview.net\/forum?id=9Vrb9D0WI4.](https:\/\/openreview.net\/forum?id=9Vrb9D0WI4)\n\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. _arXiv preprint arXiv:2302.04761_ [, 2023. URL https:\/\/arxiv.org\/abs\/2302.](https:\/\/arxiv.org\/abs\/2302.04761)\n[04761.](https:\/\/arxiv.org\/abs\/2302.04761)\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klim"," the answer is: tuberculosis.\n\n\nQ: Which film has the director died earlier, When The Mad Aunts Arrive or The Miracle Worker (1962 Film)?\nA: When The Mad Aunts Arrive was directed by Franz Josef Gottlieb. The Miracle Worker (1962 film) was directed by\n\nArthur Penn. Franz Josef Gottlieb died on 23 July 2006. Arthur Penn died on September 28, 2010. Thus, of the two, the\ndirector to die earlier is Franz Josef Gottlieb, who directed When The Mad Aunts Arrive. So the answer is: When The\nMad Aunts Arrive.\n\n\nQ: Which album was released earlier, What'S Inside or Cassandra'S Dream (Album)?\nA: What's Inside was released in the year 1995. Cassandra's Dream (album) was released in the year 2008. Thus, of the two,\n\nthe album to release earlier is What's Inside. So the answer is: What's Inside.\n\n\nQ: Are both mountains, Serre Mourene and Monte Galbiga, located in the same country?\nA: Serre Mourene is located in Spain. Monte Galbiga is located in Italy. Thus, the two countries are not located in the same\n\n\ncountry. So the answer is: no.\n\n\nQ: What is the date of birth of the director of film Best Friends (1982 Film)?\nA: The film Best Friends was directed by Norman Jewison. Norman Jewison was born on July 21, 1926. So the answer is:\n\nJuly 21, 1926.\n\n\nQ: Which film has the director born first, Two Weeks With Pay or Chhailla Babu?\nA: Two Weeks with Pay was directed by Maurice Campbell. Chhailla Babu was directed by Joy Mukherjee. Maurice\n\nCampbell was born on November 28, 1919. Joy Mukherjee was born on 24 February 1939. Thus, from the two directors,\n\nChhailla Babu was born first, who directed Two Weeks With Pay. So the answer is: Two Weeks With Pay.\n\n\nQ: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nA: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah has a\n\ngrandchild named Prithvipati Shah. So the answer is:"," graph comprehension\nand question-answering capabilities through soft prompting\n\n- f the LLM, and employs the Prize-Collecting Steiner Tree\n(PCST) optimization problem for targeted graph retrieval. On\nthe contrary, it requires additional effort to build, validate,\nand maintain structured databases. On the contrary, it requires\nadditional effort to build, validate, and maintain structured\n\ndatabases.\n\n_LLMs-Generated Content._ Addressing the limitations of\nexternal auxiliary information in RAG, some research has\nfocused on exploiting LLMs\u2019 internal knowledge. SKR [58]\nclassifies questions as known or unknown, applying retrieval\nenhancement selectively. GenRead [13] replaces the retriever\nwith an LLM generator, finding that LLM-generated contexts\n\n- ften contain more accurate answers due to better alignment\nwith the pre-training objectives of causal language modeling.\nSelfmem [17] iteratively creates an unbounded memory pool\nwith a retrieval-enhanced generator, using a memory selector to choose outputs that serve as dual problems to the\n\n- riginal question, thus self-enhancing the generative model.\nThese methodologies underscore the breadth of innovative\ndata source utilization in RAG, striving to improve model\nperformance and task effectiveness.\n_2) Retrieval Granularity:_ Another important factor besides\nthe data format of the retrieval source is the granularity of\nthe retrieved data. Coarse-grained retrieval units theoretically\ncan provide more relevant information for the problem, but\nthey may also contain redundant content, which could distract\nthe retriever and language models in downstream tasks [50],\n\n[87]. On the other hand, fine-grained retrieval unit granularity\nincreases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge. Choosing\n\n\nthe appropriate retrieval granularity during inference can be\na simple and effective strategy to improve the retrieval and\ndownstream task performance of dense retrievers.\nIn text, retrieval granularity ranges from fine to coarse,\nincluding Token, Phrase, Sentence, Proposition, Chunks, Document. Among them, DenseX [30]proposed the concept of\nusing propositions as retrieval units. Propositions are defined\nas atomic expressions in the text, each encapsulating a unique\nfactual segment and presented in a concise, self-contained natural language format. This approach aims to enhance retrieval\nprecision and relevance. On the Knowledge Graph (KG),\nretrieval granularity includes Entity, Triplet, and sub-Graph"," answer is: 1.\n\n\nQ: How many years was the ship that took the battalion from New South Wales to Ceylon in service?\nA: The ship that took the battalion from New South Wales to Ceylon is General Hewitt. General Hewitt was launched in\n\nCalcutta in 1811. General Hewitt was sold for a hulk or to be broken up in 1864. So she served for a total of 1864 \u2212\n1811 = 53 years. So the answer is: 53.\n\n\nQ: What year was the theatre that held the 2016 NFL Draft built?\nA: The theatre that held the 2016 NFL Draft is Auditorium Theatre. The Auditorium Theatre was built in 1889. So the answer\n\n\nis: 1889.\n\n\nQ: How long had Milan been established by the year that Nava returned there as a reserve in the first team's defense?\nA: Nava returned to Milan as a reserve in the first team's defense in the year 1990. Milan had been established in the year\n\n\n1899. Thus, Milan had been established for 1990 \u22121899 = 91 years when Milan returned to Milan as a reserve in the\nfirst team's defense. So the answer is: 91.\n\n\nQ: When was the town Scott was born in founded?\nA: Scott was born in the town of Cooksville, Illinois. Cooksville was founded in the year 1882. So the answer is: 1882.\n\n\nQ: In what country did Wright leave the French privateers?\nA: Wright left the French privateers in Bluefield's river. Bluefields is the capital of the South Caribbean Autonomous Region (\n\nRAAS) in the country of Nicaragua. So the answer is: Nicaragua.\n\n\nQ: Who plays the A\u2212Team character that Dr. Hibbert fashioned his hair after?\nA: Dr. Hibbert fashioned his hair after Mr. T from The A\u2212Team. Mr T.'s birthname is Lawrence Tureaud. So the answer is:\n\n\nLawrence Tureaud.\n\n\nQ: How many people attended the conference held near Berlin in January 1942?\nA: The conference held near Berlin in January 1942 is Wannsee Conference. Wannsee Conference was attended by 15 people.\n\nSo the answer is: 15.\n\n\nQ: When did the country Ottwalt went into exile in founded?\nA:","0. REALM:\nRetrieval-augmented language model pre-training.\n_ArXiv_, abs\/2002.08909.\n\n\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yunhsuan Sung, L\u00b4aszl\u00b4o Luk\u00b4acs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart\nreply. _ArXiv_, abs\/1705.00652.\n\n\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for Web search using\n\n\n\nclickthrough data. In _ACM International Confer-_\n_ence on Information and Knowledge Management_\n_(CIKM)_, pages 2333\u20132338.\n\n\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architectures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In _International Conference_\n\n_on Learning Representations (ICLR)_ .\n\n\nGautier Izacard and Edouard Grave. 2020. Leveraging\npassage retrieval with generative models for open domain question answering. _ArXiv_, abs\/2007.01282.\n\n\nJeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou. 2017.\nBillion-scale similarity search with GPUs. _ArXiv_,\nabs\/1702.08734.\n\n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In _Association for Computational Lin-_\n_guistics (ACL)_, pages 1601\u20131611.\n\n\nOmar Khattab and Matei Zaharia. 2020. ColBERT:\nEfficient and effective passage search via contextualized late interaction over BERT. In _ACM SIGIR_\n_Conference on Research and Development in Infor-_\n_mation Retrieval (SIGIR)_, pages 39\u201348.\n\n\nBrian Kulis. 2013. Metric learning: A survey. _Foun-_\n_dations and Trends in Machine Learning_, 5(4):287\u2013"],"5":[" relies on retrieving support passages before processing them with a\ngenerative model. We show that while conceptually\nsimple, this approach is competitive with existing\nmethods, and that it scales well with the number\n\n- f retrieved passages. In future work, we plan to\nmake this model more efficient, in particular when\nscaling to large number of support passages. We\nalso plan to integrate the retrieval in our model, and\nto learn the whole system end-to-end.\n\n\n\n**References**\n\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph\nfor question answering. In _Proc. ICLR_ .\n\n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. _arXiv preprint arXiv:2005.14165_ .\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer opendomain questions. In _Proc. ACL_ .\n\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehension. In _Proc. ACL_ .\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In _Proc. NAACL_ .\n\n\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In _Proc. ACL_ .\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. _arXiv_\n_preprint arXiv:2002.08909_ .\n\n\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2019. How can we know what language\nmodels know? _","\nthe parametric knowledge of LLMs and the external sources. In contrast, our RAG-Star framework\nintegrates tree-based search to fully explore the solution space and repurpose the retrieval information\nas external guidance to the reasoning process.\n\n\n**Enhancing LLMs with Search.** Applying search\n\n- n top of LLMs has been a topic of much interest. Several recent works have explored search\nalgorithms to improve the performance of LLMs\nduring the inference stage (Wang et al., 2024a;\nZhang et al., 2024). The bitter lesson (Sutton,\n2019) famously suggests that two forms of scaling, _i.e.,_ learning and search, supersede all other\napproaches. Many studies have proven that scaling the inference-time computation can lead to\nsubstantial improvements in the performance of\nLLMs without training (Brown et al., 2024; Snell\net al., 2024). These search algorithms, where\nmultiple branches of outcomes are explored during search, have been widely applied in reinforcement learning algorithms (Hart et al., 1968; Silver\net al., 2017) and many real-world applications such\nas AlphaGo (Silver et al., 2016) for their good\nexploration-exploitation trade-off. However, these\napproaches mainly rely on the internal knowledge\n\n- f LLMs to search potential solutions, which might\nnot be optimal and leads to a amount of rollouts,\nsignificantly slowing down the decoding process.\nIn this paper, we leverage the external retrieval\nsources to enhance the deliberative search process\nwith LLMs, effectively differentiate the internal\nreasoning and external retrieval.\n\n\n**3** **Preliminary**\n\n\nIn this section, we will first formally define our task\nand then introduce Monte Carlo Tree Search which\n\nis used in our proposed RAG-Star approach.\n\n\n**Task Formulation.** In this work, we mainly focus on open-domain multi-hop question answering (Chen et al., 2019; Yang et al., 2018), which\nrequires multiple steps of reasoning across different documents to answer questions. Previous work\ntypically adopts an iterative _reason-then-generate_\npipeline (Wei et al., 2022; Huang and Chang, 2023).\nAt each step, the LLM first infers an intermediate\nsub-query based on the current situation and then\n","clweb.org\/anthology\/D18-1255](https:\/\/www.aclweb.org\/anthology\/D18-1255)` .\n\n\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\nsystems. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language_\n_Processing_, pages 3950\u20133959, Brussels, Belgium, October-November 2018. Association for\nComputational Linguistics. doi: 10.18653\/v1\/D18-1429. URL `[https:\/\/www.aclweb.org\/](https:\/\/www.aclweb.org\/anthology\/D18-1429)`\n`[anthology\/D18-1429](https:\/\/www.aclweb.org\/anthology\/D18-1429)` .\n\n\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne, editors,\n_Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic_\n\n\n13\n\n\n_approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing_\n_Systems (NIPS 2016), Barcelona, Spain, December 9, 2016_, volume 1773 of _CEUR Workshop_\n_Proceedings_ . CEUR-WS.org, 2016. URL `[http:\/\/ceur-ws.org\/Vol-1773\/CoCoNIPS_](http:\/\/ceur-ws.org\/Vol-1773\/CoCoNIPS_2016_paper9.pdf)`\n`[2016_paper9.pdf](http:\/\/ceur-ws.org\/Vol-1773\/CoCoNIPS_2016_paper9.pdf)` .\n\n\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. _arXiv preprint_\n_arXiv:1901.04085_, 2019. URL `[https:\/\/arxiv.org\/abs\/1901.04085](https:\/\/arxiv.org\/abs\/1901.04085)` .\n\n\n[45] Myle Ott"," efficiency of research workflows and\nfostering interdisciplinary collaborations, LLMs are poised\nto accelerate the pace of scientific discovery and innovation\nacross various fields. To distill knowledge from an LLM,\nDARWIN Series (Xie et al., 2023a) utilizes a semi selfinstruct for instruction generation for science papers, which\nis then used to fine-tune an LLM. SciGLM (Zhang et al.,\n2024) proposes to train a scientific LLM, which prompts a\nteacher LLM to generate detailed answers for unlabelled\nscientific questions, as well as a self-reflective critic-andrevise to improve data quality. Besides the above knowledge\ndistillation methods to adapt LLMs to science, we will also\ndelve into how the distillation happens in sub-domains, e.g.,\nmathematics, astronautics, chemistry, etc.\n\n\n_**Mathematics.**_ The application of LLMs within the subdomain of mathematics heralds a transformative era in\nmathematical research, education, and problem-solving\n(Azerbayev et al., 2023; Yu et al., 2023b). LLMs in mathematics facilitate the exploration and understanding of complex\nmathematical theories and problems by providing intuitive\nexplanations, proofs, and solutions that can bridge the\ngap between advanced mathematical concepts and learners at various levels. These models have shown potential\nin conjecturing new mathematical theorems and patterns,\nthus opening new avenues for research and discovery that\nmight not have been readily accessible to humans alone.\nIn education, they serve as personalized tutors, offering\nstudents step-by-step guidance through mathematical problems and adapting explanations to the learner\u2019s level of understanding. This democratizes access to high-quality mathematical education and fosters a deeper appreciation and\nunderstanding of mathematics among a broader audience.\nBy enhancing collaborative efforts through the generation\n\n- f new ideas and the simplification of complex concepts,\nLLMs are poised to significantly advance the field of mathematics, making it more accessible, efficient, and innovative. WizardMath (Luo et al., 2023b) enhances the mathematical reasoning capabilities of Llama-2 by applying the\n\n\nnovel Reinforcement Learning from Evol-Instruct Feedback\n(RLEIF) method, significantly outperforming other opensource LLMs on the GSM8k and MATH benchmarks, as\nwell as surpassing several closed-source LLM","Raffel et al., 2019), achieving good performance on\n\n- pen-domain QA and other knowledge-intensive\ntasks.\n\n\n**8** **Conclusion**\n\n\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the traditional sparse retrieval component in open-domain\nquestion answering. While a simple dual-encoder\napproach can be made to work surprisingly well,\nwe showed that there are some critical ingredients\nto training a dense retriever successfully. Moreover,\n\n- ur empirical analysis and ablation studies indicate\nthat more complex model frameworks or similarity\nfunctions do not necessarily provide additional values. As a result of improved retrieval performance,\nwe obtained new state-of-the-art results on multiple\n\n- pen-domain question answering benchmarks.\n\n\n**Acknowledgments**\n\n\nWe thank the anonymous reviewers for their helpful\ncomments and suggestions.\n\n\n**References**\n\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over Wikipedia graph\nfor question answering. In _International Conference_\n\n_on Learning Representations (ICLR)_ .\n\n\nPetr Baudi\u02c7s and Jan Sediv`y. 2015. [\u02c7] Modeling of the\nquestion answering task in the yodaqa system. In _In-_\n_ternational Conference of the Cross-Language Eval-_\n_uation Forum for European Languages_, pages 222\u2013\n228. Springer.\n\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\n\n\nquestion-answer pairs. In _Empirical Methods in Nat-_\n_ural Language Processing (EMNLP)_ .\n\n\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard\nS\u00a8ackinger, and Roopak Shah. 1994. Signature verification using a \u201cSiamese\u201d time delay neural network.\nIn _NIPS_, pages 737\u2013744.\n\n\nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,\nMatt Deeds, Nicole Hamilton, and Greg Hullender.\n2005. Learning to rank using gradient descent. In\n_Proceedings of the 22nd international conference on_\n_Machine learning_, pages 89\u201396.\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to"],"6":["Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set. Until recently, retrieval models were dependent\n\n- n traditional term-based information retrieval (IR) methods, which fail to capture the semantics of\nthe question beyond lexical matching and remain a major performance bottleneck for the task. Recent\nwork on dense retrieval methods instead uses pretrained encoders to cast the question and documents\ninto dense representations in a vector space and relies on fast maximum inner-product search (MIPS)\nto complete the retrieval. These approaches (Lee et al., 2019; Guu et al., 2020; Karpukhin et al.,\n2020) have demonstrated significant retrieval improvements over traditional IR baselines.\n\n\nHowever, such methods remain limited to _simple_ questions, where the answer to the question\nis explicit in a single piece of text evidence. In contrast, _complex_ questions typically involve\naggregating information from multiple documents, requiring logical reasoning or sequential (multihop) processing in order to infer the answer (see Figure 1 for an example). Since the process for\nanswering such questions might be sequential in nature, single-shot approaches to retrieval are\ninsufficient. Instead, iterative methods are needed to recursively retrieve new information at each\nstep, conditioned on the information already at hand. Beyond further expanding the scope of existing\ntextual open-domain QA systems, answering more complex questions usually involves _multi-hop_\n_reasoning_, which poses unique challenges for existing neural-based AI systems. With its practical\n\n\n_\u2217_ Equal Contribution\n1\n[https:\/\/github.com\/facebookresearch\/multihop_dense_retrieval.](https:\/\/github.com\/facebookresearch\/multihop_dense_retrieval)\n\n\n1\n\n\nPublished as a conference paper at ICLR 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: An overview of the multi-hop dense retrieval approach.\n\n\nand research values, multi-hop QA has been extensively studied recently (Talmor & Berant, 2018;\nYang et al., 2018; Welbl et al., 2018) and remains an active research area in NLP (Qi et al., 2019; Nie\net al., 2019; Min et al., 2019; Zhao et al., 2020; Asai","\u2190-_ 30%_\u2190-_ of respon-<br>dents have_\u2190-_ anaccount-up_\u2190-_ 10% from last year_\u2190-_ more women_\u2190-_ than men say_\u2190-_ they don\u2019t<br>have_\u2190-_ an account_\u2190-_ 53%_\u2190-_ In Europe_\u2190-_ 49%_\u2190-_ In North America_\u2190-_ 40%_\u2190-_ In the UK_\u2190-_<br>Facebook_\u2190-_ Despite last year\u2019s hype around Graph Search,Facebook hasn\u2019t made any progress with<br>monetising_\u2190-_ its recruitment potential -jobseekers remain very negative about Facebook playing any<br>part_\u2190-_ 13%_\u2190-_ said they\u2019d be happy_\u2190-_ to see adverts_\u2190-_ 92%_\u2190-_ said they would not be_\u2190-_ happy<br>to be contacted by_\u2190-_ a recruiter on Facebook_\u2190-_ 1%_\u2190-_ Don\u2019t bank on social media \u2013 Michael<br>Page brings you a broader range of talent, and jobs_\u2190-_ www.michaelpage.com.au\/salarycentre_\u2190-_ of<br>respondents_\u2190-_ (who are job seekers) said they_\u2190-_ would use it to look for jobs_\u2190-_ MichaelPage_\u2190-_<br>Financial Services_\u2190-_ Specialists in fnancial services recruitment_\u2190-_ www.michaelpage.com.au_\u2190-_|\n|**Answer**|49% \u2717**Incorrect**|53% \u2713**Correct**|\n\n\n\n23\n\n\nPublished as a conference paper at ICLR 2025\n\n\nG ADDITIONAL RETRIEVAL AND GENERATION RESULTS\n\n\nTable 10: Additional retrieval performance in MRR@10.\n\n\nTable 11: Additional generation performance in accuracy (%). All models and methods utilize the\nsame retriever, VisRAG. Performance relative to Oracle is colored in blue.\n\n\n\n\n\n\n|top-6<br>MiniCPM-V 2 .6 t Oo rp a- c1 l0<br>e|6789(955%)<br>64. (91.<br>95 4%)<br>. .<br>7108(100%)<br>.|5714(837%)<br>57. (83.<br>14 7%)<br>68. (10.<br>25 0%)<br>.|7005("," the answer is: tuberculosis.\n\n\nQ: Which film has the director died earlier, When The Mad Aunts Arrive or The Miracle Worker (1962 Film)?\nA: When The Mad Aunts Arrive was directed by Franz Josef Gottlieb. The Miracle Worker (1962 film) was directed by\n\nArthur Penn. Franz Josef Gottlieb died on 23 July 2006. Arthur Penn died on September 28, 2010. Thus, of the two, the\ndirector to die earlier is Franz Josef Gottlieb, who directed When The Mad Aunts Arrive. So the answer is: When The\nMad Aunts Arrive.\n\n\nQ: Which album was released earlier, What'S Inside or Cassandra'S Dream (Album)?\nA: What's Inside was released in the year 1995. Cassandra's Dream (album) was released in the year 2008. Thus, of the two,\n\nthe album to release earlier is What's Inside. So the answer is: What's Inside.\n\n\nQ: Are both mountains, Serre Mourene and Monte Galbiga, located in the same country?\nA: Serre Mourene is located in Spain. Monte Galbiga is located in Italy. Thus, the two countries are not located in the same\n\n\ncountry. So the answer is: no.\n\n\nQ: What is the date of birth of the director of film Best Friends (1982 Film)?\nA: The film Best Friends was directed by Norman Jewison. Norman Jewison was born on July 21, 1926. So the answer is:\n\nJuly 21, 1926.\n\n\nQ: Which film has the director born first, Two Weeks With Pay or Chhailla Babu?\nA: Two Weeks with Pay was directed by Maurice Campbell. Chhailla Babu was directed by Joy Mukherjee. Maurice\n\nCampbell was born on November 28, 1919. Joy Mukherjee was born on 24 February 1939. Thus, from the two directors,\n\nChhailla Babu was born first, who directed Two Weeks With Pay. So the answer is: Two Weeks With Pay.\n\n\nQ: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nA: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah has a\n\ngrandchild named Prithvipati Shah. So the answer is:","xiv.org\/abs\/1603.09320](https:\/\/arxiv.org\/abs\/1603.09320)` .\n\n\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. _arXiv_\n_preprint arXiv:2002.06177_, 2020. URL `[https:\/\/arxiv.org\/abs\/2002.06177](https:\/\/arxiv.org\/abs\/2002.06177)` .\n\n\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\u00e4schel, Vassilis\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\nverifiability of generated text. _arXiv preprint arXiv:1911.03587_, 2019. URL `[https:](https:\/\/arxiv.org\/abs\/1911.03587)`\n`[\/\/arxiv.org\/abs\/1911.03587](https:\/\/arxiv.org\/abs\/1911.03587)` .\n\n\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In _ICLR_, 2018. URL `[https:\/\/openreview.net\/forum?id=r1gs9JgRZ](https:\/\/openreview.net\/forum?id=r1gs9JgRZ)` .\n\n\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In _Proceedings of the 2018_\n_Conference on Empirical Methods in Natural Language Processing_, pages 2322\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653\/v1\/D18-1255. URL `[https:\/\/www.aclweb.org\/anthology\/D18-1255](https:\/\/www.aclweb.org\/anthology\/D18-1255)` .\n\n\n[42] Preksha Nema and Mitesh M. Khapra. Towards a","Published as a conference paper at ICLR 2021\n\n## ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS\n### WITH MULTI-HOP DENSE RETRIEVAL\n\n\n**Wenhan Xiong** [1] _[\u2217]_ **Xiang Lorraine Li** [2] _[\u2217]_ **Srinivasan Iyer** _[\u2021]_ **Jingfei Du** _[\u2021]_\n\n\n**Patrick Lewis** _[\u2021\u2020]_ **William Wang** [1] **Yashar Mehdad** _[\u2021]_ **Wen-tau Yih** _[\u2021]_\n\n\n**Sebastian Riedel** _[\u2021\u2020]_ **Douwe Kiela** _[\u2021]_ **Barlas O\u02d8guz** _[\u2021]_\n\n\n_\u2021_ Facebook AI\n1University of California, Santa Barbara\n2University of Massachusetts Amherst\n\n_\u2020_ University College London\n_{_ xwhan, william _}_ @cs.ucsb.edu, xiangl@cs.umass.edu,\n_{_ sviyer, jingfeidu, plewis, mehdad, scottyih, sriedel, dkiela, barlaso _}_ @fb.com\n\n\nABSTRACT\n\n\nWe propose a simple and efficient multi-hop dense retrieval approach for answering\ncomplex open-domain questions, which achieves state-of-the-art performance on\ntwo multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previ\n     - us work, our method does not require access to any corpus-specific information,\nsuch as inter-document hyperlinks or human-annotated entity markers, and can\nbe applied to any unstructured text corpus. Our system also yields a much better\nefficiency-accuracy trade-off, matching the best published accuracy on HotpotQA\nwhile being 10 times faster at inference time. [1]\n\n\n1 INTRODUCTION\n\n\n_Open domain question answering_ is a challenging task where the answer to a given question needs to\nbe extracted from a large pool of documents. The prevailing approach (Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set"],"7":["2019) explicitly encode the hyperlink graph structure within their\nanswer prediction models. In fact, this particular inductive bias provides a perhaps unreasonably\nstrong advantage in the specific context of HotpotQA, which by construction guarantees groundtruth passage sequences to follow hyperlinks. Despite not using such prior knowledge, our model\n\n- utperforms all previous systems by large margins, especially on supporting fact prediction, which\nbenefits more directly from better retrieval.\n\n\n\n**Reader** **Model** **Variants** Results for\nreader model variants are shown in Table 6. [7]\n\nTable 6: Reader comparison on HotpotQA dev set.\n\nFirst, we see that the BERT-wwm reader\nis 1-2% worse than the ELECTRA reader Model Top k EM F1\nwhen using enough passages. However, ELECTRA Top 50 61.7 74.3\nit still outperforms the results in (Asai Extractive ELECTRA Top 250 63.4 76.2\net al., 2020) which also uses BERT-wwm BERT-wwm Top 250 61.5 74.7\nfor answer prediction. While RAG and Multi-hop RAG Top 4*4 51.2 63.9\n\nGenerative\n\nFiD have shown strong improvements over FiD Top 50 61.7 73.1\nextractive models on single-hop datasets\nsuch as NaturalQuestions (Kwiatkowski\net al., 2019), they do not show an advantage in the multi-hop case. Despite having twice as many\nparameters as ELECTRA, FiD fails to outperform it using the same amount of context (top 50). In\ncontrast, on NaturalQuestions, FiD is 4 points better than a similar extractive reader when using the\ntop 100 passages in both. [8] We hypothesize that the improved performance on single-hop questions is\ndue to the ability of larger pretrained models to more effectively memorize single-hop knowledge\nabout real-world entities. [9] Compared to multi-hop questions that involve multiple relations and\nmissing entities, simple questions usually only ask about a certain property of an entity. It is likely\nthat such simple entity-centric information is explicitly mentioned by a single text piece in the\npretraining corpus, while the evidence for multihop questions is typically dispersed, making the\n\n\n7",",\nbut at least a model with a separate reader is always better or close to the one without. So overall\nwe go with the choice of using the reader for the\nexperiments in this paper.\n\n\n\nModel HotpotQA 2WikiMQA MuSiQue IIRC\n\n\nIRCoT QA **59.1** _\u00b1_ **0.9** **66.5** _\u00b1_ **1.4** **30.8** _\u00b1_ **0.2 42.5** _\u00b1_ **2.1**\nw\/o reader 52.6 _\u00b1_ 0.3 60.9 _\u00b1_ 0.6 24.9 _\u00b1_ 0.2 40.3 _\u00b1_ 0.2\n\n\nIRCoT QA 60.7 _\u00b1_ 1.1 68.0 _\u00b1_ 1.5 **36.5** _\u00b1_ **1.2 49.9** _\u00b1_ **1.1**\nw\/o reader **61.0** _\u00b1_ **0.7** **70.4** _\u00b1_ **1.5** 31.5 _\u00b1_ 0.6 48.4 _\u00b1_ 1.0\n\n\nTable 6: Answer F1 of IRCoT QA with and without\na separate reader for Flan-T5-XXL (top two rows) and\nGPT3 (bottom two rows). When the reader is not used,\nthe answer is extracted from the CoT generated by\nIRCoT while doing the retrieval. Ablating the reader\nusually hurts the performance.\n\n\n**G** **Prompts**\n\n\nOur manually written chain-of-thought annotations\nfor HotpotQA, 2WikiMultihopQA, MuSiQue, and\nIIRC are given in Listing 1, 2, 3 and 4 respectively. Our prompts for GPT3 CoT Prompting are\nthe same as these, except they have Wikipipedia\nparagraphs on the top of the questions as shown\nin \u00a7 3.1 [20] . Our prompts for GPT3 Direct Prompting are the same as that of CoT prompting, except\nhave the answer after \"A:\" directly. Our prompts\nfor Flan-T5-* are slightly different from that of\nGPT3. For CoT Prompting, we prefix"," of the gold paragraphs among the retrieved set of paragraphs. We search for the hyperparameter _K_ (and _M_ for IRCoT) that maximizes\nthe recall on the dev set and use it on the test set.\n\n\n[6https:\/\/www.elastic.co\/](https:\/\/www.elastic.co\/)\n\n\n\nThe reported metric can thus be viewed as the _fixed-_\n_budget optimal recall_ for each system considered. [7]\n\n\n**QA Reader.** To implement the reader, we use\nthe same LMs as used in the reason-step of\nIRCoT Retriever. We found that QA readers implemented with Flan-T5-* perform better with the\nDirect Prompting strategy and GPT3 performs better with CoT Prompting strategy (see App. E).\nHence we use Direct prompting strategy for QA\nwith Flan-T5-* and CoT with GPT3 for the experiments. [8]\n\n\nThe QA reader has one hyperparameter _M_ : the\nnumber of distractor paragraphs in the in-context\ndemonstrations. We search for _M_ in _{_ 1 _,_ 2 _,_ 3 _}_ .\nWhen used in conjunction with IRCoT retriever\n_M_ is tied for the CoT generator and the reader.\n\n\n**Open-Domain QA (ODQA) Models.** Putting retrievers and readers together, we experiment with\nODQA models constructed from the various language models denoted as **OneR QA** and **IRCoT**\n**QA** . For IRCoT QA, the choice of LM for the CoT\ngenerator and the reader is kept the same. We also\nexperiment with retriever-less QA readers **NoR QA**\nto assess how well LMs can answer the question\nfrom their parametric knowledge alone. To select\nthe best hyperparameters for the ODQA model,\nwe search for the hyperparameters _K_ and _M_ that\nmaximize the answer F1 on the development set.\nIIRC is structured slightly differently from the\n\n- ther datasets, in that its questions are grounded\nin a main passage and other supporting paragraphs\ncome from the Wikipedia pages of entities mentioned in this passage. We slightly modify the retrievers and readers to account for this (see App. B).\n\n\n**5** **Results**\n\n\n**IRCoT retrieval is better than one-step.** Fig. 3\ncompares OneR with"," multi-hop retrieval.\n\n\nMore details about these two classes of reader models are described in Appendix B.2.\n\n\n3.2.1 RESULTS\n\nTable 5: HotpotQA-fullwiki test results.\n\n\nAnswer Support Joint\nMethods\nEM F1 EM F1 EM F1\n\n\nGoldEn Retriever (Qi et al., 2019) 37.9 48.6 30.7 64,2 18.9 39.1\nSemantic Retrieval (Nie et al., 2019) 46.5 58.8 39.9 71.5 26.6 49.2\nTransformer-XH (Zhao et al., 2020) 51.6 64.1 40.9 71.4 26.1 51.3\nHGN (Fang et al., 2019) 56.7 69.2 50.0 76.4 35.6 59.9\nDrKIT (Dhingra et al., 2020) 42.1 51.7 37.1 59.8 24.7 42.9\nGraph Recurrent Retriever (Asai et al., 2020) 60.0 73.0 49.1 76.4 35.4 61.2\n\n\nMDR (ELECTRA Reader) **62.3** **75.3** **57.5** **80.9** **41.8** **66.6**\n\n\n**Comparison with Existing Systems** Table 5 compares the HotpotQA test performance of our best\nELECTRA reader with recently published systems, using the numbers from the official leaderboard,\nwhich measure answer and supporting sentence exact match (EM)\/F1 and joint EM\/F1. Among\nthese methods, only GoldEn Retriever (Qi et al., 2019) does not exploit hyperlinks. In particular,\nGraph Recurrent Retriever trains a graph traversal model for chain retrieval; TransformerXH (Zhao\net al., 2020) and HGN (Fang et al., 2019) explicitly encode the hyperlink graph structure within their\nanswer prediction models. In fact, this particular inductive bias provides a perhaps unreasonably\nstrong advantage in the specific context of HotpotQA, which by construction guarantees groundtruth passage sequences"," generation models,\nnumerical comparison is still a bigger issue compared to extractive models.\n\n\nB MODEL DETAILS\n\n\nB.1 BEST MODEL HYPERPARAMETERS\n\n\nTable 11: Hyperparameters of Retriever\n\nlearning rate 2e-5\nbatch size 150\nmaximum passage length 300\nmaximum query length at initial hop 70\nmaximum query length at 2nd hop 350\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 50\nweight decay 0\n\n\nTable 12: Hyperparameters of Extractive Reader (ELECTRA)\n\n\nlearning rate 5e-5\nbatch size 128\nmaximum sequence length 512\nmaximum answer length 30\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 7\nweight decay 0\n# of negative context per question 5\nweight of SP sentence prediction loss 0.025\n\n\n16\n\n\n\nTable 10: Answer EM using top 50 retrieved passage chains\n\n\n\nComp Bridge\nModel Overall\n(20%) (80%)\n\n\n\nELECTRA 61.7 79.0 57.4\n\nFiD 61.7 75.3 58.3\n\n\nPublished as a conference paper at ICLR 2021\n\n\nB.2 FURTHER DETAILS ABOUT READER MODELS\n\n\nB.2.1 EXTRACTIVE READER\n\n\nThe extractive reader is trained with four loss functions. With the [CLS] token, we predict a\nreranking score based on whether the passage sequence match the groundtruth supporting passages.\nOn top of the representation of each token, we predict a answer start score and answer end score.\nFinally, we prepend each sentence with the [unused0] special token and predict whether the\nsentence is one of the supporting sentences using the representations of the special token. At training\ntime, we pair each question with 1 groundtruth passage sequence and 5 negative passage sequence\nwhich do not contain the answer. At inference time, we feed in the top 250 passage sequences from\nMDR. We rank the predicted answer for each sequence with a linear combination of the reranking\nscore and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is"],"8":[" only receive the question\/claim as input without access to support\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\nan external environment to support reasoning.\n\n\n**Action Space** We design a simple Wikipedia web API with three types of actions to support\ninteractive information retrieval: (1) **search** [entity], which returns the first 5 sentences from\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\nWikipedia search engine, (2) **lookup** [string], which would return the next sentence in the page\ncontaining string, simulating Ctrl+F functionality on the browser. (3) **finish** [answer], which\nwould finish the current task with answer. We note that this action space mostly can only retrieve a\nsmall part of a passage based on exact passage name, which is significantly weaker than state-of-theart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\nand force models to retrieve via explicit reasoning in language.\n\n\n3.2 METHODS\n\n\n**ReAct Prompting** For HotpotQA and Fever, we randomly select 6 and 3 cases [2] from the training\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\nthought), where free-form thoughts are used for various purposes. Specifically, we use a combination\n\n- f thoughts that decompose questions (\u201cI need to search x, find y, then find z\u201d), extract information\nfrom Wikipedia observations (\u201cx was started in 1844\u201d, \u201cThe paragraph does not tell x\u201d), perform\ncommonsense (\u201cx is not y, so z must instead be...\u201d) or arithmetic reasoning (\u201c1844 < 1989\u201d), guide\n\n\n2We find more examples do not improve performance.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n**HotpotQA** **Fever**\n**Prompt Method** _[a]_\n(EM) (Acc)\n\n\nStandard 28.7 57.1\nCoT (Wei et al., 2022) 29.4 56.3\nCoT-SC (Wang et al., 2022a) 33.4 60.4\n\n\nAct 25.7 58.9\n\nReAct ","_ and _Multi-evidence FEVER_ . HotpotQA (Yang et al., 2018) includes 113k multi-hop questions. Unlike other multi-hop QA\ndatasets (Zhang et al., 2018; Talmor & Berant, 2018; Welbl et al., 2018), where the information\nsources of the answers are knowledge bases, HotpotQA uses documents in Wikipedia. Thus, its\nquestions are not restricted by the fixed KB schema and can cover more diverse topics. Each\nquestion in HotpotQA is also provided with ground truth support passages, which enables us to\nevaluate the intermediate retrieval performance. Multi-evidence FEVER includes 20k claims from\nthe FEVER (Thorne et al., 2018) fact verification dataset, where the claims can only be verified using\nmultiple documents. We use this dataset to validate the general applicability of our method.\n\n\n**Implementation Details** All the experiments are conducted on a machine with 8 32GB V100\nGPUs. Our code is based on Huggingface Transformers (Wolf et al., 2019). Our best retrieval results\nare predicted using the exact inner product search index (IndexFlatIP) in FAISS (Johnson et al., 2017).\n\n\n3\n\n\nPublished as a conference paper at ICLR 2021\n\n\nTable 1: Retrieval performance in recall at _k_ retrieved passages and precision\/recall\/F1.\n\n\nHotpotQA FEVER\nMethod\nR@2 R@10 R@20 Precision Recall F1\n\n\nTF-IDF 10.3 29.1 36.8 14.9 28.2 19.5\n\nTF-IDF + Linked 17.3 50.0 62.7 18.6 35.8 24.5\n\nDrKIT 38.3 67.2 71.0         -         -         Entity Linking          -          -          - 30.6 53.8 39.0\n\n\nMDR **65.9** **77.5** **80.2** **45.7** **69.1** **55.0**\n\n\nBoth datasets assume 2 hops, so we fix _n_ = 2 for all experiments. Since HotpotQA does not provide\nthe order of the passage sequences, as a heuristic, we consider the passage that includes the answer\n",". The answer relevance score, AR,\nfor question _q_ is then computed as:\n\n\n\nThis metric evaluates how closely the generated\nanswer aligns with the initial question or instruction.\n\n\n**Context relevance** The context _c_ ( _q_ ) is considered relevant to the extent that it exclusively contains information that is needed to answer the question. In particular, this metric aims to penalise the\n\n\n\nAR = [1]\n\n\n_n_\n\n\n\n_n_\n\n- sim( _q, qi_ ) (1)\n\n\n_i_ =1\n\n\ninclusion of redundant information. To estimate\n\ncontext relevance, given a question _q_ and its context _c_ ( _q_ ), the LLM extracts a subset of sentences,\n_Sext_, from _c_ ( _q_ ) that are crucial to answer _q_, using\nthe following prompt:\n\n\n_Please extract relevant sentences from_\n_the provided context that can potentially_\n_help answer the following question. If no_\n_relevant sentences are found, or if you_\n_believe the question cannot be answered_\n_from the given context, return the phrase_\n_\"Insufficient Information\". While extract-_\n_ing candidate sentences you\u2019re not al-_\n_lowed to make any changes to sentences_\n_from given context._\n\n\nThe context relevance score is then computed as:\n\n\nnumber of extracted sentences\nCR = (2)\ntotal number of sentences in _c_ ( _q_ )\n\n\n**4** **The WikiEval Dataset**\n\n\nTo evaluate the proposed framework, we ideally\nneed examples of question-context-answer triples\nwhich are annotated with human judgments. We\ncan then verify to what extent our metrics agree\nwith human assessments of faithfulness, answer\n\nrelevance and context relevance. Since we are not\n\naware of any publicly available datasets that could\nbe used for this purpose, we created a new dataset,\nwhich we refer to as _WikiEval_ [4] . To construct the\n\ndataset, we first selected 50 Wikipedia pages covering events that have happened since the start of\n2022 [5] . In selecting these pages, we prioritised\nthose with recent edits. For each of the 50 pages,\nwe then asked ChatGPT to suggest a question that\ncan be answered based on the introductory section\n\n- f the page, using the following prompt:\n\n\n_Your task is to formulate a question from_\n_given context satisfying the rules given_\n\n_below:_\n\n_1. The question should be fully answered_\n_from the given","**\n\n\nBoth datasets assume 2 hops, so we fix _n_ = 2 for all experiments. Since HotpotQA does not provide\nthe order of the passage sequences, as a heuristic, we consider the passage that includes the answer\nspan as the final passage. [2] In \u00a73.1.3, we show that the order of the passages is important for effective\nretriever training. The hyperparameters can be found in Appendix B.1.\n\n\n3.1 EXPERIMENTS: RETRIEVAL\n\n\nWe evaluate our multi-hop dense retriever (MDR) in two different use cases: _direct_ and _reranking_,\nwhere the former outputs the top- _k_ results directly using the retriever scores and the latter applies a\ntask-specific reranking model to the initial results from MDR.\n\n\n3.1.1 DIRECT\n\n\nWe first compare MDR with several efficient retrieval methods that can directly find the top- _k_ passage\nsequences from a large corpus, including TF-IDF, TF-IDF + Linked, DrKIT and Entity Linking. **TF-**\n**IDF** is the standard term-matching baseline, while **TF-IDF + Linked** is a straightforward extension\nthat also extracts the hyperlinked passages from TF-IDF passages, and then reranks both TF-IDF\nand hyperlinked passages with BM25 [3] scores. **DrKIT** (Dhingra et al., 2020) is a recently proposed\ndense retrieval approach, which builds a entity-level (mentions of entities) dense index for retrieval.\nIt relies on hyperlinks to extract entity mentions and prunes the search space with a binary mask\nthat restricts the next hop to using hyperlinked entities. On FEVER, we additionally consider an\nentity linking baseline (Hanselowski et al., 2018) that is commonly used in existing fact verification\npipelines. This baseline first uses a constituency parser to extract potential entity mentions in the fact\nclaim and then uses the MediaWiki API to search documents with titles that match the mentions.\n\n\nTable 1 shows the performance of different retrieval methods. On HotpotQA the metric is recall at the\ntop _k_ paragraphs [4], while on FEVER the metrics are precision, recall and F1 in order to be consistent\nwith previous results. On both datasets, MDR substantially outperforms all baselines.\n\n\n3.1."," 48.4 _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nReAct _\u2212_ | _\u2212_ 35.1 | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nSelfAsk _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ 40.1 | _\u2212_ 15.2 | _\u2212_\n\nDecomP _\u2212_ | 50.0 _\u2212_ | _\u2212_ _\u2212_ | 59.3 _\u2212_ | _\u2212_\n\n\nIRCoT QA **45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8**\n\n\nTable 1: Comparison with other LLM-based ODQA\nsystems on EM and F1 scores. \u2018 _\u2212_ \u2019: score is unavailable. HpQA [Br] : Bridge questions subset of HotpotQA.\nMQ [2H] : MuSiQue 2-hop questions. IRCoT QA with\nGPT3 (ours) outperforms other systems by a large margin. Note: Comparisons aren\u2019t head-to-head as discussed in the text. App. \u00a7C reports updated SOTA numbers, including contemporaneous and newer works.\n\n\nAs shown in Table 1, IRCoT QA significantly\n\n- utperforms all of these recent systems by a large\nmargin, setting a new state of the art in terms of\nwhat\u2019s achievable via retrieval-augmented LLMs\n(without supervised training).\n\n\n**6** **Conclusions**\n\n\nChain-of-thought prompting has significantly improved LLMs\u2019 ability to perform multi-step reason\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nThe tap dancing\nsoloist in My\nFeet Keep\nDancing also\nstarred in what\n\n1935 MGM\n\nmusical?\n\n\nWho is the\n\nfather-in-law\n\n- f Queen Hyojeong?\n\n\nWhat is the\n\nname - f the\n\ncastle in the\ncity where the\nperformer - f\nA Collection\n\n1984\u20131989 was\n\nborn?\n\n\n\n_\u21d2_ The tap dancing\nsoloist in My Feet\nKeep Dancing is Fayard\nNicholas.\n_\u21d2_ Fayard Nicholas\nstarred in the 1935\n\nM"],"9":[" by the decomposed system. These errors\ncould be potentially avoided if the model has access to the full information in the original question or\nprevious hop results. The important clue for correctly retrieving the documents or avoiding errors is\nmarked in blue. Once decomposed, the marked information are not longer available in one of the\ndecomposed retrieval hop.\n\n\n**Multi-hop Question** : What is the birthday of the author of \u201dShe Walks These Hills\u201d?\n**Decomposed Questions** :\n1. Who is the author of She Walks These Hills?\n2. What is the birthday of Sharyn McCrumb?\n**Ground-truth SP Passages** :\nShe Walks These Hills: She Walks These Hills is a book written by Sharyn McCrumb and\npublished by Charles Scribner\u2019s Sons in 1994, which later went on to win the Anthony Award\nfor Best Novel in 1995.\nSharyn McCrumb: Sharyn McCrumb (born February 26, 1948) is an American writer whose\n~~books celebrate th~~ e history and folklore of Appalachia. McCrumb is the winner of numerous\nliterary awards...\n**Decomposed Error Case:**\n1. She Walks These Hills (\u2713)\n2. Tane McClure\u00b4 : Tan\u00b4e M. McClure (born June 8, 1958) is an American singer and actress.\n\n\n**Multi-hop Question:** When was the album with the song Unbelievable by American rapper\nThe Notorious B.I.G released?\n**Decomposed Questions:**\n1. What is the album with the song Unbelievable by American rapper The Notorious B.I.G?\n2. When was the album Ready to Die released?\n**Ground-truth SP Passages:**\nUnbelievable (The Notorious B.I.G. song): Unbelievable is a song by American rapper The\n~~Notorious B.I.G., recorded for his debut st~~ udio album Ready to Die...\nReady to Die: Ready to Die is the debut studio album by American rapper The Notorious B.I.G.;\n~~it was release~~ d on September 13, 1994, by Bad Boy Records and Arista Records...\n**Decomposed Error Case:**\n1. Unbelievable (The Notorious B.I.G. song) (\u2713)\n2. Ready to Die (The Stooges album"," James Paris Lee is best known for investing the Lee\u2212Metford rifle and another rifle often referred to by what acronymn?\nA: James Paris Lee is best known for investing the Lee\u2212Metford rifle and Lee\u00e2 A\u00b8SEnfield series of rifles. Lee\u00e2 [\u02d8] A\u00b8SEnfield is [\u02d8]\n\n   - ften referred to by the acronym of SMLE. So the answer is: SMLE.\n\n\nQ: Who was born first, James D Grant, who uses the pen name of Lee Child, or Bernhard Schlink?\nA: James D Grant, who uses the pen name of Lee Child, was born in 1954. Bernhard Schlink was born in 1944. Thus,\n\nBernhard Schlink was born first. So the answer is: Bernhard Schlink.\n\n\nQ: Which American neo\u2212noir science fiction has Pierce Gagnon starred?\nA: Pierce Gagnon has starred in One Tree Hill, Looper, Wish I Was Here and Extant. Of these, Looper is an American neo\u2212\n\nnoir science fiction. So the answer is: Looper.\n\n\nQ: What year did Edburga of Minster\u2212in\u2212Thanet's father die?\nA: The father of Edburga of Minster\u2212in\u2212Thanet is King Centwine. Centwine died after 685. So the answer is: after 685.\n\n\nQ: Were Lonny and Allure both founded in the 1990s?\nA: Lonny (magazine) was founded in 2009. Allure (magazine) was founded in 1991. Thus, of the two, only Allure was\n\nfounded in 1990s. So the answer is: no.\n\n\nQ: The actor that stars as Joe Proctor on the series \"Power\" also played a character on \"Entourage\" that has what last name?\n\n\nA: The actor that stars as Joe Proctor on the series \"Power\" is Jerry Ferrara. Jerry Ferrara also played a character on Entourage\n\nnamed Turtle Assante. Thus, Turtle Assante's last name is Assante. So the answer is: Assante.\n\n\nQ: Nobody Loves You was written by John Lennon and released on what album that was issued by Apple Records, and was\n\nwritten, recorded, and released during his 18 month separation from Yoko Ono?\nA: The album issued","I\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So\n\nthe answer is: 1894.\n\n\nQ: When was Neville A. Stanton's employer founded?\nA: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862.\n\nSo the answer is: 1862.\n\n\nQ: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to?\nA: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located\n\nin the Prince County. So the answer is: Prince County.\n\n\nQ: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America\u2212\n\nLite: How Imperial Academia Dismantled Our Culture's author?\nA: The author of America\u2212Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was\n\neducated at the Yale University. The city in Connecticut that has the highest number of Zagat\u2212rated restaurants is New\nHaven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale\nHerald.\n\n\nQ: What is the headquarters for the organization who sets the standards for ISO 21500?\nA: The standards for ISO 21500 were set by International Organization for Standardization. The International Organization\n\nfor Standardization has headquarters in Geneva. So the answer is: Geneva.\n\n\nQ: What did the publisher of Banjo\u2212Tooie rely primarily on for its support?\nA: The publisher of Banjo\u2212Tooie is Nintendo. Nintendo relied primarily for its support on first\u2212party games. So the answer is:\n\nfirst\u2212party games.\n\n\nQ: In which county was the birthplace of the Smoke in tha City performer?\nA: The performer of Smoke in tha City is MC Eiht. MC Eiht's birthplace is Compton. Compton is located in the county of Los\n\nAngeles County. So the answer is: Los Angeles County.\n\n\nQ: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?\n\n\nA: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central\n\nJakarta.\n\n\nQ:"," the same country. So the answer is: no.\n\n\nQ: Who was born first out of Martin Hodge and Ivania Martinich?\nA: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was born\n\nfirst. So the answer is: Martin Hodge.\n\n\nQ: Which film came out first, The Night Of Tricks or The Genealogy?\nA: The Night of Tricks was published in the year 1939. The Genealogy was published in the year 1979. Thus, The Night of\n\nTricks came out first. So the answer is: The Night Of Tricks.\n\n\nQ: When did the director of film Laughter In Hell die?\nA: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the answer is:\n\nAugust 25, 1963.\n\n\nQ: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\nA: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by Frederick de\n\nCordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus, the person\nto die later from the two is Twenty Plus Two. So the answer is: Twenty Plus Two.\n\n\nQ: Who is Boraqchin (Wife Of \u00c3Ugedei)'s father\u2212in\u2212law? [\u02dd]\nA: Boraqchin is married to \u00c3Ugedei Khan. \u00c3 [\u02dd] Ugedei Khan's father is Genghis Khan. Thus, Boraqchin's father\u2212in\u2212law is [\u02dd]\n\nGenghis Khan. So the answer is: Genghis Khan.\n\n\nQ: What is the cause of death of Grand Duke Alexei Alexandrovich Of Russia's mother?\nA: The mother of Grand Duke Alexei Alexandrovich of Russia is Maria Alexandrovna. Maria Alexandrovna died from\n\n\ntuberculosis. So the answer is: tuberculosis.\n\n\nQ: Which film has the director died earlier, When The Mad Aunts Arrive or The Miracle Worker (1962 Film)?\nA: When The Mad Aunts Arrive was directed by Franz Josef Gottlieb.","\n\n\nPreprint.\n\n\n**Input** What genre is A Question and Answer Guide to Astronomy? (PopQA)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] A Question and Answer Guide to Astronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ **ISSUP** =Fully\nSupported][ **ISUSE** =5]\n\n**Reference [1]** A Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is\nintended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene\nRoy, and published in English by Cambridge University Press in 2010. It was originally written in French. The\ncontent within the book is written using a question and answer format. It contains some 250 questions, which\nThe Science Teacher states each are answered with a concise and well-formulated essay that is informative and\nreadable. The Science Teacher review goes on to state that many of the answers given in the book are little gems\n\n - f science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a\nstudent, and that the answers are informative, well constructed, and thorough. The book covers information about\nthe planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in\nthe Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers,\nand the Moon illusion.\n\n**S&P** Supported and Plausible.\n**Correctness of** **ISREL and** **ISSUP :** Both **ISREL** and **ISSUP** are correct, as the Reference [1] provides\ndetailed information of the book including the genre, and the model output is fully supported by the reference.\n**Input** Who is the author of The Lie? (PopQA)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] The author of The Lie is Sam Harris.[\n**ISSUP** =Fully Supported][ **ISUSE** =5]\n\n**Reference [1]** Lying is a 2011 long-form essay book by American author and neuroscience expert Sam Harris.\nHarris argues that we can radically simplify our lives and improve society"],"10":["downstream models, our system can match the best published result while being 10x faster.\n\n\n2 METHOD\n\n\n2.1 PROBLEM DEFINITION\n\n\nThe retrieval task considered in this work can be described as follows (see also Figure 1). Given a\nmulti-hop question _q_ and a large text corpus _C_, the retrieval module needs to retrieve a sequence of\npassages _Pseq_ : _{p_ 1 _, p_ 2 _, ..., pn}_ that provide _sufficient_ information for answering _q_ . Practically, the\nretriever returns the _k_ best-scoring sequence candidates, _{Pseq_ [1] _[,][ P]_ _seq_ [2] _[, ...,][ P]_ _seq_ _[k]_ _[}]_ [ (] _[k][ \u226a|C|]_ [), with the]\nhope that at least one of them has the desired qualities. _k_ should be small enough for downstream\nmodules to process in a reasonable time while maintaining adequate recall. In general, retrieval also\nneeds to be efficient enough to handle real-world corpora containing millions of documents.\n\n\n2.2 MULTI-HOP DENSE RETRIEVAL\n\n\n**Model** Based on the sequential nature of the multi-hop retrieval problem, our system solves it in an\niterative fashion. We model the probability of selecting a certain passage sequence as follows:\n\n\n\n_P_ ( _Pseq|q_ ) =\n\n\n\n_n_\n\n- _P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) _,_\n\n\n_t_ =1\n\n\n2\n\n\nPublished as a conference paper at ICLR 2021\n\n\nwhere for _t_ = 1, we only condition on the original question for retrieval. At each retrieval step, we\nconstruct a new query representation based on previous results and the retrieval is implemented as\nmaximum inner product search over the dense representations of the whole corpus:\n\n\nexp ( _\u27e8_ _**p**_ _t,_ _**q**_ _t\u27e9_ )\n_P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) =\n\n~~\ufffd~~ _p\u2208C_ [exp (] _[\u27e8]_ _**[p]**_ _[,]_ _**[ q]**_ _[t][\u27e9]_ [)] _[,]_"," a\nstraightforward way of expressing information\nneeds for retrieval is to generate \u201c[Search(query)]\u201d\nwhen additional information is needed (Schick\net al., 2023), e.g., \u201cThe colors on the flag of\nGhana have the following meanings. Red is for\n\n[Search(Ghana flag red meaning)] the blood of martyrs, ...\u201d When working with GPT-3.5 models that\n\n\nSearch results:  !!\n\n[2]: \u2026\n\n\n\nretrieval instructions might not be reliable. Therefore, we propose a more direct way of forwardlooking active retrieval that uses the next sentence\nto decide when and what to retrieve.\n\n\n**3.2.1** **Confidence-based Active Retrieval**\n\n\n\n\n\n_Retriever_\n\n\n\n\n\n\n\n$\n\n\n&#\n\n\n%$\n\n\n&$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%\n\n\n&%\n\n\n\n\n\n\n\nFigure 2: An illustration of forward-looking active retrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in _gray italic_ ) to retrieve relevant information to\naid future generations.\n\n\n- ffer only API access, we elicit such behavior by\nfew-shot prompting (Brown et al., 2020).\nSpecifically, for a downstream task, we place\nthe search-related instruction and exemplars at the\nbeginning as skill 1, followed by the instruction and\nexemplars of the downstream task as skill 2. Given\na test case, we ask LMs to combine skills 1 and 2 to\ngenerate search queries while performing the task.\nThe structure of the prompt is shown in Prompt 3.1,\nand full details can be found in Prompt D.3.\n\n\n\nAs shown in Figure 1, at step _t_, we first generate a\ntemporary next sentence \u02c6 _**s**_ _t_ = LM([ _**x**_ _,_ _**y**_ _<t_ ]) with\n- ut conditioning on retrieved documents. Then we\ndecide whether to trigger retrieval and formulate\nqueries based on \u02c6 _**s**_ _t_ . If the LM is confident about \u02c6 _**s**_ _t_,\nwe accept it without retrieving additional information; if not, we use \u02c6 _**s**_ _t_ to formulate search queries\n_**q**_ _t_ to retrieve relevant documents, and then regenerate the next sentence _**s**_","\n\n\n[6https:\/\/github.com\/aurelio-labs\/semantic-router](https:\/\/github.com\/aurelio-labs\/semantic-router)\n[7https:\/\/huggingface.co\/spaces\/mteb\/leaderboard](https:\/\/huggingface.co\/spaces\/mteb\/leaderboard)\n\n\n\n9\n\n\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)\ncan be utilized to learn term weights to enhance sparse\nretrieval. Specifically, it also demonstrates that sparse retrieval\nmodels can enhance the zero-shot retrieval capability of dense\nretrieval models and assist dense retrievers in handling queries\ncontaining rare entities, thereby improving robustness.\n_2) Fine-tuning Embedding Model:_ In instances where the\ncontext significantly deviates from pre-training corpus, particularly within highly specialized disciplines such as healthcare,\nlegal practice, and other sectors replete with proprietary jargon,\nfine-tuning the embedding model on your own domain dataset\nbecomes essential to mitigate such discrepancies.\nIn addition to supplementing domain knowledge, another\npurpose of fine-tuning is to align the retriever and generator,\nfor example, using the results of LLM as the supervision signal\nfor fine-tuning, known as LSR (LM-supervised Retriever).\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\ngenerator to create task-specific retrievers, addressing challenges in supervised fine-tuning, particularly in data-scarce\ndomains. Another approach, LLM-Embedder [97], exploits\nLLMs to generate reward signals across multiple downstream\ntasks. The retriever is fine-tuned with two types of supervised\nsignals: hard labels for the dataset and soft rewards from\nthe LLMs. This dual-signal approach fosters a more effective\nfine-tuning process, tailoring the embedding model to diverse\ndownstream applications. REPLUG [72] utilizes a retriever\nand an LLM to calculate the probability distributions of the\nretrieved documents and then performs supervised training\nby computing the KL divergence. This straightforward and\neffective training method enhances the performance of the\nretrieval model by using an LM as the supervisory signal,\neliminating the need for specific cross-attention mechanisms.\nMoreover, inspired by RLHF (Reinforcement Learning from\nHuman Feedback), utilizing LM-based feedback to reinforce\nthe retriever through reinforcement learning.\n\n\n_E. Adapter_"," its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily\nbetter at using their input context (\u00a72.3).\nGiven that language models struggle to retrieve\nand use relevant information in the multi-document\n\nquestion answering task, to what extent can language models even _retrieve_ from their input contexts? We study this question with a synthetic keyvalue retrieval task, which is designed to be a minimal testbed for the basic ability to retrieve matching\ntokens from the input context. In this task, models\nare given a collection of JSON-formatted key-value\npairs and must return the value associated with a\nspecific key. Similar to the multi-document QA\ntask, the key-value retrieval task admits controlled\nchanges to the input context length (adding more\nkey-value pairs) and the position of relevant information. Although some models perform the\nsynthetic key-value retrieval task perfectly, other\nmodels struggle to simply retrieve matching tokens\nthat occur in the middle of their input context and\ncontinue to exhibit a U-shaped performance curve.\nTo better understand why language models struggle to robustly access and use information in their\ninput contexts, we study the role of model architecture (decoder-only vs. encoder-decoder), queryaware contextualization, and instruction fine-tuning\n(\u00a74). We find that:\n\n\n  - Encoder-decoder models are relatively robust\nto changes in the position of relevant information within their input context, but only when\nevaluated on sequences within its trainingtime sequence length. When evaluated on\nsequences longer than those seen during training, we observe a U-shaped performance\ncurve (\u00a74.1).\n\n\n  - Query-aware contextualization (placing the\nquery before _and_ after the documents or keyvalue pairs) enables near-perfect performance\n\n   - n the synthetic key-value task, but minimally\nchanges trends in multi-document QA (\u00a74.2).\n\n\n  - Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of\nrelevant information in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that",". To condition language models on these retrieved documents, we simply include them in the\nprompt. We evaluate retriever recall and reader\naccuracy (whether any of the annotated answers\nappear in the predicted output) as a function of the\nnumber of retrieved documents _k_ . We use a subset\n\n- f NaturalQuestions-Open where the long answer\nis a paragraph (as opposed to a table or a list).\nFigure 11 presents retriever recall and open\n\nFigure 11: Retriever recall and model performance as a\nfunction of the number of retrieved documents. Model\n\nperformance saturates long before retriever recall, indicating that the models have difficulty making use of the\nextra retrieved documents.\n\n\ndomain QA results. We see that reader model\nperformance saturates long before retriever performance saturates, indicating that readers are not\neffectively using the extra context. Using more\nthan 20 retrieved documents only marginally improves reader performance ( _\u223c_ 1.5% for GPT-3.5Turbo and _\u223c_ 1% for Claude-1.3), while significantly\nincreasing the input context length (and thus latency and cost). These results, coupled with the\n\n- bservation that models are often better at retriev\ning and using information at the start or end of\nthe input contexts, suggest that effective reranking of retrieved documents (pushing relevant information closer to the start of the input context) or\nranked list truncation (retrieving fewer documents\nwhen appropriate; Arampatzis et al., 2009) may be\npromising directions for improving how languagemodel-based readers use retrieved context.\n\n\n**6** **Related Work**\n\n\n**6.1** **Long-Context Language Models**\n\n\nThere is much prior work in designing performant\nlanguage models with cheaper scaling than Transformers in the context length. Many lines of work\npursue Transformer variants with attention modifications like recurrence (Dai et al., 2019), factorizing attention into computationally less intensive\napproximations (Beltagy et al., 2020; Zaheer et al.,\n2020), or low-rank approximations (Wang et al.,\n2020; Peng et al., 2021). Dao et al. (2022) instead provide a faster exact attention by a carefully\n\n\ncrafted IO-aware CUDA kernel. Separately, there\nare attempts to do away with"],"11":[".:\nToward multimodal image-to-image translation. In: Advances in Neural Information\nProcessing Systems (2017) 3\n91. Zontak, M., Irani, M.: Internal statistics of a single natural image. In: IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) (2011) 7\n\n\n**Appendix A** **Additional Image-to-Image Results**\n\n\nWe first show additional, randomly selected results on datasets used in our main\npaper. We then show results on additional datasets.\n\n\n**A.1** **Additional comparisons**\n\n\nIn Figure 10, we show additional, randomly selected results for Horse _\u2192_ Zebra\nand Cat _\u2192_ Dog. This is an extension of Figure 3 in the main paper. We compare\nto baseline methods CycleGAN [89], MUNIT [30], DRIT [41], Self-Distance and\nDistanceGAN [4], and GcGAN [18].\n\n\n**A.2** **Additional datasets**\n\n\nIn Figure 11 and Figure 12, we show additional datasets, compared against\nbaseline method CycleGAN [89]. Our method provides better or comparable\nresults, demonstrating its flexibility across a variety of datasets.\n\n_\u2022 Apple\u2192Orange_ contains 996 apple and 1,020 orange images from ImageNet\nand was introduced in CycleGAN [89].\n\n_\u2022 Yosemite Summer\u2192Winter_ contains 1,273 summer and 854 winter images of\nYosemite scraped using the FlickAPI was introduced in CycleGAN [89].\n\n_\u2022 GTA\u2192Cityscapes_ GTA contains 24,966 images [63] and Cityscapes [13] contains\n19,998 images of street scenes from German cities. The task was originally\nused in CyCADA [29].\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 21\n\n\nInput CUT FastCUT CycleGAN MUNIT DRIT DistanceGAN SelfDistGAN GcGAN\n\n\nFig. 10: **Randomly selected Horse** _\u2192_ **Zebra and Cat** _\u2192_ **Dog results** . This is an\nextension of Figure 3 in the main paper.\n\n\n22 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\nInput Ours(idt) CycleGAN Input Ours(idt) CycleGAN\n\n\nFig. 11: **Apple** _\u2192_ **Orange** and **Summer","The Eleventh_\n_International Conference on Learning Representa-_\n_tions_ .\n\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu[taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-](https:\/\/openreview.net\/forum?id=6p3AuaHAFiN)\n[guage models are zero-shot reasoners. In](https:\/\/openreview.net\/forum?id=6p3AuaHAFiN) _ICML 2022_\n_Workshop on Knowledge Retrieval and Language_\n_Models_ .\n\n\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot\nprompting for open-domain question answering.\n_arXiv preprint arXiv:2203.05115_ .\n\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\n[Retrieval-augmented generation for knowledge-](https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/6b493230205f780e1bc26945df7481e5-Paper.pdf)\n[intensive nlp tasks. In](https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/6b493230205f780e1bc26945df7481e5-Paper.pdf) _Advances in Neural Infor-_\n_mation Processing Systems_, volume 33, pages 9459\u2013\n9474. Curran Associates, Inc.\n\n\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. _arXiv_\n_preprint arXiv:2212.08410_ .\n\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Web","27B Suff 26.9% 27.5% 10.8% 23.3% 40.7% 64.1%\n\nInsuff 11.8% 6.9% 7.2% 10.1% 22.7% 37.9%\n\nClaude 3.5 Suff 67.9% 73.1% 48.9% 74.0% 46.3% 66.7%\n\nSonnet Insuff 26.5% 33.3% 19.9% 40.4% 29.0% 38.3%\n\n\nThe Contains Answer metric exhibits several characteristics when compared to LLMEval:\n\n\n1. Different formatting affects matching:\n\n\nQ: What date did the creator of Autumn Leaves die?\nGround Truth: 13 August 1896\nResponse: August 13, 1896.\nContains Answer: False\n\nLLMEval: Correct\n\n\n2. Semantic equivalents are not captured:\n\n\nQ: What former Los Angeles Lakers majority owner is the\nfather of Jeanie Marie Buss?\n\nGround Truth: Gerald Hatten Buss\n\nResponse: Jerry Buss.\nContains Answer: False\n\nLLMEval: Correct\n\n\n3. Partial matches can be marked as correct:\n\n\nQ: What is Amazon Prime Video\u2019s most watched premiere ever?\nGround Truth: The Rings of Power\nResponse: The series explores the forging of the Rings of Power,\nthe rise of Sauron...\n\nContains Answer: True\n\nLLMEval: Hallucinate\n\n\nThe LLM QA evaluator provides several practical advantages:\n\n\n20\n\n\nPublished as a conference paper at ICLR 2025\n\n\n    - Handles variations in model verbosity and formatting\n\n\n    - Distinguishes between correct, abstain, and incorrect responses\n\n\n    - Enables efficient evaluation across multiple datasets\n\n\nOur analysis shows two key findings that are consistent across both metrics: LLMs (i) exhibit\nhallucination even with sufficient context and (ii) struggle to abstain with insufficient context.\n\n\n21\n\n\nPublished as a conference paper at ICLR 2025\n\n\nC PROMPTS\n\n\nC.1 SUFFICIENT CONTEXT AUTORATER PROMPT\n\n\n\n\n\n22\n\n\nPublished as a conference paper at ICLR 2025\n\n\nC.2 FLAME PROMPT\n\n\nC.3 LLMEVAL PROMPT\n\n\nSince the questions in our datasets ask for free form answers, the LLM responses may not exactly\nmatch","doi.org\/10.48550\/arXiv.2304.04358)\n[Learning to generate factually correct articles for](https:\/\/doi.org\/10.48550\/arXiv.2304.04358)\n[queries by grounding on large web corpus.](https:\/\/doi.org\/10.48550\/arXiv.2304.04358) _CoRR_,\nabs\/2304.04358.\n\n\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\n[Liu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-](https:\/\/doi.org\/10.48550\/arXiv.2305.06849)\n[teractive web search for chinese long-form question](https:\/\/doi.org\/10.48550\/arXiv.2305.06849)\n[answering.](https:\/\/doi.org\/10.48550\/arXiv.2305.06849) _CoRR_, abs\/2305.06849.\n\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\n[Dario Amodei, and Ilya Sutskever. 2019. Language](https:\/\/d4mucfpksywv.cloudfront.net\/better-language-models\/language-models.pdf)\n[models are unsupervised multitask learners.](https:\/\/d4mucfpksywv.cloudfront.net\/better-language-models\/language-models.pdf) _OpenAI_\n_Blog_, 1(8).\n\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented language models. _arXiv preprint arXiv:2302.00083_ .\n\n\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\n\n[How much knowledge can you pack into the param-](https:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.437)\n[eters of a language model? In](https:\/\/doi.org\/","\/testing splitting method as in previous\nwork (Lee et al., 2019). Below we briefly describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\n**Natural Questions (NQ)** (Kwiatkowski et al.,\n2019) was designed for end-to-end question answering. The questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identified by annotators.\n**TriviaQA** (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web.\n\n**WebQuestions (WQ)** (Berant et al., 2013) consists\n\n- f questions selected using Google Suggest API,\nwhere the answers are entities in Freebase.\n**CuratedTREC (TREC)** (Baudis and\u02c7 Sediv [\u02c7] y`,\n2015) sources questions from TREC QA tracks\n\n\n5However, Wang et al. (2019) also propose splitting documents into overlapping passages, which we do not find advantageous compared to the non-overlapping version.\n\n\n\n**Dataset** **Train** **Dev** **Test**\n\n\nNatural Questions 79,168 58,880 8,757 3,610\nTriviaQA 78,785 60,413 8,837 11,313\nWebQuestions 3,417 2,474 361 2,032\nCuratedTREC 1,353 1,125 133 694\nSQuAD 78,713 70,096 8,886 10,570\n\n\nTable 1: Number of questions in each QA dataset. The\ntwo columns of **Train** denote the original training examples in the dataset and the actual questions used for\ntraining DPR after filtering. See text for more details.\n\n\nas well as various Web sources and is intended for\n\n- pen-domain QA from unstructured corpora.\n**SQuAD v1.1** (Rajpurkar et al., 2016) is a popular benchmark dataset for reading comprehension.\nAnnotators were presented with a Wikipedia paragraph, and asked to write questions that could be\nanswered from the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison"],"12":[", including very strong modern Transformer training recipes based\n\n - n LLaMa (Touvron et al. 2023). Our Mamba language model has 5\u00d7 generation throughput compared to Transformers\n\n - f similar size, and Mamba-3B\u2019s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common\nsense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\n\n\n[Model code and pre-trained checkpoints are open-sourced at https:\/\/github.com\/state-spaces\/mamba.](https:\/\/github.com\/state-spaces\/mamba)\n\n### **2 State Space Models**\n\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related\nto RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n\n2\n\n\n**Selective State Space Model**\n\n_**with Hardware-aware State Expansion**_\n\n\n\ud835\udc34\n\n\n\n\ud835\udc65!\n\n\n\n\n\n\ud835\udc66!\n\n\n\n\n\n\n\n\n\n\n\n**Selection Mechanism**\n\n|\ud835\udc35 \ud835\udc36<br>! !<br>\u2206<br>!<br>Discretize<br>Project|Col2|\n|---|---|\n|||\n|||\n\n\n\nFigure 1: ( **Overview** .) Structured SSMs independently map each channel (e.g. _\ud835\udc37_ = 5) of an input _\ud835\udc65_ to output _\ud835\udc66_ through a higher\ndimensional latent state _\u210e_ (e.g. _\ud835\udc41_ = 4). Prior SSMs avoid materializing this large effective state ( _\ud835\udc37\ud835\udc41_, times batch size _\ud835\udc35_ and sequence\nlength _\ud835\udc3f_ ) through clever alternate computation paths requiring time-invariance: the (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ) parameters are constant across time. Our\nselection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize\nthe expanded states in more efficient levels of the GPU memory hierarchy.\n\n\n1-dimensional function or sequence _\ud835\udc65_ ( _\ud835\udc61_ ) \u2208 R \u21a6\u2192 _\ud835\udc66_ ( _\ud835\udc61_ ) \u2208 R through an implicit latent state _\u210e_ ( _\ud835\udc61_ )","## Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\nAlbert Gu [\u2217][1] and Tri Dao [\u2217][2]\n\n\n1\nMachine Learning Department, Carnegie Mellon University\n\n\n2\nDepartment of Computer Science, Princeton University\nagu@cs.cmu.edu, tri@tridao.me\n\n\n**Abstract**\n\n\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the\nTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,\ngated convolution and recurrent models, and structured state space models (SSMs) have been developed to address\nTransformers\u2019 computational inefficiency on long sequences, but they have not performed as well as attention on important\nmodalities such as language. We identify that a key weakness of such models is their inability to perform content-based\nreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses\ntheir weakness with discrete modalities, allowing the model to _selectively_ propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though this change prevents the use of efficient\nconvolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a\nsimplified end-to-end neural network architecture without attention or even MLP blocks ( **Mamba** ). Mamba enjoys fast\ninference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves\n\n    - n real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art\nperformance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model\n\n    - utperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream\nevaluation.\n\n### **1 Introduction**\n\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged\nas an effective paradigm in modern machine learning. The backbone of these FMs are often _sequence models_, operating on\narbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and\ngenomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al"," is that the model\u2019s dynamics are constant\nthrough time. In other words (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ), and consequently ( _\ud835\udc68, \ud835\udc69_ ) as well, are fixed for all time-steps. This property is\n\n\n3\n\n\ncalled _linear time invariance (LTI)_, which is deeply connected to recurrence and convolutions. Informally, we think of\nLTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these\nclasses of models.\n\n\nThus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental efficiency constraints,\ndiscussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling\ncertain types of data, and our technical contributions involve removing the LTI constraint while overcoming the efficiency\nbottlenecks.\n\n\n**Structure and Dimensions.** Finally, we note that structured SSMs are so named because computing them efficiently\nalso requires imposing structure on the _\ud835\udc68_ matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022;\nGupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.\n\n\nIn this case, the _\ud835\udc68_ \u2208 R _[\ud835\udc41]_ [\u00d7] _[\ud835\udc41]_ _, \ud835\udc69_ \u2208 R _[\ud835\udc41]_ [\u00d7][1] _, \ud835\udc6a_ \u2208 R [1][\u00d7] _[\ud835\udc41]_ matrices can all be represented by _\ud835\udc41_ numbers. To operate over an input\nsequence _\ud835\udc65_ - f batch size _\ud835\udc35_ and length _\ud835\udc3f_ with _\ud835\udc37_ channels, the SSM is applied independently to each channel. Note that in\nthis case, the total hidden state has dimension _\ud835\udc37\ud835\udc41_ per input, and computing it over the sequence length requires _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ )\ntime and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3.\n\n\n**General State Space","\ud835\udc68_ is manually set to 0,\ninstead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically\nto this setting and probe if the model has learned to automatically reset its state on episode boundaries.\n\n\n- Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of\nbeing an exponential moving average (EMA). They additionally make an interesting connection of the discretization step\n\n - f SSMs to an EMA _damping_ term. Contrary to findings in the original S4 papers, this was the first model to show that\nreal-valued SSMs are empirically effective in certain settings or when combined with different architectural components.\n\n\n- Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this\nperspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally\nand close to LTI.\n\n\n- SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox\n\n2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of\nS4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do\nfast autoregressive inference directly.\n\n\nNotably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually\nstrictly LTI (linear time invariant).\n\n\n**B.2** **SSM Architectures**\n\n\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating\n\n- ne of the previous SSMs as a black box layer.\n\n\n- GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated\nattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most\nimportantly, its projection","phabetical by first name.\n\n\n1\n\n\nArena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu,\nand Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been\nsuccessful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022;\nSaon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such\n\nas text.\n\n\nWe propose a new class of **selective state space models**, that improves on prior work on several axes to achieve the\nmodeling power of Transformers while scaling linearly in sequence length.\n\n\n**Selection Mechanism.** First, we identify a key limitation of prior models: the ability to efficiently _select_ data in an\ninput-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic\ntasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM\nparameters based on the input. This allows the model to filter out irrelevant information and remember relevant information\nindefinitely.\n\n\n**Hardware-aware Algorithm.** This simple change poses a technical challenge for the computation of the model; in\nfact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this\nwith a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not\nmaterialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The\nresulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to\npseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3\u00d7 faster on A100 GPUs).\n\n\n**Architecture.** We simplify prior deep sequence model architectures by combining the design of prior SSM architectures\n(Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single"],"13":[" \u201cPotter\u201d by copying from history.\n\n\n**Dataset.** We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is\ncomparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We additionally investigate\ngeneralization and extrapolation abilities by evaluating on a range of sequence lengths from 2 [6] = 64 up to 2 [20] = 1048576 at\ntest time.\n\n\n**Models.** Following established work on induction heads, we use 2 layer models, which allows attention to mechanistically\nsolve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads, with various positional\nencodings) and SSM variants. We use a model dimension _\ud835\udc37_ - f 64 for Mamba and 128 for the other models.\n\n\n**Results.** Table 2 shows that Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the task perfectly\nbecause of its ability to selectively remember the relevant token while ignoring everything else in between. **It generalizes**\n**perfectly to million-length sequences, or** 4000\u00d7 **longer than it saw during training**, while no other method goes\nbeyond 2\u00d7.\n\n\n10\n\n\nModel Arch. Layer Acc.\n\n\nS4 No gate S4 18.3\n\n  - No gate S6 **97.0**\n\n\nH3 H3 S4 57.0\n\nHyena H3 Hyena 30.1\n\n  - H3 S6 **99.7**\n\n\n  - Mamba S4 56.4\n\n  - Mamba Hyena 28.4\nMamba Mamba S6 **99.8**\n\n\nTable 1: ( **Selective Copying** .)\nAccuracy for combinations of architectures\nand inner sequence layers.\n\n\n\n\n\n\n\n\n\n|Induction Heads Extrapolation|Col2|Col3|nduction He|eads Extrapolation|n|\n|---|---|---|---|---|---|\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<","HA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|2<br>|3|4|5|\n\n\nTable 2: ( **Induction Heads** .) Models are trained on sequence length 2 [8] =\n256, and tested on increasing sequence lengths of 2 [6] = 64 up to 2 [20] =\n1048576. Full numbers in Table 11.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: ( **Scaling Laws** .) Models of size \u2248 125 _\ud835\udc40_ to \u2248 1 _._ 3 _\ud835\udc35_ parameters, trained on the Pile. Mamba scales better than all other\nattention-free models and is the first to match the performance of a very strong \u201cTransformer++\u201d recipe that has now become standard,\nparticularly as the sequence length grows.\n\n\nOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly\nbetter than the others; also note that all attention models were only tested up to sequence length 2 [14] = 16384 due to\nmemory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the findings in Poli et al. (2023).\n\n\n**4.2** **Language Modeling**\n\n\nWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on both\npretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to mirror GPT3\nspecifications. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown\net al. (2020). All training details are in Appendix E.2.\n\n\n**","aving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an MLP) with\nstandard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondingly\nincreased by 1 _._ 5\u00d7 to preserve parameter count.\n\n\n- **H3++** : The H3 architecture with a few modifications, including (i) using the same \u201cthin\u201d Hyena dimensions above (ii) the\nimproved training recipe above (iii) a linear attention _head dimension_  - f 8.\n\n\n- **RWKV** : The default RWKV model from B. Peng et al. (2023), including its modified MLP block. We also used as much of\nits specified training recipe as possible, such as increasing the learning rates by 2\u00d7 or 3\u00d7 on certain parameters.\n\n\n- **RetNet** : The default RetNet model from Y. Sun et al. (2023). We also gave it the improved training recipe above.\n\n\n- **Mamba** : The standard Mamba architecture, with the improved training recipe.\n\n\n**E.2.2** **Additional Scaling Law Ablations**\n\n\nWe perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws in\nFigure 4 ( _Left_ ).\n\n\n**Mamba Architecture: Interleaving Blocks.** We test the effect of different architectural blocks combined with the\nMamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra\nconv \u2192 SSM path added. This leads to two natural ablations:\n\n\nWhat if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also be\ninterpreted as taking Mamba and removing half of the SSMs.\n\n\n30\n\n\nFigure 9: ( **Scaling laws: extra ablations** .) ( _Left_ ) Instead of ( _Right_ ) Instead of\n\n\nWhat if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking\na Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks.\n\n\nFigure 9 ( _Right_ ) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neither"," a total of 10 _\ud835\udc35_ tokens.\n\n\n**Results.** Figure 5 ( _Left_ ) shows that Mamba\u2019s pretraining perplexity improves smoothly with model size, and that Mamba\nscales better than both HyenaDNA and Transformer++. For example, at the largest model size of \u2248 40 _\ud835\udc40_ parameters, the\ncurve shows that **Mamba can match the Transformer++ and HyenaDNA models with roughly** 3\u00d7 **to** 4\u00d7 **fewer**\n**parameters** .\n\n\n**4.3.2** **Scaling: Context Length**\n\n\nIn the next DNA experiment, we investigate the scaling properties of models with respect to sequence length. We only\ncompare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at longer sequence\nlengths. We pretrain models on sequence lengths 2 [10] = 1024, 2 [12] = 4096, 2 [14] = 16384, 2 [16] = 65536, 2 [18] = 262144,\n2 [20] = 1048576. We fix a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models were trained for 20 _\ud835\udc3e_\ngradient steps for a total of \u2248 330 _\ud835\udc35_ tokens. The longer sequence lengths used sequence length warmup similar to (Nguyen,\nPoli, et al. 2023).\n\n\n**Results.** Figure 5 ( _Right_ ) shows that **Mamba is able to make use of longer context even up to extremely long**\n**sequences of length 1M**, and its pretraining perplexity improves as the context increases. On the other hand, the\nHyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on properties of the\nselection mechanism. In particular, LTI models cannot selectively ignore information; from a convolutional perspective, a\nvery long convolution kernel is aggregating all information across a long sequence which may be very noisy. Note that\nwhile HyenaDNA claims to improve with longer context, their results do not control for computation time.\n\n\n**4.3.3** **Synthetic Species Classification**\n\n\nWe evaluate models on a downstream task of classifying between 5 different species by","2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|nduction He|ads Extrapolation||\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|\n|2<br>3<br>4<br>5<br>0"],"14":["aca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryasov and Tastet, 2023) IF 10M-word BabyLM dataset GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S\nMiniLLM (Gu et al., 2024) IF Dolly Dataset GPT2 + OPT + LLaMA GPT2 + OPT + LLaMA Feature D&S\nSelf-Align (Sun et al., 2024b) IF Human-written Principles LLaMA LLaMA Expansion + Self-Knowledge SFT\nSelf-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL\nSTaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT\nLlama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT\nReflection-Tuning (Li et al., 2023e) IF Alpaca\/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nSelective Reflection-Tuning (Li et al., 2024d) IF Alpaca\/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nVicuna (Chiang et al., 2023) IF\/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT\nKoala (Geng et al., 2023) IF\/MD Human Conversation ChatGPT LLaMA Labeling SFT\nBaize (Xu et al., 2023b) IF\/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT\nUltraChat (Ding et al., 2023b) IF\/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT\nOrca (Mukherjee et al., 2023) IF\/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al"," predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach. We also find that ARES consistently\ndistinguishes competitive RAG systems that are\n\n- nly a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop\n\n\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\n[available on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**2** **Related Work**\n\n\nRAG (Guu et al., 2020; Lewis et al., 2020; Khattab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combining them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual information (Shuster et al., 2021; Huo et al., 2023), and\n\n- ffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essential for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\n\n- ut of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. However, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min"," Codes ChatGPT LLaMa Curation SFT\nWaveCoder (Yu et al., 2024) Code Existing Source Codes GPT4 LLaMa Curation SFT\nCode Alpaca (Chaudhary, 2023) Code Code Instructions ChatGPT LLaMA Expansion + Self-Knowledge SFT\nCode Llama (Rozi`ere et al., 2023) Code Human-written Instructions LLaMA LLaMA Expansion + Self-Knowledge SFT\nCode Clean (Jain et al., 2023) Code Code Datasets ChatGPT LLaMA Labeling SFT\n\n\n_**Multi-Modality**_\n\n\nLLaVA (Liu et al., 2023e) Vision-Language COCO GPT4 LLaMA Labeling SFT\nSVIT (Zhao et al., 2023b) Vision-Language Visual Genome + COCO GPT4 LLaMA Labeling SFT\nLVIS-Instruct4V (Wang et al., 2023e) Vision-Language LVIS GPT4V LLaMA Labeling SFT\nLLaVAR (Zhang et al., 2023d) Vision-Language LAION GPT4 LLaMA Labeling SFT\nMacaw-LLM (Lyu et al., 2023) Multiple Modalities Image\/Video with Caption ChatGPT LLaMA Labeling SFT\nMIMIC-IT (Li et al., 2023f) Multiple Modalities Image\/Video Dataset ChatGPT LLaMA Labeling SFT\nChatBridge (Zhao et al., 2023d) Multiple Modalities Task-Specific\/Multimodal-Chat Data GPT4 + ChatGPT LLaMA Labeling SFT\n\n\nTABLE 3: A summary of skill distillation works. IF: Instruction Following, MD: Multi-turn Dialoue, TP: Think Pattern,\nRAG: Retrieval-Augmented Generation, NLU: Natural Language Understanding, NLG: Natural Language Generation, IR:\nInformation Retrieval, SFT: Supervised Fine-Tuning, D&S: Divergence and Similarity, RL: Reinforcement Learning, RO:\nRanking Optimization.\n\n\n\nformats with templates, such as prefacing machine translation data with _\u201dTranslate this sentence to Spanish:\u201d_ . However,\nthese approaches have limitations. Manual data creation is\nlabor-intensive, while template-based","LLM (Qin et al., 2023a) Tool Real-world APIs ChatGPT LLaMA Curation SFT\nMLLM-Tool (Wang et al., 2024) Tool HuggingFace Model Cards GPT4 LLaMA Curation SFT\nFireAct (Chen et al., 2023b) Planning Mixed QA Dataset GPT4 LLaMA Labeling SFT\nAgentTuning (Zeng et al., 2023a) Planning 6 Agent Tasks GPT4 + ChatGPT LLaMA Labeling + Expansion SFT\nLumos (Yin et al., 2023a) Planning Mixed Interactive Tasks GPT4 LLaMA Labeling SFT\nAUTOACT (Qiao et al., 2024) Planning Mixed QA Tasks LLaMA LLaMA Labeling SFT\n\n\n_**NLP Task Specialization**_\n\n\nAugGPT (Dai et al., 2023a) NLU Amazon\/Symptoms\/PubMed20k Dataset ChatGPT BERT Label SFT\nTDG (He et al., 2023b) NLU SST + QQP + MNLI GPT3 BERT Expansion SFT\nSunGen (Gao et al., 2023a) NLU Text Classification Tasks GPT2 DistilBERT Curation SFT\nUDG (Wang et al., 2021a) NLU NLU Tasks GPT3 BERT Expansion SFT\nInheritSumm (Xu et al., 2023c) NLG Pile + ArXiv + CNN\/DM + WikiHow GPT3.5 ZCode++ Label SFT\nDIMSUM+ (Jung et al., 2023) NLG None GPT2 + CTRL + BioGPT T5 Curation + Self-Knowledge SFT\nGenie (Yehudai et al., 2024) NLG ELI5 + ASQA + NQ + CNN\/DM Falcon + LLaMA FLAN + LLaMA Label SFT\nGKD (Agarwal et al., 2024) NLG\/NLU\/IF XSum+WMT14 en-de+GSM8K+FLAN2021 T5-XL T5 Feature + Feedback D&S + RL\nQUILL (Srinivasan et al., 2022) IR IR Datasets"," configuration, in which we\nsampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall\u2019s _\u03c4_ for ARES is\n0.08 higher on average, across both context and answer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides\na more accurate ranking of the RAG systems than\nthe GPT-3.5 judge, averaging a Kendall\u2019s tau 0.06\nhigher over both context relevance and answer relevance. Between the judge configurations, the finetuned LLM judge of ARES can more precisely distinguish between RAG systems and guide configuration decisions surrounding document splitting, retriever selection, and generative LLM choice. However, while the fine-tuned LLM judge had a higher\nKendall\u2019s tau on average, the GPT-3.5 judge is\nmore readily deployable and does not require any\nadditional fine-tuning. The GPT-3.5 judge does\ncome with its own querying costs, which can vary\nbased on the date of querying as well as the total\ntokens used in evaluation.\n\nWe also wanted to better understand the importance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we\n\n\n**ARES Ranking of Pseudo RAG Systems**\n\n\nNQ HotpotQA WoW FEVER MultiRC ReCoRD\n\n\nC.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R.\n\n\nKendall\u2019s Tau for\n0.83 0.89 0.78 0.78 0.78 0.83 **0.89** **0.89** 0.83 0.83 0.72 0.94\nSampled Annotations\n\n\nKendall\u2019s Tau\n0.89 0.89 **0.94** 0.89 0.94 0.94 0.72 0.61 0.83 **0.94** **0.89** 0.44\nfor RAGAS\n\n\nKendall\u2019s Tau\n"],"15":["ves into the student\u2019s\ncapability to align its output with the teacher\u2019s responses.\nMoving forward, _Agent_ underscores the autonomous nature\n\n- f language models. _NLP Task Specialization_ highlights the\nLLM\u2019s versatility in specializing across various Natural\nLanguage Processing tasks, demonstrating its adaptability.\nFinally, _Multi-Modality_ encompasses the knowledge transfer from teacher LLMs to multi-modal models. Table 3\nsummarizes the representative works, encompassing details\nsuch as the skills involved, seed knowledge, teacher LLM,\nstudent model, knowledge elicitation method, and training\n\n- bjectives.\n\n\n**4.1** **Context Following**\n\n\nThis part concentrates on the distillation of context following skills from LLMs. This process involves transferring the\nability of LLMs to handle a variety of complex contexts \u2014\nsuch as few-shot demonstrations, intricate instructions, dialogue history, and retrieval-augmented information \u2014 into\nsmaller models. Many research efforts in this domain aim\nto imbue smaller models with these sophisticated, contextfollowing capabilities. Our discussion here will dissect this\nfacet of skill distillation, categorizing it based on different\ntypes of context and elaborating on how each is distilled\nand incorporated into smaller, efficient models.\n\n\n_4.1.1_ _Instruction Following_\nInstruction-following capacity enables LLMs to understand\nand follow user-given instructions. This ability significantly\nenhances human-AI interaction, allowing for seamless understanding and execution of tasks as directed by users. A\nprimary method for acquiring this skill involves constructing instruction-like prompt-response pairs and employing\nSupervised Fine Tuning (SFT) for model training. Data for\nthis purpose can be manually curated by human experts\n\n- r transformed from existing NLP tasks into instructional\n\n\n\nE\n( _x,yw,yl_ ) _\u223cD_ [(fd)]\n\n\n\nlog _\u03c3_ _\u03b2_ log _[\u03c0][\u03b8]_ [(] _[y][w][|][x]_ [)]\n\n- - _\u03c0_ ref ( _yw|x_\n\n\n\n\n_[\u03c0][\u03b8]_ [(] _[y][w][|][x]_ [)] _[\u03c0][\u03b8]_ [(] _[y][l][|][x]_ [)]\n\n_\u03c0_ ref ( _yw|x_ ) _[\u2212]_ _[\u03b2]_ [ log] _\u03c0_ ref ("," typed as above. We sample new entities uniform randomly from the 5% least common entities\nin NewsCrawl ( _I_ _[RandArg][\u2193]_ ), and the 5% most common ( _I_ _[RandArg][\u2191]_ ). We insert the sampled entities\nwhile preserving the rest of each statement.\n\n\n**4** **Querying Models with Prompts**\n\n\n**4.1** **Models**\n\n\n**LLaMA** is a recent LLM model family which\nrivals or surpasses GPT-3 performance while being\n\n- pen to scientific study. A range of model sizes\n\n\nare provided, and we test the largest **LLaMA-65B**\nmodel. LLaMA is not fine-tuned. In preliminary\nexperiments on the Levy\/Holt dataset, we found\ntwo popular fine-tuned LLaMA variants, Alpaca\n(Taori et al., 2023) and Vicuna (Chiang et al., 2023),\nperform similarly to LLaMA base models and underperform LLaMA-65B, so we leave them out of\nfurther experiments.\n\n\n**GPT-3 Series** models are closed to deep scientific review (Brown et al., 2020), though they are a\nwidely-used comparison for their performance, and\nhave been reasonably well-studied. We evaluate on\n**text-davinci-003 (GPT-3.5)**, as it is the largest, and\nhas undergone instruction- and RLHF-finetuning,\nenabling interesting comparisons.\n\n\n**PaLM** is larger than GPT-3, which often claims\nstate-of-the-art on evaluation datasets. We use the\n\nlargest **PaLM-540B** base model, which is also only\npretrained, so it serves as a further comparison\npoint to LLaMA.\n\n\nLater GPT models (like text-davinci-003 in our\nexperiments) have been pre-trained and fine-tuned,\nwhile base LLaMA and PaLM have only undergone pre-training, so their contrast indicates what\nstage of training is responsible for the phenomena\nwe study. Our aim is not to judge which LLM\nis superior, but to show the common sources of\nhallucination they share.\nWe also omit models superseded in performance\nby LLaMA (e.g. OPT, GPT-J, etc","solving trajectories generated by the model (Act, Thought) and the environment (Obs).\n\n\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\n2022). However, this \u201cchain-of-thought\u201d reasoning is a static black box, in that the model uses\nits own internal representations to generate thoughts and is not grounded in the external world,\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,\nrecent work has explored the use of pre-trained language models for planning and acting in interactive\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\na focus on predicting actions via language priors. These approaches usually convert multi-modal\n\n- bservations into text, use a language model to generate domain-specific actions or plans, and then\nuse a controller to choose or execute them. However, they do not employ language models to reason\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\nand if such a combination can bring systematic benefits compared to reasoning or acting alone.\n\n\nIn this work, we present ReAct, a general paradigm to combine reasoning and acting with language\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\nadjust high-level plans for acting (reason to act), while also interact with the external environments\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\n\n\n2\n\n\nPublished as a conference paper at ICLR 2023\n\n\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\nquestion answering (HotPotQA, Yang et al., 2018), fact verification (Fever"," must be\nable to represent. This has motivated our interest in understanding the limits of what embeddings\ncan represent, as current work expects it to handle _every_ task.\n\n\nPrevious work has explored empirically the limits of models: Reimers and Gurevych [2020] showed\nthat smaller dimension embedding models have more false positives, especially with larger-scale\ncorpora. Ormazabal et al. [2019] showed the empirical limitations of models in the cross-lingual\nsetting and Yin and Shen [2018] showed how embedding dimensions relate to the bias-variance\ntradeoff. In contrast, our work provides a theoretical connection between the embedding dimension\nand the sign-rank of the query relevance ( _qrel_ ) matrix, while also showing empirical limitations.\n\n\n**2.3. Theoretical Limits of Vectors in Geometric Space**\n\n\nUnderstanding and finding nearest neighbors in semantic space has a long history in mathematics\nresearch, with early work such as the Voronoi diagram being studied as far back as 1644 and formalized\nin 1908 [Voronoi, 1908]. The order-k version of the Voronoi diagram (i.e. the Voronoi diagram\n\n\n2You can imagine an easy way to connect any two documents merely by using logical operators, i.e. X and Y.\n\n\n3\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\ndepicting the set of closest k points) is an obvious analog to information retrieval and has been studied\nfor many years [Clarkson, 1988]. However, proofs placing a bound on the count of the number of\nregions in the order-k Voronoi problem are notoriously different to bound tightly and do not provide\nmuch practical insight for IR [Bohler et al., 2015, Lee, 1982, Chen et al., 2023].\n\n\nWe approach this problem from another angle by proving that the set of the constraints implied\nby the top- _\ud835\udc58_ retrieval problem can be formalized to show that it places a lower bound on the\ndimensionality of the embedding needed to represent it. We then show that this dimensionality can be\nmuch larger than the dimensionality of embedding models for practical IR problems. This approach\nrelies on previous work in the communication complexity theory community to place bounds using\nthe sign-rank of a matrix. Due to the difficulty of of computing"," S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\n\u201cWizard of wikipedia: Knowledge-powered conversational agents,\u201d\n_arXiv preprint arXiv:1811.01241_, 2018.\n\n[134] H. Wang, M. Hu, Y. Deng, R. Wang, F. Mi, W. Wang, Y. Wang, W.C. Kwan, I. King, and K.-F. Wong, \u201cLarge language models as source\n\n\nplanner for personalized knowledge-grounded dialogue,\u201d _arXiv preprint_\n_arXiv:2310.08840_, 2023.\n\n[135] \u2014\u2014, \u201cLarge language models as source planner for personalized knowledge-grounded dialogue,\u201d _arXiv preprint arXiv:2310.08840_,\n2023.\n\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, and S. Wang,\n\u201cLong time no see! open-domain conversation with long-term persona\nmemory,\u201d _arXiv preprint arXiv:2203.05797_, 2022.\n\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\nSu, S. Ultes, D. Vandyke, and S. Young, \u201cConditional generation\nand snapshot learning in neural dialogue systems,\u201d _arXiv preprint_\n_arXiv:1606.03352_, 2016.\n\n[138] R. He and J. McAuley, \u201cUps and downs: Modeling the visual evolution\n\n    - f fashion trends with one-class collaborative filtering,\u201d in _proceedings_\n\n_of the 25th international conference on world wide web_, 2016, pp.\n507\u2013517.\n\n[139] S. Li, H. Ji, and J. Han, \u201cDocument-level event argument extraction\nby conditional generation,\u201d _arXiv preprint arXiv:2104.05919_, 2021.\n\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, \u201cMultisentence argument linking,\u201d _arXiv preprint arXiv:1911.03766_, 2019"],"16":[" standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained) would have higher inference\nthroughput than a 5\u00d7 smaller Transformer-1.3B. Details in Appendix E.5, which additionally includes a benchmark of\nmemory consumption.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: ( **Efficiency Benchmarks** .) ( _Left_ ) Training: our efficient scan is 40\u00d7 faster than a standard implementation. ( _Right_ ) Inference:\nas a recurrent model, Mamba can achieve 5\u00d7 higher throughput than Transformers.\n\n\n**4.6** **Model Ablations**\n\n\nWe perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with\nsize \u2248 350M models at Chinchilla token counts (same setting as Figure 4).\n\n\n**4.6.1** **Architecture**\n\n\nTable 6 investigates the effects of the architecture (block) and its inner SSM layer (Figure 3). We find that\n\n\n- Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.\n\n\nReplacing the complex-valued S4 variant from previous work with a real-valued one does not affect performance much,\nsuggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware efficiency.\n\n\nReplacing any of these with a selective SSM (S6) significantly improves performance, validating the motivation of\nSection 3.\n\n\n15\n\n\nTable 6: ( **Ablations: Architecture and SSM layer** .) The Mamba block performs similarly to H3 while being simpler. In the inner layer,\nthere is little difference among different parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More\nspecifically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.\n\n\n\nModel Arch. SSM Layer Perplexity\n\n\nHyena H3 Hyena 10 _._ 24\nH3 H3 S4 (complex) 10 _._ 30\n\n      - H3 S4 (real) 10 _._","The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of D = 64 and\npooling factor 4. The S4+MLP block has roughly 2 _\ud835\udc37_ [2] + 4 _\ud835\udc37_ [2] parameters (expansion factor 2 in the MLP). The Transformer\nblock has 4 _\ud835\udc37_ [2] + 2 _\ud835\udc37_ [2] parameters (expansion factor 1 in the MLP). The Mamba block has the usual \u2248 6 _\ud835\udc37_ [2] parameters. All\nmodels have roughly 6M total parameters.\n\n\n**E.5** **Efficiency Benchmark**\n\n\n**Scan Operation.** We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), against\nconvolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operations\n\n- utside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing the\nQKV projections in attention.\n\n\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing the\nparameters _\ud835\udc68, \ud835\udc69, \ud835\udc6a_ in HBM.\n\n\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the large\nparameters in HBM.\n\n\n35\n\n\nTable 15: ( **Memory benchmark** .) Mamba\u2019s memory footprint is comparable to the most optimized Transformer. Results for 125M\nmodels.\n\n\nBatch size Transformer (w\/ FlashAttention-2) Mamba\n\n\n1 4.6GB 4.8GB\n\n2 5.2GB 5.8GB\n\n4 6.9GB 7.3GB\n\n8 11.5GB 12.3GB\n\n16 20.7GB 23.1GB\n\n32 34.5GB 38.2GB\n\n\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and the\nfilters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexity\nis _\ud835\udc42_ ( _\ud835\udc3f_ log( _\ufffd","Test    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC09 Model Ablations** ) Models with 6M parameters. In\nSaShiMi\u2019s U-Net backbone, there are 8 center blocks operating on\nsequence length 1000, sandwiched on each side by 8 outer blocks on\nsequence length 4000, sandwiched by 8 outer blocks on sequence\nlength 16000 (40 blocks total). The architecture of the 8 center\nblocks are ablated independently of the rest. Note that Transformers\n(MHA+MLP) were not tested in the more important outer blocks\nbecause of efficiency constraints.\n\n\nOuter Center NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\n\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\n\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\n\nMamba MHA+MLP **1.850** 1.37 5.63 58.23 0.62\n\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\n\nMamba Mamba 1.852 **0.94** **6.26** **88.54** **0.52**\n\n\n\nWe benchmark the speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of\n(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6."," the\ncomputation compared to storing them and reading them from HBM.\n\n\nBeyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize the\nmemory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output projection).\nIn particular, we do not save intermediate activations that take a lot of memory but are fast to recompute (e.g. output of\nactivation function or short convolution). As a result, the selective SSM layer has the same memory requirement as an\n\n\n28\n\n\nTable 11: ( **Induction heads** .) Models are trained on sequence length 2 [8] = 256, and tested on various sequence lengths of 2 [6] = 64 up to\n2 [20] = 1048576. \u2713 denotes perfect generalization accuracy, while \u2717 denotes out of memory.\n\n\nModel Params Test Accuracy (%) at Seqence Length\n\n\n2 [6] 2 [7] 2 [8] 2 [9] 2 [10] 2 [11] 2 [12] 2 [13] 2 [14] 2 [15] 2 [16] 2 [17] 2 [18] 2 [19] 2 [20]\n\n\nMHA-Abs 137K \u2713 99.6 100.0 58.6 26.6 18.8 9.8 10.9 7.8 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717\n\nMHA-RoPE 137K \u2713 \u2713 100.0 83.6 31.3 18.4 8.6 9.0 5.5 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717\n\nMHA-xPos 137K \u2713 \u2713 100.0 99.6 67.6 25.4 7.0 9.0 7.8 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717\n\nH3 153K \u2713 \u2713 100.0 80.9 39.5 23.8 14.8 8.2 5.9 6.6 8.2 4.7 8.2 6.3 7.4\nHyena 69M [\u2217] 97.7 \u2713 100.0 \u2713 44","-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5,\nshowing that it is up to 7\u00d7 times faster than attention at sequence length 32K, and is as memory-efficient as the best\nattention implementation (FlashAttention).\n\n\n**Speed.** On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by memorybandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This the case with our\nscan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to significant speedup compared to\na standard implementation.\n\n\nThe standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input _\ud835\udc68, \ud835\udc69_ - f size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in GPU\nHBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel associative scan implementation to\nwrite the scan output of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) to GPU HBM, then multiply that scan output with _\ud835\udc6a_ to produce an output of size\n( _\ud835\udc35, \ud835\udc3f, \ud835\udc37_ ). However, this requires the number of memory reads\/writes on the order of _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ ). We can instead fuse the\ndiscretization step, the scan, and the multiplication with _\ud835\udc6a_ into one kernel:\n\n\n1. We read in _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37_ + _\ud835\udc37\ud835\udc41_ ) bytes of memory (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ) from slow HBM to fast SRAM.\n\n\n2. We discretize to produce _\ud835\udc68, \ud835\udc69_  - f size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in SRAM.\n\n\n3. We perform a parallel associative scan, yielding intermediate states of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in SRAM.\n\n\n4."],"17":[" set the rank to 4 and alpha to 8 for all experiments. The models were\nfine-tuned over 2 epochs with a batch size of 16 and a learning rate of 1 _\u00d7_ 10 _[\u2212]_ [5] . We note that the\ntraining was not smooth, and different checkpoints led to very different results. To be systematic,\nwe chose the best checkpoint in terms of Correct % after either 1 or 2 epochs (where for Musique it\nturned out to be after 1 epoch, and for HotPotQA we found that 2 epochs was better).\n\n\nA.3 DATASETS\n\n\nWe sample 500 examples from HotPotQA and Musique-Ans dev sets, following prior work. We use\nall \u2018True Premise\u2019 questions from FreshQA.\n\n\n**Retrieval for HotpotQA.** We adopt the FlashRAG framework (Jin et al., 2024) to implement our\nRetrieval-Augmented Generation (RAG) process. Our retrieval corpus is based on the wiki-18 dataset,\nutilizing \u2018intfloat\/e5-base-v2\u2018 from Hugging Face\u2019s model hub as a Dense Retriever [3] . For each query,\nwe retrieved the top 5 documents, which are subsequently concatenated with the query and placed\nwithin a prompt template for inference.\n\n\nTo explore advanced retrieval techniques, we also evaluated the REPLUG (Shi et al., 2023b) method.\nREPLUG enhances the generation quality by prepending each retrieved document individually to the\ninput context and ensembling output probabilities across different passes. The REPLUG method is\nalso implemented based on the FlashRAG framework (Jin et al., 2024).\n\n\n1Available at huggingface.co\/mistralai\/Mistral-7B-Instruct-v0.3\n2We use dev set for HotPotQA since the training set had a much different distribution of sufficient context\nexamples. Namely, we found the train set to be over 88% sufficient context, while the dev set was only 44%.\n3huggingface.co\/intfloat\/e5-base-v2\n\n\n15\n\n\nPublished as a conference paper at ICLR 2025\n\n\n**Retrieval for FreshQA** We use the urls provided in the FreshQA dataset as retrieval for the\ncontext. We scraped each url and discarded extra HTML content such as headers, footers,","|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|||||||||||||\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens||||||pot|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|\n\n\nFigure 2: We compare the % of instances that our autorater labels as sufficient across datasets,\neither with the first 10k, 6k, or 2k tokens of the provided sources. FreshQA has hand-curated URLs\nthat support the answers and exhibits high sufficient context. HotPotQA and Musique have lower\nsufficient context (and even lower with 2000 tokens). We use 6000 token contexts in the remainder.\n\n\nThis is not the focus of our work, as we wish to understand how models perform with or without\nsufficient context. Having a mix of both is inevitable in generic RAG systems.\n\n\n**Datasets.** We consider **FreshQA**, **Musique-Ans**, and **HotpotQA** as a representative spread of open\nbook QA datasets. FreshQA (Vu et al., 2023) evaluates time-sensitive information and has up-to-date\nURLs that should support an answer to the queries, which we use to construct the context (see\nAppendix A.3 for details on the retrieval). We use the \u2018True Premise\u2019 setting (452 instances), skipping\n\u2018False Premise\u2019 questions that"," points.\n\n\n\nWe speculate DecomP performs well on 2WikiMultihopQA because it has only a few easy-to-predict\ndecomposition patterns, which DecomP\u2019s question\ndecomposition can leverage. The lack of such patterns in HotpotQA and MuSiQue causes it to underperform compared to IRCoT. Lastly, it will be\nuseful to assess whether DSP, which is hardcoded\nfor 2-hop questions like that of HotpotQA, will\nwork well for a dataset with a varied number of\n\nhops like that of MuSiQue. We leave this further\ninvestigation to future work.\n\n\n**D** **Additional CoT Generation Examples**\n\n\nTable 5 provides illustrations, in addition to the\n\n- nes provided in Table 2, for how the CoT generations for NoR QA, OneR QA, and IRCoT QA\nmethods vary. This gives an insight into how IRCoT improves QA performance. Since NoR relies completely on parametric knowledge, it often\nmakes a factual error in the first sentence, which derails the full reasoning chain. Some of this factual\ninformation can be fixed by OneR, especially information closest to the question (i.e., can be retrieved\nusing the question). This is insufficient for fixing\n\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nWhat was the\n\nisland, - n\nwhich Marinelli\n\nGlacier is located, formerly\nknown as?\n\n\nWho is the\n\nchild - f the\ndirector of film\nMukhyamantri\n(1996 Film)?\n\n\nWhen did the\nGovernorship\nend - f the\ncity where the\nauthor of Sac\nerdotii Nostri\n\nPrimordia died?\n\n\n\n_\u21d2_ Marinelli Glacier is\n\nlocated on the island of\nTierra del Fuego.\n_\u21d2_ The island of Tierra\ndel Fuego was formerly\nknown as Isla de Xativa.\n\n\n_\u21d2_ The director - f\nMukhyamantri (1996\nfilm) is Anjan Choudhury.\n_\u21d2_ Anjan Choudhury\nhas a child named\nSandip Choudhury.\n\n\n_\u21d2_ Sacerdotii Nostri Primordia was written by\nPope John XXIII.\n_\u21d2_ Pope"," support an answer to the queries, which we use to construct the context (see\nAppendix A.3 for details on the retrieval). We use the \u2018True Premise\u2019 setting (452 instances), skipping\n\u2018False Premise\u2019 questions that mislead by design. Musique-Ans (Trivedi et al., 2022) is a multi-hop\nQA benchmark, created by composing two to four single-hop interconnected questions. Here, \u2018Ans\u2019\nis the standard \u2018answerable\u2019 subset. Musique instances have 20 supporting text snippets as sources,\nwhich we use as the context. HotPotQA (Yang et al., 2018) is a Wikipedia-based QA dataset, with\nsingle- and multi-hop questions. The corpus is a large set of snippets; we retrieve the top 5 as the\ncontext (via REPLUG (Shi et al., 2023b) from FlashRAG (Jin et al., 2024)). We randomly sample\n500 instances from the development sets of Musique-Ans and HotPotQA for evaluation.\n\n\n**Sufficient Context % of Datasets.** Figure 2 shows the fraction of instances that our autorater\nclassifies as having sufficient context. We explore three context lengths, ranging from a maximum\n\n- f 2000 to maximum of 10000 tokens. The motivation behind this is to assess if there is a large\nchange in sufficient context if we were to simply truncate the retrieval (e.g., for models that have\nsmall context windows). In general, we see a modest difference from 2000 to 6000 token limit, but\neffectively none from 6000 to 10000 tokens. FreshQA has the highest sufficient context percentage,\nwhich makes sense as the context comes from oracle supporting documents. The lower sufficient\ncontext in Musique is perhaps surprising, given that the retrieval is fixed as part of the dataset. From\nthe results in Figure 2, we truncate at 6000 tokens for all three datasets in the remainder of the paper.\n\n\n4.2 INITIAL FINDINGS BASED ON SUFFICIENT CONTEXT\n\n\nIn general, the ideal behavior for a language generation model is to answer questions correctly when\npossible and to otherwise abstain. RAG seeks to move models towards this desired behavior, such\nthat the provided context shifts hallucinations to correct answers, or to abstentions if needed. We\nanalyze several cases to assess how far we are"," we use an LLM with ground truth answers to classify\nresponses as Correct ( **%C** ), Abstention ( **%A** ), or Hallucination ( **%H** ).\n\n\n**Fine-tuning Results and Discussion.** Table 3 shows our experimental results. We verify that the FT\nvariants have a higher rate of generating correct answers ( **%C** ) compared to closed-book and Vanilla\nRAG for Musique but not for HotPotQA. On the other hand, refuting our hypothesis, Data Mix 2 and\n3 do not lead to more abstentions than Vanilla RAG. But, they do abstain more than with Data Mix 1,\nshowing the impact of adding \u201cI don\u2019t know\u201d in the training set. In general, FT models using RAG\n\n- utput incorrect answers ( **%H** ) much of the time, and often more than they abstain ( **%A** ).\n\n\nB.2 PERFORMANCE BREAKDOWN BY SUFFICIENT CONTEXT\n\n\nWe explore RAG performance by different models for various RAG benchmark datasets. Here,\nthe first column shows performance without RAG (closed-book) while the second column shows\nperformance with RAG (open-book). To better understand RAG performance, we use our sufficient\ncontext autorater to stratify the retrieval augmented generation (RAG) datasets into sufficient and\ninsufficient context. The third and fourth columns show the performance of the second column\nstratified by sufficient vs insufficient context respectively.\n\n\n17\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Correct, hallucination, and abstention fractions across models for dataset FreshQA, stratified\nby sufficient context. FreshQA includes hand-curated source URLs, which explains the larger\npercentage of sufficient context (77.4%). FreshQA also specifically explores questions with answers\nthat change based on the question\u2019s timestamp, which may explain the frequent abstentions without\nRAG (100% for Gemini 1.5 Pro).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Correct, hallucination, and abstention fractions across models for dataset HotpotQA,\nstratified by sufficient context. HotpotQA includes questions that are more likely to be answerable\nwithout context (e.g., yes or no questions, multiple choice questions, or questions with answers that\nmight be answerable due to pre-training). This explains the higher fraction of correct answers without"],"18":["., 2018; Bapna and Firat,\n2019). Since the attention mechanism is becoming\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\nkey module in lots of NLP models, integrating\nretrieved memory through attention becomes a\nvery nature and efficient way.\n\n\n**Skeleton Extraction** In the previous two meth\n- ds, the downstream generation model learns how\nto filter out irrelevant or even harmful information from the retrieved examples implicitly. There\nalso exist some works that try to explicitly extract\nuseful information, i.e., _skeleton extraction_, from\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\n2019; Cai et al., 2019b). For example, one skeleton\nshould be a part of a whole utterance with irrelevant\ncontent masked, and the generation model only integrate this skeleton in the generation process.\n\n\n**3** **Dialogue Response Generation**\n\n\n**Background** Dialogue systems can be grouped\ninto two categories: chit-chat systems and task\n- riented systems. While task-oriented dialogue\nsystems are designed to accomplish specific user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and fluent response for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity\n\n- f possible responses to a single dialogue history\n(i.e., the _one-to-many_ problem). The dialogue history alone cannot decide a meaningful and specific\nresponse. Also, external knowledge that is not\npresent in the dialogue history are often necessary\nfor avoiding safe but boring responses. We focus\n\n- n recent efforts tackling the challenges to develop\nchit-chat dialogue systems.\nMost modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrievalbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from curated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa\n\ntions and possibly post-edited by a human. However, such systems perform poorly when a given\ndialogue history is substantially different from\nthose in the retrieval pool. On the"," al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\n\n\n**Limitations** We note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneficial to combine multiple retrieval responses.\nHowever, this can be difficult due to the one-tomany nature of dialogue response generation. Sec\n- nd, current methods use universal relevance score\n\nfor retrieval. It can be more effective if we can\n\nuse more customized retrieval metric especially\nfor controlled dialogue response generation (e.g.,\npersona, emotion, etc). Third, the retrieval pool\n\n- f existing methods is limited to dialogue corpora\n(context-response pairs) or documents. It might\nbe useful to enlarge the retrieval pool by including\nmore corpora in other domains or in other modali\n\nties. As discussed, there leaves plenty of possible\ndirections to explore in the future.\n\n\n**4** **Machine Translation**\n\n\nRetrieval augmented translation originates from human translation scenarios (Somers, 2003). When\ntranslating \u02c6 _**y**_ from an input source sentence _**x**_, a human translator typically involves a search engine to\nretrieve similar sentences _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ from a bilingual database. Such a technique called **translation**\n**memory** is helpful to improve the translation quality and efficiency for human translators (Dillon\nand Fraser, 2006). As the development of machine translation techniques, there is a surge of\ninterests in improving machine translation models\nwith translation memory. In the rest of this section,\nwe will review translation memory for both statistical machine translation (SMT) and neural machine\ntranslation (NMT).\n\n\n**4.1** **Translation Memory in SMT**\n\n\nGenerally, SMT includes three key components in\na pipeline manner such as phrase table extraction,\nparameter tuning and decoding (Koehn et al., 2003;\nChiang, 2007). As a result, many efforts have been\nmade to make use of translation memory (TM) on\ntop of each component.\n\n\n**Constrained"," grammatical\nas they are collected from real-world conversa\n\ntions and possibly post-edited by a human. However, such systems perform poorly when a given\ndialogue history is substantially different from\nthose in the retrieval pool. On the other hand,\nthe generation-based models (Shang et al., 2015;\nVinyals and Le, 2015; Li et al., 2016a) generate\na new utterance from scratch. Those generationbased models have better generalization capacity\nwhen handling unseen dialogue contexts. Nevertheless, the generated utterances are inclined to be\ndull and non-informative (e.g., \u201cI don\u2019t know\u201d, \u201cI\nthink so\u201d, \u201cMe too\u201d etc.) (Li et al., 2016a).\n\n\n**Shallow Integration** As discussed, retrievalbased models may give informative but inappropriate responses while generation-based models\n\n- ften do the opposite. It is desirable to combine the\nbest of both worlds. Early work (Qiu et al., 2017)\nattempts to re-rank the output from both models.\nFor a deep integration, Song et al. (2016) and Yang\net al. (2019) extend the standard SEQ2SEQ encoderdecoder model (Bahdanau et al., 2014) with an extra encoder for encoding the retrieval result. The\n\n- utput of the extra encoder, along with the output\nfrom the original encoder for dialogue history, is\nused to feed the decoder. Weston et al. (2018) use\na single encoder that takes the concatenation of\nthe original dialogue history and the retrieved as\ninput. Wu et al. (2019) note that the retrieved information should be used in awareness of the context\n\ndifference, and further proposed to construct an\nedit vector by explicitly encoding the lexical differences between the input dialogue history and the\nretrieved dialogue history. Pandey et al. (2018) further propose to weight different training instances\nby context similarity.\n\n\n**Deep Integration** To prevent the inflow of erroneous information, Cai et al. (2019a) propose\na general framework that first extracts a skeleton\nfrom the retrieved response and then generates the\nresponse based on the extracted skeleton. This\nframework is also adopted for stylistic response\ngeneration (Su et al., 2021c). Gupta et al. (2021)\nsuggest to use the semantic","## **Sources of Hallucination by Large Language Models on Inference Tasks**\n\n**Nick M** **[c]** **Kenna** _[\u2020]_ ***** **Tianyi Li** _[\u2020]_ *****\n**Liang Cheng** _[\u2020]_ **Mohammad Javad Hosseini** _[\u2021]_ **Mark Johnson** _[\u00a7]_ **Mark Steedman** _[\u2020]_\n\n_\u2020_ University of Edinburgh _\u2021_ Google Research _\u00a7_ Macquarie University\n{nick.mckenna, tianyi.li}@ed.ac.uk\n\n\n\n**Abstract**\n\n\nLarge Language Models (LLMs) are claimed\nto be capable of Natural Language Inference\n(NLI), necessary for applied tasks like question\nanswering and summarization. We present a\nseries of behavioral studies on several LLM\n\nfamilies (LLaMA, GPT-3.5, and PaLM) which\nprobe their behavior using controlled experiments. We establish two biases originating\nfrom pretraining which predict much of their\nbehavior, and show that these are major sources\n\n  - f hallucination in generative LLMs. First,\nmemorization at the level of sentences: we\n\nshow that, regardless of the premise, models\nfalsely label NLI test samples as entailing when\nthe hypothesis is attested in training data, and\nthat entities are used as \u201cindices\u201d to access the\n\nmemorized data. Second, statistical patterns\n\n  - f usage learned at the level of corpora: we\nfurther show a similar effect when the premise\npredicate is less frequent than that of the hypothesis in the training data, a bias following\nfrom previous studies. We demonstrate that\nLLMs perform significantly worse on NLI test\nsamples which do not conform to these biases\nthan those which do, and we offer these as valuable controls for future LLM evaluation. [1]\n\n\n**1** **Introduction**\n\n\nLarge Language Models (LLMs) such as LLaMA,\nGPT-3\/4, PaLM, and more (Touvron et al., 2023;\nBrown et al., 2020; Chowdhery et al., 2022), have\nbeen trusted by many to perform language understanding in downstream tasks such as summarization, question answering, and fact verification,\namong others (Zhang et al., 2023"," generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n\n\n**2** **Retrieval-Augmented Paradigm**\n\n\nIn this section, we first give a general formulation\n\n- f retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrievalaugmented generation paradigm, including the re\n\nOutput\n\n\n\n\n\nFigure 1: The overview of this survey.\n\n\n\ntrieval source, retrieval metric and integration meth\n- ds.\n\n\n**2.1** **Formulation**\n\n\nMost text generation tasks can be formulated as a\nmapping from input sequence _**x**_ to output sequence\n_**y**_ : _**y**_ = _f_ ( _**x**_ ). For instance, _**x**_ and _**y**_ could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\n\n\n_**y**_ = _f_ ( _**x**_ _,_ _**z**_ ) (1)\n\n\nwhere _**z**_ = _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that _**y**_ _[r]_\n\nmay benefit the response generation, if _**x**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r"],"19":["Published as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto finetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved significant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\n\n- f questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model\u2019s hidden states. However, their method requires an initial round\n\n- f TF-IDF\/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\n","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [","models (PaLM-540B), and require fine-tuning to\n\n- utperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be\neffective for smaller models without any training.\nWhile a direct comparison with these approaches is\nnot straightforward (difference in knowledge corpus, LLMs, examples), we find that our ODQA\nperformance is much higher than all their reported\nnumbers where available (\u00a75).\n\n\n**Supervised** **Multi-Step** **Open-Domain** **QA.**\nPrior work has explored iterative retrieval for\n\n- pen-domain QA in a fully supervised setting. Das\net al. (2019) proposes an iterative retrieval model\nthat retrieves using a neural query representation\nand then updates it based on a reading comprehension model\u2019s output. Feldman and El-Yaniv\n(2019) apply similar neural query reformulation\nidea for multihop open-domain QA. Xiong et al.\n(2021) extends the widely-used Dense Passage\nRetrieval (DPR) (Karpukhin et al., 2020) to\nmultihop setting, which has since been improved\nby Khattab et al. (2021). Asai et al. (2020)\nleverages the graph structure induced by the entity\nlinks present in Wikipedia paragraphs to perform\niterative multi-step retrieval. GoldEn (Gold Entity)\nretriever (Qi et al., 2019) iteratively generates\ntext queries based on paragraphs retrieved from\nan off-the-shelf retriever but requires training\ndata for this next query generator. Nakano et al.\n(2021) used GPT3 to answer long-form questions\nby interacting with the browser but relied on\nhuman annotations of these interactions. All of\n\nthese methods rely on supervised training on a\nlarge-scale dataset and can not be easily extended\nto a few-shot setting.\n\n\n\n**3** **Chain-of-Thought-Guided Retrieval**\n**and Open-Domain QA**\n\n\nOur goal is to answer a knowledge-intensive multistep reasoning question _Q_ in a few-shot setting\nby using a knowledge source containing a large\nnumber of documents. To do this we follow a\n\nretrieve-and-read paradigm (Zhu et al., 2021),\nwhere the retriever first retrieves documents from\nthe knowledge source and the QA model reads the\nretrieved documents and the question to","T:\n\n\n1. improves both retrieval and few-shot QA performance on several multi-step open-domain\nQA datasets, in both IID and OOD settings;\n2. reduces factual errors in generated CoTs; and\n3. improves performance with both large-scale\n(175B models) as well as smaller-scale models (Flan-T5-*, _\u2264_ 11B) without any training.\n\n\n**2** **Related Work**\n\n\n**Prompting for Open-Domain QA.** LLMs can\nlearn various tasks by simply using a few examples as prompts (Brown et al., 2020). They\u2019ve\nalso been shown to answer complex questions\nby producing step-by-step reasoning (chain-ofthoughts, or CoT) when prompted with a few or\nzero demonstrations (Wei et al., 2022; Kojima et al.,\n2022). Prompting has been applied to open-domain\nQA (Lazaridou et al., 2022; Sun et al., 2022; Yu\net al., 2023) but its value in improving retrieval and\nQA for multi-step open-domain questions remains\nrelatively underexplored.\n\n\n3We explain later (in the Metric section and Footnote 7)\nthe appropriateness of this metric in our setting as opposed to\nmore mainstream information recall metrics.\n\n\nRecently three approaches have been proposed\nfor multi-step open-domain QA. SelfAsk (Press\net al., 2022) prompts LLMs to decompose a question into subquestions and answers subquestions by\na call to Google Search API. DecomP (Khot et al.,\n2023) is a general framework that decomposes a\ntask and delegates sub-tasks to appropriate submodels. They also decompose questions but delegate retrieval to a BM25-based retriever. Both of\nthese approaches are not developed for CoT reasoning, do not focus on the retrieval problem, and require a single-hop QA model to answer the decomposed questions. Recently proposed ReAct (Yao\net al., 2022) system frames the problem as generating a sequence of reasoning and action steps. These\nsteps are much more complex, rely on much larger\nmodels (PaLM-540B), and require fine-tuning to\n\n- utperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be\neffective for smaller models without any training.\nWhile a direct comparison","\ntop _k_ paragraphs [4], while on FEVER the metrics are precision, recall and F1 in order to be consistent\nwith previous results. On both datasets, MDR substantially outperforms all baselines.\n\n\n3.1.2 RERANKING\n\n\n_Reranking_ documents returned by efficient retrieval methods with a more sophisticated model is a\ncommon strategy for improving retrieval quality. For instance, state-of-the-art multi-hop QA systems\nusually augment traditional IR techniques with large pretrained language models to select a more\ncompact but precise passage set. On HotpotQA, we test the effectiveness of MDR after a simple\ncross-attention reranking: each of the top _k_ passage sequences from MDR is first prepended with the\n\n- riginal question and then fed into a pretrained Transformer encoder, i.e., ELECTRA-large (Clark\net al., 2020), that predicts relevant scores. We train this reranking model with a binary crossentropy loss, with the target being whether the passage sequence cover both groundtruth passages.\nWe empirically compare our approach with two other existing reranking-based retrieval methods:\n**Semantic Retrieval** (Nie et al., 2019) uses BERT at both passage-level and sentence-level to select\ncontext from the initial TF-IDF and hyperlinked passages; **Graph Recurrent Retriever** (Asai et al.,\n2020) learns to recursively select the best passage sequence on top of a hyperlinked passage graph,\nwhere each passage node is encoded with BERT.\n\n\nTable 2 shows the reranking results. Following Asai et al. (2020), we use _Answer Recall_ and\n_Support Passage Exact Match (SP EM)_ [5] as the evaluation metrics. Even without reranking, MDR\nis already better than Semantic Retrieval, which requires around 50 BERT encoding (where each\n\n\n2If the answer span is in both, the one that has its title mentioned in the other passage is treated as the second.\n3https:\/\/pypi.org\/project\/rank-bm25\n4As the sequence length is 2 for HotpotQA, we pick the top _k_ \/2 sequences predicted by MDR.\n5Whether the final predicted sequence covers both gold passages.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2021\n\n\nTable 2: HotpotQA reranked retrieval results\n(input passages for final answer prediction).\n\n\nMethod SP"],"20":[" network architecture incorporating SSMs. It is motivated by the gated\nattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most\nimportantly, its projection _contracts_ the model dimension to reduce the state size of the SSM, while ours _expands_ the\nmodel dimension in order to increase the state size, based on the motivation in Section 3.1.\n\n\n- Mega (Ma et al. 2023) combined the EMA simplification of S4 described above into a hybrid architecture using an efficient\nattention approximation.\n\n\n- H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is\nthe first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later\narchitectures.\n\n\n- Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the\ninput. While sharing the \u201cselection\u201d name, we consider this an architectural modification that is closer to architectural\ngating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective\n\n\n25\n\n\nCopying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones\n(indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0).\n\n\n- RetNet (Y. Sun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to a\nspecial case where the state dimension is _\ud835\udc41_ = 1. Although not framed as such, its recurrence can be viewed as a special\ncase of a linear SSM.\n\n\nIts primary source of improvement is using a linear attention with large _head dimension_, which can be viewed as another\nmethod to perform input-dependent state expansion. Using a larger head dimension in the context of linear attention\nvariants was first done by H3, but not extensively used since this requires a proportional amount of extra computation.\nRetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention\ninstead of convolutions, made feasible by their"," al. 2020). However, as previously\nmentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives\nused LTI (non-selective) models, most commonly in the form of global convolutions.\n\n\n**3.3.1** **Motivation of Prior Models**\n\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods.\n\n\nAt a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in\nSection 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to _maximize_\n_hidden state dimension without paying speed and memory costs_ .\n\n\n- Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding\nthe former (2) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and\nmaterializing the latent state _\u210e_ with shape (B _,_ L _,_ D _,_ N), which is much larger (by a factor of _\ud835\udc41_, the SSM state dimension)\nthan the input _\ud835\udc65_ and output _\ud835\udc66_  - f shape (B _,_ L _,_ D). Thus the more efficient convolution mode was introduced which could\nbypass the state computation and materializes a convolution kernel (3a) of size only (B _,_ L _,_ D).\n\n\nPrior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by\na factor of _\ud835\udc41_ (\u2248 10 \u2212 100), much larger than traditional RNNs, without efficiency penalties.\n\n\n6\n\n\n**3.3.2** **Overview of Selective Scan: Hardware-Aware State Expansion**\n\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to\nrevisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and\nrecomputation. We make two main observations:\n\n\n- The naive recurrent computation uses _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ ) FLOPs while the convolutional computation uses _\ufffd"," the sequence length requires _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ )\ntime and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3.\n\n\n**General State Space Models.** We note that the term _state space model_ has a very broad meaning which simply represents\nthe notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in different\ndisciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal\nmodeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman filters (controls (Kalman 1960)),\nhidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes\nconvolutional) models at large (deep learning).\n\n\nThroughout this entire paper we use the term \u201cSSM\u201d to refer exclusively to the class of structured SSMs or S4 models (Gu,\nGoel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington,\nand Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of such\nmodels, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvieto\net al. 2023; Poli et al. 2023), and clarify nuances when necessary.\n\n\n**SSM Architectures.** SSMs are standalone sequence transformations that can be incorporated into end-to-end neural\nnetwork architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear\nconvolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our\nprimary baselines.\n\n\n- Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be\nviewed as a degenerate linear SSM.\n\n\n- H3 (Dao, Fu, Saab, et al. "," done by H3, but not extensively used since this requires a proportional amount of extra computation.\nRetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention\ninstead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.\n\n\n- RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-free\nTransformer (S. Zhai et al. 2021)), another variant of linear attention. Its main \u201cWKV\u201d mechanism involves LTI recurrences\nand can be seen as the ratio of two SSMs.\n\n\nWe also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformer\u2019s\nMHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLP\nblocks.\n\n\n**B.3** **Relationship to RNNs**\n\n\nRNNs and SSMs are broadly related, as they both involve the concepts of _recurrence_ - n a latent _state_ .\n\n\nSeveral older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016),\nand simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities.\nBecause of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs,\nand are thus more powerful in a sense than the family of LTI structured SSMs above. The main differences are:\n\n\n  - They do not use state expansion ( _\ud835\udc41_ = 1) or selective _\ud835\udc69, \ud835\udc6a_ parameters, both of which are important for performance\n(Section 4.6).\n\n\n   They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +\ndiscretization (Theorem 1). The connections to principled SSM theory provides better parameterizations and\ninitializations (Section 3.6).\n\n\nAdditionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991;\nHochreiter","## Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\nAlbert Gu [\u2217][1] and Tri Dao [\u2217][2]\n\n\n1\nMachine Learning Department, Carnegie Mellon University\n\n\n2\nDepartment of Computer Science, Princeton University\nagu@cs.cmu.edu, tri@tridao.me\n\n\n**Abstract**\n\n\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the\nTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,\ngated convolution and recurrent models, and structured state space models (SSMs) have been developed to address\nTransformers\u2019 computational inefficiency on long sequences, but they have not performed as well as attention on important\nmodalities such as language. We identify that a key weakness of such models is their inability to perform content-based\nreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses\ntheir weakness with discrete modalities, allowing the model to _selectively_ propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though this change prevents the use of efficient\nconvolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a\nsimplified end-to-end neural network architecture without attention or even MLP blocks ( **Mamba** ). Mamba enjoys fast\ninference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves\n\n    - n real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art\nperformance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model\n\n    - utperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream\nevaluation.\n\n### **1 Introduction**\n\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged\nas an effective paradigm in modern machine learning. The backbone of these FMs are often _sequence models_, operating on\narbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and\ngenomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al"],"21":["https:\/\/doi.org\/10.48550\/arXiv.2305.14002)\n_CoRR_, abs\/2305.14002.\n\n\nYury Zemlyanskiy, Michiel de Jong, Joshua Ainslie,\nPanupong Pasupat, Peter Shaw, Linlu Qiu, Sumit\n[Sanghai, and Fei Sha. 2022. Generate-and-retrieve:](https:\/\/aclanthology.org\/2022.coling-1.438)\n[Use your predictions to improve retrieval for seman-](https:\/\/aclanthology.org\/2022.coling-1.438)\n[tic parsing. In](https:\/\/aclanthology.org\/2022.coling-1.438) _Proceedings of the 29th International_\n_Conference on Computational Linguistics, COLING_\n_2022, Gyeongju, Republic of Korea, October 12-17,_\n_2022_, pages 4946\u20134951. International Committee on\nComputational Linguistics.\n\n\n\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang\nZan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\n[2023. Repocoder: Repository-level code completion](https:\/\/doi.org\/10.48550\/arXiv.2303.12570)\n[through iterative retrieval and generation.](https:\/\/doi.org\/10.48550\/arXiv.2303.12570) _CoRR_,\nabs\/2303.12570.\n\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open\npre-trained transformer language models. _ArXiv_,\nabs\/2205.01068.\n\n\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhip"," to answer rather\nthan seeking embedding similarity for the problem or query.\nUsing the Step-back Prompting method [10], the original\nquery is abstracted to generate a high-level concept question\n(step-back question). In the RAG system, both the step-back\nquestion and the original query are used for retrieval, and both\nthe results are utilized as the basis for language model answer\ngeneration.\n_3) Query Routing:_ Based on varying queries, routing to\ndistinct RAG pipeline,which is suitable for a versatile RAG\nsystem designed to accommodate diverse scenarios.\n_Metadata Router\/ Filter_ . The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\n\n- n the keywords and metadata within the chunks to narrow\ndown the search scope.\n_Semantic Router_ is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router [6] . Certainly, a hybrid routing\napproach can also be employed, combining both semantic and\nmetadata-based methods for enhanced query routing.\n\n\n_D. Embedding_\n\n\nIn RAG, retrieval is achieved by calculating the similarity\n(e.g. cosine similarity) between the embeddings of the question and document chunks, where the semantic representation\ncapability of embedding models plays a key role. This mainly\nincludes a sparse encoder (BM25) and a dense retriever (BERT\narchitecture Pre-training language models). Recent research\nhas introduced prominent embedding models such as AngIE,\nVoyage, BGE,etc [94]\u2013[96], which are benefit from multi-task\ninstruct tuning. Hugging Face\u2019s MTEB leaderboard [7] evaluates\nembedding models across 8 tasks, covering 58 datasests. Additionally, C-MTEB focuses on Chinese capability, covering\n6 tasks and 35 datasets. There is no one-size-fits-all answer\nto \u201cwhich embedding model to use.\u201d However, some specific\nmodels are better suited for particular use cases.\n_1) Mix\/hybrid Retrieval :_ Sparse and dense embedding\napproaches capture different relevance features and can benefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n\n\n[6https:\/\/github.com\/aurelio-labs\/semantic-router](https:\/\/github.com\/aurelio-labs\/semantic-router)\n[7https:\/\/huggingface.co\/spaces\/mteb\/leaderboard](https:\/\/hugging"," effective method is to distill the more\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\nbe coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT [27], aligns the\nscoring functions between Retriever and Generator using KL\ndivergence.\n\n\nV. AUGMENTATION PROCESS IN RAG\n\n\nIn the domain of RAG, the standard practice often involves\na singular (once) retrieval step followed by generation, which\ncan lead to inefficiencies and sometimes is typically insufficient for complex problems demanding multi-step reasoning,\nas it provides a limited scope of information [105]. Many\nstudies have optimized the retrieval process in response to this\nissue, and we have summarised them in Figure 5.\n\n\n_A. Iterative Retrieval_\n\n\nIterative retrieval is a process where the knowledge base\nis repeatedly searched based on the initial query and the text\ngenerated so far, providing a more comprehensive knowledge\n\n\n11\n\n\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\n\n\n\nbase for LLMs. This approach has been shown to enhance\nthe robustness of subsequent answer generation by offering\nadditional contextual references through multiple retrieval\niterations. However, it may be affected by semantic discontinuity and the accumulation of irrelevant information. ITERRETGEN [14] employs a synergistic approach that leverages \u201cretrieval-enhanced generation\u201d alongside \u201cgenerationenhanced retrieval\u201d for tasks that necessitate the reproduction\n\n- f specific information. The model harnesses the content\nrequired to address the input task as a contextual basis for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n\n\n_B. Recursive Retrieval_\n\n\nRecursive retrieval is often used in information retrieval and\n\nNLP to improve the depth and relevance of search results","ing\": \"Answer 1 is better because <your reasoning>.\"\n```\n\n_}}_\n\n```\n---Question--\n```\n\n_{_ `question` _}_\n\n```\n---Answer 1--\n```\n\n_{_ `answer1` _}_\n\n```\n---Answer 2--\n```\n\n_{_ `answer2` _}_\n\n```\nAssess which answer is better according to the following measure:\n\n```\n\n_{_ `criteria` _}_\n\n```\nOutput:\n\n```\n\n**F.2** **Relative Assessment Metrics**\n\n\n`CRITERIA =` _{_\n```\n\"comprehensiveness\": \"How much detail does the answer provide to cover all the aspects and details of the\nquestion? A comprehensive answer should be thorough and complete, without being redundant or irrelevant.\nFor example, if the question is \u2019What are the benefits and drawbacks of nuclear energy?\u2019, a comprehensive\nanswer would provide both the positive and negative aspects of nuclear energy, such as its efficiency,\nenvironmental impact, safety, cost, etc. A comprehensive answer should not leave out any important points\nor provide irrelevant information. For example, an incomplete answer would only provide the benefits of\nnuclear energy without describing the drawbacks, or a redundant answer would repeat the same information\nmultiple times.\",\n\"diversity\": \"How varied and rich is the answer in providing different perspectives and insights\non the question? A diverse answer should be multi-faceted and multi-dimensional, offering different\nviewpoints and angles on the question. For example, if the question is \u2019What are the causes and effects\nof climate change?\u2019, a diverse answer would provide different causes and effects of climate change, such\nas greenhouse gas emissions, deforestation, natural disasters, biodiversity loss, etc. A diverse answer\nshould also provide different sources and evidence to support the answer. For example, a single-source\nanswer would only cite one source or evidence, or a biased answer would only provide one perspective or\nopinion.\",\n\"directness\": \"How specifically and clearly does the answer address the question? A direct answer should\nprovide a clear and concise answer to the question. For example, if the question is \u2019What is the capital\nof France?\u2019, a direct answer would be \u2019Paris\u2019. A direct answer should not provide any irrelevant or\nunnecessary information that does not answer the question. For example, an indirect answer would be \u2019The\ncapital of France is located"," generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n\n\n**2** **Retrieval-Augmented Paradigm**\n\n\nIn this section, we first give a general formulation\n\n- f retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrievalaugmented generation paradigm, including the re\n\nOutput\n\n\n\n\n\nFigure 1: The overview of this survey.\n\n\n\ntrieval source, retrieval metric and integration meth\n- ds.\n\n\n**2.1** **Formulation**\n\n\nMost text generation tasks can be formulated as a\nmapping from input sequence _**x**_ to output sequence\n_**y**_ : _**y**_ = _f_ ( _**x**_ ). For instance, _**x**_ and _**y**_ could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\n\n\n_**y**_ = _f_ ( _**x**_ _,_ _**z**_ ) (1)\n\n\nwhere _**z**_ = _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that _**y**_ _[r]_\n\nmay benefit the response generation, if _**x**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r"],"22":[" the \u201cFilter-Reranker\u201d\nparadigm, which combines the strengths of LLMs and SLMs.\n\n\n\n10\n\n\nIn this paradigm, SLMs serve as filters, while LLMs function\nas reordering agents. The research shows that instructing\nLLMs to rearrange challenging samples identified by SLMs\nleads to significant improvements in various Information\nExtraction (IE) tasks. Another straightforward and effective\napproach involves having the LLM evaluate the retrieved\ncontent before generating the final answer. This allows the\nLLM to filter out documents with poor relevance through LLM\ncritique. For instance, in Chatlaw [104], the LLM is prompted\nto self-suggestion on the referenced legal provisions to assess\ntheir relevance.\n\n\n_B. LLM Fine-tuning_\n\n\nTargeted fine-tuning based on the scenario and data characteristics on LLMs can yield better results. This is also one\n\n- f the greatest advantages of using on-premise LLMs. When\nLLMs lack data in a specific domain, additional knowledge can\nbe provided to the LLM through fine-tuning. Huggingface\u2019s\nfine-tuning data can also be used as an initial step.\nAnother benefit of fine-tuning is the ability to adjust the\nmodel\u2019s input and output. For example, it can enable LLM to\nadapt to specific data formats and generate responses in a particular style as instructed [37]. For retrieval tasks that engage\nwith structured data, the SANTA framework [76] implements\na tripartite training regimen to effectively encapsulate both\nstructural and semantic nuances. The initial phase focuses on\nthe retriever, where contrastive learning is harnessed to refine\nthe query and document embeddings.\nAligning LLM outputs with human or retriever preferences\nthrough reinforcement learning is a potential approach. For\ninstance, manually annotating the final generated answers\nand then providing feedback through reinforcement learning.\nIn addition to aligning with human preferences, it is also\npossible to align with the preferences of fine-tuned models\nand retrievers [79]. When circumstances prevent access to\npowerful proprietary models or larger parameter open-source\nmodels, a simple and effective method is to distill the more\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\nbe coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA","Retrieved document formatting.** Multiple retrieved documents are linearized according to their\nranking and then added to the beginning of the user\ninput using Prompt D.1.\nOther implementation details such as sentence tokenization and efficiency are included Appendix A.\n\n\n**4** **Multi-time Retrieval Baselines**\n\n\nExisting passive multi-time retrieval augmented\nLMs can also be formulated using our framework\n(subsection 2.3). In this section, we formally introduce three baseline categories based on when\nand what to retrieve. These baselines are not exact\n\nreproductions of the corresponding paper because\nmany design choices differ which makes direct\ncomparisons impossible. We implemented them\nusing the same settings, with the only variation\nbeing when and what to retrieve.\n\n\n**Previous-window** approaches trigger retrieval\nevery _l_ tokens, where _l_ represents the window size.\nGenerated tokens from the previous window are\nused as the query:\n\n\n_**q**_ _t_ = _**y**_ _t\u2212_ 1 ( _t \u2265_ 2) _,_\n\n_**y**_ _t_ = [ _w_ ( _t\u2212_ 1) _l_ +1 _, ..., wtl_ ] _._\n\n\nSome existing methods in this category are RETRO\n(Borgeaud et al., 2022), IC-RALM (Ram et al.,\n\n\n2https:\/\/api.openai.com\/v1\/completions April 23.\n[3https:\/\/www.microsoft.com\/en-us\/bing\/apis\/](https:\/\/www.microsoft.com\/en-us\/bing\/apis\/bing-web-search-api)\n[bing-web-search-api](https:\/\/www.microsoft.com\/en-us\/bing\/apis\/bing-web-search-api)\n\n\n2023), which retrieve every few tokens, and KNNLM (Khandelwal et al., 2020), which retrieves every token. [4] We follow Ram et al. (2023) to use a\nwindow size of _l_ = 16.\n\n\n**Previous-sentence** approaches trigger retrieval\nevery sentence and use the previous sentence as the\nquery, and IRCoT (Trivedi et al., 2022) belongs to\nthis category:\n\n\n_**q**_ _t_ = _**y**_ _t\u2212_ 1 ( _t \u2265_ 2) _,_\n\n\n_**y**_ _t_ = _**s**_ _t._","2023):\n\n\n   - search ~~q~~ uery\n\n   - search ~~d~~   - cument\n\n   - classification\n\n\n20\n\n\nquery document\n\n\npopulation of breckenridge mi breckenridge, michigan. breckenridge is a village\nin gratiot county in the u. s. state of michigan. the\npopulation was 1, 328 at the 2010 census. the village\nis located in wheeler township.\ncan a deposition be used in a criminal case depositions are commonly used in civil litigation\n(suits for money damages or equitable relief) [...]\nwhat cases require strict scrutiny the strict scrutiny standard is one of three employed\nby the courts in reviewing laws and government policies. the rational basis [...]\nfunction of state supreme courts it has also initiated several programs designed to\nimprove the effectiveness of the court system. a\nprimary function of the supreme court is to ensure\n\n[...]\nwhat is the population in idaho idaho \u2019 s population grows to nearly 1. 7 million.\nidaho \u2019 s population grew by 1. 2 percent between\nmid - 2014 and mid - 2015, the 12th strongest increase among the states and four - tenths of a percentage point ahead of the national growth rate.\nwhat is the population of manson, ia manson, iowa. manson is a city in calhoun county,\niowa, united states. the population was 1, 690 at the\n2010 census.\nwhat happens after a sentencing hearing find answers. sentencing. after a criminal defendant\nis convicted or pleads guilty, a judge will decide [...]\nflathead county population flathead county, montana. flathead county is a county\nlocated in the u. s. state of montana. as of the 2010\ncensus, the population was 90, 928, making it [...]\nwhiting, ks population the city of whiting had a population of 177 as of july\n1, 2017. whiting ranks in the lower quartile for population density and diversity index when compared to\nthe other cities, towns [...]\nwhat is the population of lewiston id lewiston, id population and races. as of 2010 - 2014,\nthe total population of lewiston is 32, 178, which is\n4. 12% more than it was"," (DisambigF1), ROUGE (Lin, 2004), and an overall score combining Disambig-F1 and ROUGE (DR).\n\n\n**Open-domain summarization** The goal of opendomain summarization is to generate a comprehensive summary about a topic by gathering information from open web (Giorgi et al., 2022). We use\nWikiAsp (Hayashi et al., 2021) which aims to generate aspect-based summaries about entities from\n20 domains in Wikipedia, e.g., \u201cGenerate a summary about Echo School (Oregon) including the\n\n\nNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp\n\n\nFigure 4: Comparision between FLARE and baselines across all tasks\/datasets. We report the primary metric for\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\n\n\n\nfollowing aspects: academics, history.\u201d Experimental setting details are included in Appendix B.\nMetrics include ROUGE, named entity-based F1,\nand UniEval (Zhong et al., 2022) which measures\nfactual consistency.\n\n\n**6** **Experimental Results**\n\n\nWe first report overall results across 4 tasks\/datasets\nand compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\n\nrun ablation experiments to study the efficacy of\nvarious design choices of our method.\n\n**6.1** **Comparison with Baselines**\n\n**Overall results.** The overall performance of\nFLARE and baseline across all tasks\/datasets are\n\nreported in Figure 4. FLARE outperforms all baseline on all tasks\/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks, multihop QA shows the\nmost significant improvement. This is largely due\nto the task\u2019s clear definition and specific objective\n\n- f producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to\ngenerate on-topic output. In contrast, ASQA and\nWikiAsp are more open-ended, which increases the\ndifficulty of both generation and evaluation. The\n"," effective method is to distill the more\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\nbe coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT [27], aligns the\nscoring functions between Retriever and Generator using KL\ndivergence.\n\n\nV. AUGMENTATION PROCESS IN RAG\n\n\nIn the domain of RAG, the standard practice often involves\na singular (once) retrieval step followed by generation, which\ncan lead to inefficiencies and sometimes is typically insufficient for complex problems demanding multi-step reasoning,\nas it provides a limited scope of information [105]. Many\nstudies have optimized the retrieval process in response to this\nissue, and we have summarised them in Figure 5.\n\n\n_A. Iterative Retrieval_\n\n\nIterative retrieval is a process where the knowledge base\nis repeatedly searched based on the initial query and the text\ngenerated so far, providing a more comprehensive knowledge\n\n\n11\n\n\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\n\n\n\nbase for LLMs. This approach has been shown to enhance\nthe robustness of subsequent answer generation by offering\nadditional contextual references through multiple retrieval\niterations. However, it may be affected by semantic discontinuity and the accumulation of irrelevant information. ITERRETGEN [14] employs a synergistic approach that leverages \u201cretrieval-enhanced generation\u201d alongside \u201cgenerationenhanced retrieval\u201d for tasks that necessitate the reproduction\n\n- f specific information. The model harnesses the content\nrequired to address the input task as a contextual basis for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n\n\n_B. Recursive Retrieval_\n\n\nRecursive retrieval is often used in information retrieval and\n\nNLP to improve the depth and relevance of search results"],"23":[" small Mamba model outperforms the state-of-the-art (and much larger) GAN-**\n**and diffusion- based models.** A larger model parameter-matched to the baselines further improves on fidelity metrics\ndramatically.\n\n\nTable 5 takes the small Mamba model and investigates combinations of different architectures for the outer stages and\ncenter stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba _>_ S4+MLP _>_\nMHA+MLP in the center blocks.\n\n\n14\n\n\nTable 4: ( **SC09** ) Automated metrics for unconditional generation on\na challenging dataset of fixed-length speech clips. ( _Top to Bottom_ )\nAutoregressive baselines, non-autoregressive baselines, Mamba, and\ndataset metrics.\n\n\nModel Params NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nSampleRNN 35.0M 2.042 8.96 1.71 3.02 1.76\nWaveNet 4.2M 1.925 5.08 2.27 5.80 1.47\n\nSaShiMi 5.8M 1.873 1.99 5.13 42.57 0.74\n\n\nWaveGAN 19.1M   - 2.03 4.90 36.10 0.80\n\nDiffWave 24.1M   - 1.92 5.26 51.21 0.68\n\n+ SaShiMi 23.0M     - 1.42 5.94 69.17 0.59\n\n\n**Mamba** 6.1M **1.852** 0.94 6.26 88.54 0.52\n\n**Mamba** 24.3M 1.860 **0.67** **7.33** **144.9** **0.36**\n\n\nTrain    -    - 0 _._ 00 8 _._ 56 292 _._ 5 0 _._ 16\n\nTest    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC","Test    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC09 Model Ablations** ) Models with 6M parameters. In\nSaShiMi\u2019s U-Net backbone, there are 8 center blocks operating on\nsequence length 1000, sandwiched on each side by 8 outer blocks on\nsequence length 4000, sandwiched by 8 outer blocks on sequence\nlength 16000 (40 blocks total). The architecture of the 8 center\nblocks are ablated independently of the rest. Note that Transformers\n(MHA+MLP) were not tested in the more important outer blocks\nbecause of efficiency constraints.\n\n\nOuter Center NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\n\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\n\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\n\nMamba MHA+MLP **1.850** 1.37 5.63 58.23 0.62\n\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\n\nMamba Mamba 1.852 **0.94** **6.26** **88.54** **0.52**\n\n\n\nWe benchmark the speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of\n(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6."," standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained) would have higher inference\nthroughput than a 5\u00d7 smaller Transformer-1.3B. Details in Appendix E.5, which additionally includes a benchmark of\nmemory consumption.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: ( **Efficiency Benchmarks** .) ( _Left_ ) Training: our efficient scan is 40\u00d7 faster than a standard implementation. ( _Right_ ) Inference:\nas a recurrent model, Mamba can achieve 5\u00d7 higher throughput than Transformers.\n\n\n**4.6** **Model Ablations**\n\n\nWe perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with\nsize \u2248 350M models at Chinchilla token counts (same setting as Figure 4).\n\n\n**4.6.1** **Architecture**\n\n\nTable 6 investigates the effects of the architecture (block) and its inner SSM layer (Figure 3). We find that\n\n\n- Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.\n\n\nReplacing the complex-valued S4 variant from previous work with a real-valued one does not affect performance much,\nsuggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware efficiency.\n\n\nReplacing any of these with a selective SSM (S6) significantly improves performance, validating the motivation of\nSection 3.\n\n\n15\n\n\nTable 6: ( **Ablations: Architecture and SSM layer** .) The Mamba block performs similarly to H3 while being simpler. In the inner layer,\nthere is little difference among different parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More\nspecifically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.\n\n\n\nModel Arch. SSM Layer Perplexity\n\n\nHyena H3 Hyena 10 _._ 24\nH3 H3 S4 (complex) 10 _._ 30\n\n      - H3 S4 (real) 10 _._"," a total of 10 _\ud835\udc35_ tokens.\n\n\n**Results.** Figure 5 ( _Left_ ) shows that Mamba\u2019s pretraining perplexity improves smoothly with model size, and that Mamba\nscales better than both HyenaDNA and Transformer++. For example, at the largest model size of \u2248 40 _\ud835\udc40_ parameters, the\ncurve shows that **Mamba can match the Transformer++ and HyenaDNA models with roughly** 3\u00d7 **to** 4\u00d7 **fewer**\n**parameters** .\n\n\n**4.3.2** **Scaling: Context Length**\n\n\nIn the next DNA experiment, we investigate the scaling properties of models with respect to sequence length. We only\ncompare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at longer sequence\nlengths. We pretrain models on sequence lengths 2 [10] = 1024, 2 [12] = 4096, 2 [14] = 16384, 2 [16] = 65536, 2 [18] = 262144,\n2 [20] = 1048576. We fix a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models were trained for 20 _\ud835\udc3e_\ngradient steps for a total of \u2248 330 _\ud835\udc35_ tokens. The longer sequence lengths used sequence length warmup similar to (Nguyen,\nPoli, et al. 2023).\n\n\n**Results.** Figure 5 ( _Right_ ) shows that **Mamba is able to make use of longer context even up to extremely long**\n**sequences of length 1M**, and its pretraining perplexity improves as the context increases. On the other hand, the\nHyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on properties of the\nselection mechanism. In particular, LTI models cannot selectively ignore information; from a convolutional perspective, a\nvery long convolution kernel is aggregating all information across a long sequence which may be very noisy. Note that\nwhile HyenaDNA claims to improve with longer context, their results do not control for computation time.\n\n\n**4.3.3** **Synthetic Species Classification**\n\n\nWe evaluate models on a downstream task of classifying between 5 different species by"," Pretraining\ndetails largely follow the standard language modeling setup (Section 4.2). Figure 7 evaluates the effect of increasing training\nsequence lengths from 2 [13] = 8192 to 2 [20] \u2248 10 [6], while keeping computation fixed. (There are some slight edge cases to the\nway the data is curated, which may lead to kinks in the scaling curves. For example, only minute-long clips were available\nso the maximum sequence length is actually bounded by 60 _\ud835\udc60_ - 16000 _\ud835\udc3b\ud835\udc67_ = 960000.)\n\n\n**Both Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is**\n**better throughout, and the gap widens at longer lengths.** The main metric is bits per byte (BPB), which is a constant\nfactor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.\n\n\nWe note one important detail: this is the only experiment in this paper in which we switched from the real parameterization\nto complex (Section 3.6). We show additional ablations in Appendix E.4.\n\n\n**4.4.2** **Autoregressive Speech Generation**\n\n\nSC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of\n1-second clips sampled at 16000 Hz of the digits \u201czero\u201d through \u201cnine\u201d with highly variable characteristics. We largely\nfollow the autoregressive training setup and generation protocol of Goel et al. (2022).\n\n\nTable 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al. (2022):\nWaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette 2019), DiffWave (Z.\nKong et al. 2021), and SaShiMi. **A small Mamba model outperforms the state-of-the-art (and much larger) GAN-**\n**and diffusion- based models.** A larger model parameter-matched to the baselines further improves on fidelity metrics\ndramatically.\n\n\nTable"],"24":[" Codes ChatGPT LLaMa Curation SFT\nWaveCoder (Yu et al., 2024) Code Existing Source Codes GPT4 LLaMa Curation SFT\nCode Alpaca (Chaudhary, 2023) Code Code Instructions ChatGPT LLaMA Expansion + Self-Knowledge SFT\nCode Llama (Rozi`ere et al., 2023) Code Human-written Instructions LLaMA LLaMA Expansion + Self-Knowledge SFT\nCode Clean (Jain et al., 2023) Code Code Datasets ChatGPT LLaMA Labeling SFT\n\n\n_**Multi-Modality**_\n\n\nLLaVA (Liu et al., 2023e) Vision-Language COCO GPT4 LLaMA Labeling SFT\nSVIT (Zhao et al., 2023b) Vision-Language Visual Genome + COCO GPT4 LLaMA Labeling SFT\nLVIS-Instruct4V (Wang et al., 2023e) Vision-Language LVIS GPT4V LLaMA Labeling SFT\nLLaVAR (Zhang et al., 2023d) Vision-Language LAION GPT4 LLaMA Labeling SFT\nMacaw-LLM (Lyu et al., 2023) Multiple Modalities Image\/Video with Caption ChatGPT LLaMA Labeling SFT\nMIMIC-IT (Li et al., 2023f) Multiple Modalities Image\/Video Dataset ChatGPT LLaMA Labeling SFT\nChatBridge (Zhao et al., 2023d) Multiple Modalities Task-Specific\/Multimodal-Chat Data GPT4 + ChatGPT LLaMA Labeling SFT\n\n\nTABLE 3: A summary of skill distillation works. IF: Instruction Following, MD: Multi-turn Dialoue, TP: Think Pattern,\nRAG: Retrieval-Augmented Generation, NLU: Natural Language Understanding, NLG: Natural Language Generation, IR:\nInformation Retrieval, SFT: Supervised Fine-Tuning, D&S: Divergence and Similarity, RL: Reinforcement Learning, RO:\nRanking Optimization.\n\n\n\nformats with templates, such as prefacing machine translation data with _\u201dTranslate this sentence to Spanish:\u201d_ . However,\nthese approaches have limitations. Manual data creation is\nlabor-intensive, while template-based","aca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryasov and Tastet, 2023) IF 10M-word BabyLM dataset GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S\nMiniLLM (Gu et al., 2024) IF Dolly Dataset GPT2 + OPT + LLaMA GPT2 + OPT + LLaMA Feature D&S\nSelf-Align (Sun et al., 2024b) IF Human-written Principles LLaMA LLaMA Expansion + Self-Knowledge SFT\nSelf-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL\nSTaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT\nLlama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT\nReflection-Tuning (Li et al., 2023e) IF Alpaca\/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nSelective Reflection-Tuning (Li et al., 2024d) IF Alpaca\/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nVicuna (Chiang et al., 2023) IF\/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT\nKoala (Geng et al., 2023) IF\/MD Human Conversation ChatGPT LLaMA Labeling SFT\nBaize (Xu et al., 2023b) IF\/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT\nUltraChat (Ding et al., 2023b) IF\/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT\nOrca (Mukherjee et al., 2023) IF\/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al","., 2023) IF\/Preference Mixed Datasets GPT4 Mistral Labeling + Feedback SFT + RO\nALMoST (Kim et al., 2023a) IF\/Preference Human-written Prompts LLaMA LLaMA Expansion + Labeling SFT + RL\nRLCD (Yang et al., 2024) IF\/Preference Human-written Prompts LLaMA LLaMA Labeling SFT + RL\nRLAIF (Lee et al., 2023a) IF\/Preference Human-written Prompts PaLM 2 PaLM 2 Labeling + Feedback RL\nGPT3 Reward (Kwon et al., 2023) Preference Human-written Prompts GPT3 GPT3 Labeling RL\nILF (Scheurer et al., 2023) Preference Task-specific Datasets GPT3 + FeedME GPT3 Labeling RL\nULTRAFEEDBACK (Cui et al., 2023a) Preference Mixed Datasets GPT4 LLaMA Labeling RL\nConstitutional AI (Bai et al., 2022a) Preference\/Value Human-written Prompts Self-defined Student Model Self-defined Model Labeling + Expansion + Feedback SFT + RL\n\nSANDBOX (Liu et al., 2023b) Value Simulation text-davinci-002\/-003 +GPT4 + ChatGPT LLaMA Data Curation SFT + RL\n\n\n_**Agent**_\n\n\nToolformer (Schick et al., 2023) Tool CCNet GPT-J GPT-J Labeling SFT\nGraph-ToolFormer (Zhang, 2023) Tool Mixed Graph Dataset ChatGPT GPT-J + LLaMA Labeling SFT\nGorilla (Patil et al., 2023) Tool Online API Documentation GPT4 LLaMA Expansion SFT\nGPT4Tools (Yang et al., 2023b) Tool Image Content ChatGPT LLaMA Curation + Expansion SFT\nToolAlpaca (Tang et al., 2023a) Tool Public-apis Repository ChatGPT LLaMA Curation SFT\nToolLLM (Qin et al., 2023a) Tool Real-world APIs ChatGPT LLaMA Curation SFT\nMLLM-Tool (Wang et al., 2024) Tool HuggingFace Model Cards GPT","e), biomedical\npapers (Luo et al., 2023e) medical knowledge-graphs (Bao\net al., 2023b), or authentic doctor-patient interactions (Bao\net al., 2023b), an expanding body of research is delving into the augmentation of medical instruction-following\ndata with advanced LLMs to enhance the alignment with\npractical user instructions. Zhang et al. (2023c) introduce\nHuatuoGPT specifically tailored for medical consultations.\nThe model leverages both _distilled data from ChatGPT_ and\n_real-world data from doctors_ during the supervised finetuning stage. In a parallel effort, Xiong et al. (2023) construct a dataset of medical dialogues in Chinese, employing ChatGPT\u2019s assistance. Their methodology encompassed various techniques to train DoctorGLM, an easily\ndeployable LLM designed for tasks such as diagnoses,\ndrug recommendations, and other medical advice. Zhang\net al. (2023f) fine-tune LLaMA-series models using 52k\ndiverse, machine-generated, medical instruction-following\ndata named MedInstruct-52k. This effort resulted in the\ndevelopment of AlpaCare, a model demonstrating robust\nmedical proficiency and generalizability across both general\nand medical-specific domain free-form instruction evaluations. In a different vein, Wang et al. (2023a) propose\nHuaTuo, a LLaMA-based model that undergoes supervised\nfine-tuning with generated QA instances. This refinement\nprocess enhances the model\u2019s possession of more reliable\nmedical knowledge. Li et al. (2023i) introduce ChatDoctor,\nwhich was first trained as a generic conversation model\nbased on LLaMA. It utilized 52K instruction-following data\n\n\nfrom Stanford University\u2019s Alpaca project (Taori et al.,\n2023). Subsequently, the conversation model underwent\nfine-tuning on a dataset of 100K patient-physician conversations collected from an online medical consultation website. This two-step training process underscores the model\u2019s\nadaptability to diverse conversational contexts, particularly\nthose specific to patient-physician interactions.\nBuilt upon existing datasets, MedAlpaca (Han et al.,\n2023) proposes to reconstruct the data with GPT-3.5-Turbo,\nwhich is then used to fine-tune LLMs for effective"," Learning, RO:\nRanking Optimization.\n\n\n\nformats with templates, such as prefacing machine translation data with _\u201dTranslate this sentence to Spanish:\u201d_ . However,\nthese approaches have limitations. Manual data creation is\nlabor-intensive, while template-based transformation lacks\ndiversity in instructions and may not align well with natural\nhuman input. LLMs like GPT-4 offer an efficient alternative\nfor creating diverse and controlled SFT data by their capabilities of in-context learning and instruction following. Most\n\n\n\nrelevant works use OpenAI\u2019s GPT series models to generate\nprompt-response data pairs and then train the student LLMs\nby supervised fine-tuning (Wang et al., 2022a; Taori et al.,\n2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a;\nMukherjee et al., 2023; Mitra et al., 2023; Luo et al., 2023b;\nPeng et al., 2023a).\n\n\n_**Basic Instructions.**_ Self-Instruct (Wang et al., 2022a) leverages the in-context learning capability of GPT-3 to expand\n\n\na seed pool of 175 tasks to 52K task-agnostic instructions,\nensuring a broad spectrum of general instructions. Additionally, a filtering and post-processing stage is introduced\nto eliminate redundant or similar instructions. Notably,\nthrough training with this enriched dataset, GPT-3 acquires\nthe ability to follow instructions, enabling it to perform\ncomparably to InstructGPT in zero-shot instruction tasks\nand when provided with expert-written instructions for\nnovel tasks. Based on the self-instruct method, Taori et al.\n(2023) train an Alpaca model using the Llama 7B model\n\n- n 52K instruction-following demonstrations, generated in\na similar style as self-instruct but utilizing the more robust\ntext-davinci-003 model. To enhance the diversity of instructional data, Wu et al. (2023c) introduce a technique known\nas _Topic-Guided Instruction Generation_ . This method involves\ngathering 3.5K common topics from Wikipedia to serve as\nguidance during the generation process.\n\n\n_**Complex Instructions.**_ Some works promote students to\nsolve more complex instructions (Xu et al.,"],"25":["aca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryasov and Tastet, 2023) IF 10M-word BabyLM dataset GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S\nMiniLLM (Gu et al., 2024) IF Dolly Dataset GPT2 + OPT + LLaMA GPT2 + OPT + LLaMA Feature D&S\nSelf-Align (Sun et al., 2024b) IF Human-written Principles LLaMA LLaMA Expansion + Self-Knowledge SFT\nSelf-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL\nSTaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT\nLlama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT\nReflection-Tuning (Li et al., 2023e) IF Alpaca\/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nSelective Reflection-Tuning (Li et al., 2024d) IF Alpaca\/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nVicuna (Chiang et al., 2023) IF\/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT\nKoala (Geng et al., 2023) IF\/MD Human Conversation ChatGPT LLaMA Labeling SFT\nBaize (Xu et al., 2023b) IF\/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT\nUltraChat (Ding et al., 2023b) IF\/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT\nOrca (Mukherjee et al., 2023) IF\/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al"," Codes ChatGPT LLaMa Curation SFT\nWaveCoder (Yu et al., 2024) Code Existing Source Codes GPT4 LLaMa Curation SFT\nCode Alpaca (Chaudhary, 2023) Code Code Instructions ChatGPT LLaMA Expansion + Self-Knowledge SFT\nCode Llama (Rozi`ere et al., 2023) Code Human-written Instructions LLaMA LLaMA Expansion + Self-Knowledge SFT\nCode Clean (Jain et al., 2023) Code Code Datasets ChatGPT LLaMA Labeling SFT\n\n\n_**Multi-Modality**_\n\n\nLLaVA (Liu et al., 2023e) Vision-Language COCO GPT4 LLaMA Labeling SFT\nSVIT (Zhao et al., 2023b) Vision-Language Visual Genome + COCO GPT4 LLaMA Labeling SFT\nLVIS-Instruct4V (Wang et al., 2023e) Vision-Language LVIS GPT4V LLaMA Labeling SFT\nLLaVAR (Zhang et al., 2023d) Vision-Language LAION GPT4 LLaMA Labeling SFT\nMacaw-LLM (Lyu et al., 2023) Multiple Modalities Image\/Video with Caption ChatGPT LLaMA Labeling SFT\nMIMIC-IT (Li et al., 2023f) Multiple Modalities Image\/Video Dataset ChatGPT LLaMA Labeling SFT\nChatBridge (Zhao et al., 2023d) Multiple Modalities Task-Specific\/Multimodal-Chat Data GPT4 + ChatGPT LLaMA Labeling SFT\n\n\nTABLE 3: A summary of skill distillation works. IF: Instruction Following, MD: Multi-turn Dialoue, TP: Think Pattern,\nRAG: Retrieval-Augmented Generation, NLU: Natural Language Understanding, NLG: Natural Language Generation, IR:\nInformation Retrieval, SFT: Supervised Fine-Tuning, D&S: Divergence and Similarity, RL: Reinforcement Learning, RO:\nRanking Optimization.\n\n\n\nformats with templates, such as prefacing machine translation data with _\u201dTranslate this sentence to Spanish:\u201d_ . However,\nthese approaches have limitations. Manual data creation is\nlabor-intensive, while template-based"," typed as above. We sample new entities uniform randomly from the 5% least common entities\nin NewsCrawl ( _I_ _[RandArg][\u2193]_ ), and the 5% most common ( _I_ _[RandArg][\u2191]_ ). We insert the sampled entities\nwhile preserving the rest of each statement.\n\n\n**4** **Querying Models with Prompts**\n\n\n**4.1** **Models**\n\n\n**LLaMA** is a recent LLM model family which\nrivals or surpasses GPT-3 performance while being\n\n- pen to scientific study. A range of model sizes\n\n\nare provided, and we test the largest **LLaMA-65B**\nmodel. LLaMA is not fine-tuned. In preliminary\nexperiments on the Levy\/Holt dataset, we found\ntwo popular fine-tuned LLaMA variants, Alpaca\n(Taori et al., 2023) and Vicuna (Chiang et al., 2023),\nperform similarly to LLaMA base models and underperform LLaMA-65B, so we leave them out of\nfurther experiments.\n\n\n**GPT-3 Series** models are closed to deep scientific review (Brown et al., 2020), though they are a\nwidely-used comparison for their performance, and\nhave been reasonably well-studied. We evaluate on\n**text-davinci-003 (GPT-3.5)**, as it is the largest, and\nhas undergone instruction- and RLHF-finetuning,\nenabling interesting comparisons.\n\n\n**PaLM** is larger than GPT-3, which often claims\nstate-of-the-art on evaluation datasets. We use the\n\nlargest **PaLM-540B** base model, which is also only\npretrained, so it serves as a further comparison\npoint to LLaMA.\n\n\nLater GPT models (like text-davinci-003 in our\nexperiments) have been pre-trained and fine-tuned,\nwhile base LLaMA and PaLM have only undergone pre-training, so their contrast indicates what\nstage of training is responsible for the phenomena\nwe study. Our aim is not to judge which LLM\nis superior, but to show the common sources of\nhallucination they share.\nWe also omit models superseded in performance\nby LLaMA (e.g. OPT, GPT-J, etc","IF XSum+WMT14 en-de+GSM8K+FLAN2021 T5-XL T5 Feature + Feedback D&S + RL\nQUILL (Srinivasan et al., 2022) IR IR Datasets T5 4-layer Transformer Internal Knowledge D&S\nRankVicuna (Pradeep et al., 2023a) IR IR Datasets ChatGPT LLaMA Labeling SFT\nRankZephyr (Pradeep et al., 2023b) IR IR Datasets ChatGPT + GPT4 Mistral Labeling SFT\nNDR (Mysore et al., 2023) Recommendation Recommendation Datasets GPT3 MPnet-110M Labeling SFT\nInstrcutRec (Zhang et al., 2023b) Recommendation 39 instruction templates ChatGPT Flan-T5 Expansion + Self-Knowledge SFT\nONCE (Liu et al., 2023c) Recommendation Recommendation Dataset ChatGPT LLaMA Labeling SFT\nPandaLM (Wang et al., 2023b) Evaluation Alpaca Data ChatGPT LLaMA Labeling SFT\nPrometheus (Kim et al., 2024) Evaluation 50 Seed Rubrics GPT4 LLaMA Labeling SFT\nInstructScore (Xu et al., 2023d) Evaluation Mixed Dataset GPT4 LLaMA Labeling SFT\nWizardMath (Luo et al., 2023b) Math GSM8k + MATH ChatGPT LLaMA Expansion + Feedback SFT + RL\nMammoth (Yue et al., 2023a) Math\/TP Mixed Math Dataset GPT4 LLaMA Labeling SFT\nMixed Distill (Chenglin et al., 2023) Math\/TP SVAMP + GSM8K + ASDIV + StrategyQA ChatGPT LLaMa Labeling SFT\nWizardCoder (Luo et al., 2023a) Code Code Alpaca Data ChatGPT StarCoder Expansion SFT\nMagicoder (Wei et al., 2023) Code Existing Source Codes ChatGPT LLaMa Curation SFT\nWaveCoder (Yu et al., 2024) Code Existing Source Codes GPT4 LLaMa Curation SFT\nCode Alpaca (Chaudhary, 202","., 2023) IF\/Preference Mixed Datasets GPT4 Mistral Labeling + Feedback SFT + RO\nALMoST (Kim et al., 2023a) IF\/Preference Human-written Prompts LLaMA LLaMA Expansion + Labeling SFT + RL\nRLCD (Yang et al., 2024) IF\/Preference Human-written Prompts LLaMA LLaMA Labeling SFT + RL\nRLAIF (Lee et al., 2023a) IF\/Preference Human-written Prompts PaLM 2 PaLM 2 Labeling + Feedback RL\nGPT3 Reward (Kwon et al., 2023) Preference Human-written Prompts GPT3 GPT3 Labeling RL\nILF (Scheurer et al., 2023) Preference Task-specific Datasets GPT3 + FeedME GPT3 Labeling RL\nULTRAFEEDBACK (Cui et al., 2023a) Preference Mixed Datasets GPT4 LLaMA Labeling RL\nConstitutional AI (Bai et al., 2022a) Preference\/Value Human-written Prompts Self-defined Student Model Self-defined Model Labeling + Expansion + Feedback SFT + RL\n\nSANDBOX (Liu et al., 2023b) Value Simulation text-davinci-002\/-003 +GPT4 + ChatGPT LLaMA Data Curation SFT + RL\n\n\n_**Agent**_\n\n\nToolformer (Schick et al., 2023) Tool CCNet GPT-J GPT-J Labeling SFT\nGraph-ToolFormer (Zhang, 2023) Tool Mixed Graph Dataset ChatGPT GPT-J + LLaMA Labeling SFT\nGorilla (Patil et al., 2023) Tool Online API Documentation GPT4 LLaMA Expansion SFT\nGPT4Tools (Yang et al., 2023b) Tool Image Content ChatGPT LLaMA Curation + Expansion SFT\nToolAlpaca (Tang et al., 2023a) Tool Public-apis Repository ChatGPT LLaMA Curation SFT\nToolLLM (Qin et al., 2023a) Tool Real-world APIs ChatGPT LLaMA Curation SFT\nMLLM-Tool (Wang et al., 2024) Tool HuggingFace Model Cards GPT"],"26":[" with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does not know, it can still refer to the context.\n\n\n4.1 Results\n\n\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\n\n- utperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes\/no question, we don\u2019t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.\n\n\nOverall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its\nanswering style not aligning with the ground truth. By applying domain-specific tuning,\nwe significantly enhance its performance. This process enables the model to learn and adopt\nthe appropriate style of answering. However, introducing RAG to a domain-specifically\nfine-tuned (DSF) model doesn\u2019t invariably lead to better outcomes. This might indicate that\nthe model lacks training in context processing and extracting useful information from it. By\nincorporating our method, RAFT, we train the model not only to match its answering style\nwith that required but also to improve its document processing capabilities. Consequently,\n\n- ur approach outperforms all others.\n\n\n4.2 Effect of CoT\n\n\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\nin enhancing the model\u2019s performance. As indicated in Table 2, simply providing the answer\nto a question","Preprint, Under Review\n\n## **RAFT: Adapting Language Model to Domain Specific RAG**\n\n\n\n**Tianjun Zhang** _[\u2217]_\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n{tianjunz}@berkeley.edu\n\n```\n\n\n**Shishir G. Patil, Naman Jain, Sheng Shen**\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\n\n```\n\n\n**Matei Zaharia, Ion Stoica, Joseph E. Gonzalez**\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n {matei,istoica,jegonzal}@berkeley.edu\n\n```\n\n**Abstract**\n\n\nPretraining Large Language Models (LLMs) on large corpora of textual\ndata is now a standard paradigm. When using these LLMs for many\ndownstream applications, it is common to additionally incorporate new information into the pretrained model either through RAG-based-prompting,\n\n    - r finetuning. However, the best methodology to incorporate information\nremains an open question. In this paper, we present Retrieval Augmented\nFine Tuning (RAFT), a training recipe which improves the model\u2019s ability\nto answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don\u2019t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT\u2019s chain-of-thought-style response helps improve\nthe model\u2019s ability to reason. In domain specific RAG, RAFT consistently\nimproves the model\u2019s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n\n\n**1** **Introduction**\n\n\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized\ndomains","underscores the importance of incorporating both standard instructional tuning and context\ncomprehension into the training dataset to preserve and enhance the model\u2019s ability to\nprocess text effectively.\n\n\nFigure 4: **Comparison of RAFT and DSF** : On the HotPot QA dataset, we can see that DSF\nmodel extracts the wrong information from the context when the question is asking who is\nthe screen writer and it answers a film name. RAFT manages to get the accurate results .\n\n\n4.4 Should we train the LLM always with the golden context for RAG?\n\n\nIn our exploration of whether large language models (LLMs) should always be trained with\nthe golden context for Retrieval-Augmented Generation (RAG), we address a key question:\nwhat proportion (p%) of the training data should include golden documents? Intuitively,\n\n- ne might assume that for effective training in reading and extracting information from\ncontext (e.g., RAG tasks), the golden document should always be included during training\n(P = 100%). However, our findings challenge this assumption: incorporating a portion of\nthe training data without the golden document in the context (P = 80%) appears to enhance\nthe model\u2019s performance on RAG tasks.\n\n\n7\n\n\nPreprint, Under Review\n\n\n\n\n\n\n\n\n\n\n|Test Domain: NQ|Col2|Col3|ain: NQ|Col5|Col6|\n|---|---|---|---|---|---|\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ|ain: NQ|ain: NQ|ain: NQ|\n|0<br>20<br>40<br>60<br>80<br>100<br>"," Wei et al. (2022); Anthropic\n(2023), and in the presence of distractor documents ( _Dk_ ). We explain the methodology in\nSection 3 and analyze the sensitivity to the number of distractor documents ( _k_ ) at train- and\ntest- time in Section 5. RAFT consistently outperforms Supervised-finetuning both withand without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),\nand HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),\npresenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.\nOur code is available at `[https:\/\/github.com\/ShishirPatil\/gorilla](https:\/\/github.com\/ShishirPatil\/gorilla)` .\n\n\n**2** **LLMs for Open-Book Exam**\n\n\nTo understand our goal better, we expand on our analogy between training an LLM with\nthe real-world setting of prepararing for an exam.\n\n\n**Closed-Book Exam** A closed book exam often refers to the scenario where the LLMs do\nnot have access to any additional documents or references to answer the questions during\n\n\n2\n\n\nPreprint, Under Review\n\n\nFigure 2: **Overview of our RAFT method.** The top-left figure depicts our approach of\nadapting LLMs to _reading_ solution from a set of positive and distractor documents in\ncontrast to standard RAG setup where models are trained based on the retriever outputs,\nwhich is a mixture of both memorization and reading. At test time, all methods follow the\nstandard RAG setting, provided with a top-k retrieved documents in the context.\n\n\nthe exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is\nused as a chatbot. In this scenario the LLM draws from the knowledge baked in during\npre-training and supervised-finetuning to respond to the users\u2019 prompt.\n\n\n**Open Book Exam** In contrast, we liken the open-book exam setting to the scenario in\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\nIn such scenarios, typically, the LLM is paired with retriever which retrieves \u2018k\u2019 documents\n(or specific segments of the"," LLM without the correct corresponding context at times can be\nbeneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we maintain this format by providing the golden document with four distractors.\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of\ntraining data without the golden documents in the context proves to be advantageous.\n\n\n**5** **RAFT Generalizes to Top-K RAG**\n\n\nWe now study another important problem: How does the number of distractor documents\nin RAFT affect the model\u2019s performance when augmented with top-k RAG results during\nevaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text\n(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is\nparticularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to\nensure high recall. Such a scenario necessitates the model to have the ability to discern and\ndisregard irrelevant content, focusing solely on pertinent information.\n\n\n5.1 Making Model Robust to top-K RAG\n\n\nTo tackle the challenge of enhancing large language models\u2019 (LLMs) ability to sift through\nirrelevant text within the retrieval pipeline, our analysis revealed that training solely with\ngolden (highly relevant) documents can inadvertently diminish the model\u2019s ability to discern and disregard irrelevant information. To address this, our algorithm, RAFT, adopts\na strategy that integrates golden documents with a mix of irrelevant ones. This method\n- logy prompts us to investigate the ideal fraction of distractor (irrelevant) documents to\nincorporate throughout the training process and to assess how well this training approach\nadapts to different volumes of documents encountered by the Retrieval-Augmented Generation (RAG) during the test phase. Our aim is to refine the balance between relevant\nand irrelevant information to strenghten the model\u2019s efficiency in identifying and utilizing\npertinent content. Notice that Sec 4.4 looked what what P% of training data should include\ndistractors, while in this section, we study test-time scenarios.\n\n\n**Training with Distractor Documents** To enhance the robustness of LLMs against irrelevant\ntext in retrieved documents, we adopted a fin"],"27":[" for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n\n\n_B. Recursive Retrieval_\n\n\nRecursive retrieval is often used in information retrieval and\n\nNLP to improve the depth and relevance of search results.\nThe process involves iteratively refining search queries based\n\n- n the results obtained from previous searches. Recursive\nRetrieval aims to enhance the search experience by gradually converging on the most pertinent information through a\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\nthe retrieval process and refines the CoT with the obtained\nretrieval results. ToC [57] creates a clarification tree that\nsystematically optimizes the ambiguous parts in the Query. It\ncan be particularly useful in complex search scenarios where\nthe user\u2019s needs are not entirely clear from the outset or where\nthe information sought is highly specialized or nuanced. The\nrecursive nature of the process allows for continuous learning\nand adaptation to the user\u2019s requirements, often resulting in\nimproved satisfaction with the search outcomes.\nTo address specific data scenarios, recursive retrieval and\nmulti-hop retrieval techniques are utilized together. Recursive\n\n\n\nretrieval involves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\n\n\n_C. Adaptive Retrieval_\n\n\nAdaptive retrieval methods, exemplified by Flare [24] and\nSelf-RAG [25], refine the RAG framework by enabling LLMs\nto actively determine the optimal moments and content for\nretrieval, thus enhancing the efficiency and relevance of the\ninformation sourced.\n\nThese methods are part of a broader trend wherein\nLLMs employ active judgment in their operations, as seen\nin model agents like AutoGPT, Toolformer, and GraphToolformer [107]\u2013[109]. Graph-Toolformer, for instance, divides its retrieval process into distinct steps where LLMs\nproactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive\nstance allows LLMs to decide when to search"," for instance, divides its retrieval process into distinct steps where LLMs\nproactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive\nstance allows LLMs to decide when to search for necessary\ninformation, akin to how an agent utilizes tools.\nWebGPT [110] integrates a reinforcement learning framework to train the GPT-3 model in autonomously using a\nsearch engine during text generation. It navigates this process\nusing special tokens that facilitate actions such as search\nengine queries, browsing results, and citing references, thereby\nexpanding GPT-3\u2019s capabilities through the use of external\nsearch engines. Flare automates timing retrieval by monitoring\nthe confidence of the generation process, as indicated by the\n\n\nprobability of generated terms [24]. When the probability falls\nbelow a certain threshold would activates the retrieval system\nto collect relevant information, thus optimizing the retrieval\ncycle. Self-RAG [25] introduces \u201creflection tokens\u201d that allow\nthe model to introspect its outputs. These tokens come in\ntwo varieties: \u201cretrieve\u201d and \u201ccritic\u201d. The model autonomously\ndecides when to activate retrieval, or alternatively, a predefined\nthreshold may trigger the process. During retrieval, the generator conducts a fragment-level beam search across multiple\nparagraphs to derive the most coherent sequence. Critic scores\nare used to update the subdivision scores, with the flexibility\nto adjust these weights during inference, tailoring the model\u2019s\nbehavior. Self-RAG\u2019s design obviates the need for additional\nclassifiers or reliance on Natural Language Inference (NLI)\nmodels, thus streamlining the decision-making process for\nwhen to engage retrieval mechanisms and improving the\nmodel\u2019s autonomous judgment capabilities in generating ac\ncurate responses.\n\n\nVI. TASK AND EVALUATION\n\n\nThe rapid advancement and growing adoption of RAG\nin the field of NLP have propelled the evaluation of RAG\nmodels to the forefront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop\/multi-hop QA, multiplechoice, domain-specific QA as well as","-time\nretrieval augmented LMs, which is the most relevant area to this paper.\n\n\n**Iterative and adaptive retrieval** Iterative retrieval and refinement has been studied in both\ntext and code generation tasks (Peng et al., 2023;\nZhang et al., 2023; Zemlyanskiy et al., 2022; Yu\net al., 2023). FLARE differs from these methods in\nthe granularity of generation and retrieval strategies.\nAdaptive retrieval has been studied in single-time\nretrieval scenarios based on either question popularity or generation probabilities (Mallen et al.,\n2022; Li et al., 2023), while we focus on long-form\ngeneration requiring active information access.\n\n\n**Browser-enhanced LMs** WebGPT (Nakano\net al., 2021) and WebCPM (Qin et al., 2023) train\nLMs to interact with browser to enhance factuality\nusing reinforcement learning or supervised training where multiple queries can be triggered before\ngeneration. FLARE is built on text-based retrievers\nbut can be combined with a browser to potentially\nimprove retrieval quality.\n\n\n\n**8** **Conclusion**\n\n\nTo aid long-form generation with retrieval augmentation, we propose an active retrieval augmented generation framework that decides when\nand what to retrieve during generation. We implement this framework with forward-looking active\nretrieval that iteratively uses the upcoming sentence\nto retrieve relevant information if it contains low\nconfidence tokens and regenerates the next sentence. Experimental results on 4 tasks\/datasets\ndemonstrate the effectiveness of our methods. Fu\nture directions include better strategies for active\nretrieval and developing efficient LM architectures\nfor active information integration.\n\n\n**9** **Limitations**\n\n\nWe also conduct experiments on Wizard of\nWikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,\n\n2019), and found that FLARE did not provide significant gains. Wizard of Wikipedia is a knowledgeintensive dialogue generation dataset where the out\n_\u223c_\nput is relatively short ( 20 tokens on average) so\nretrieving multiple disparate pieces of information\nmight not be necessary. ELI5 (Fan et al., 2019)\nis a long-form QA dataset requiring in-depth answers to open-ended questions. Due to issues mentioned in Krishna et al. (2021) such as difficulties\n\n-"," effective method is to distill the more\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\nbe coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT [27], aligns the\nscoring functions between Retriever and Generator using KL\ndivergence.\n\n\nV. AUGMENTATION PROCESS IN RAG\n\n\nIn the domain of RAG, the standard practice often involves\na singular (once) retrieval step followed by generation, which\ncan lead to inefficiencies and sometimes is typically insufficient for complex problems demanding multi-step reasoning,\nas it provides a limited scope of information [105]. Many\nstudies have optimized the retrieval process in response to this\nissue, and we have summarised them in Figure 5.\n\n\n_A. Iterative Retrieval_\n\n\nIterative retrieval is a process where the knowledge base\nis repeatedly searched based on the initial query and the text\ngenerated so far, providing a more comprehensive knowledge\n\n\n11\n\n\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\n\n\n\nbase for LLMs. This approach has been shown to enhance\nthe robustness of subsequent answer generation by offering\nadditional contextual references through multiple retrieval\niterations. However, it may be affected by semantic discontinuity and the accumulation of irrelevant information. ITERRETGEN [14] employs a synergistic approach that leverages \u201cretrieval-enhanced generation\u201d alongside \u201cgenerationenhanced retrieval\u201d for tasks that necessitate the reproduction\n\n- f specific information. The model harnesses the content\nrequired to address the input task as a contextual basis for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n\n\n_B. Recursive Retrieval_\n\n\nRecursive retrieval is often used in information retrieval and\n\nNLP to improve the depth and relevance of search results","** **FLARE Implementation Details**\n\n\n**FLAREinstruct** **implementation** **details** We\nfound that LMs can effectively combine retrieval\nand downstream task-related skills and generate\nmeaningful search queries while performing the\ntask. However, there are two issues: (1) LMs tend\nto generate fewer search queries than necessary.\n(2) Generating excessive search queries can\ndisrupt answer generation and adversely affect\nperformance. We address these issues using two\nmethods respectively. First, we increase the logit\n\n- f the token \u201c[\u201d by 2.0 to improve the chances\n\n- f LMs generating \u201c[Search(query)]\u201d. Second,\nwhenever LMs generate a search query, we use it\nto retrieve relevant information, promptly remove\nit from the generation, and generate the next few\ntokens while forbidding \u201c[\u201d by adding a large\nnegative value to the logit of \u201c[\u201d.\n\n\n**The initial query of FLARE.** FLARE starts\nwith the user input _**x**_ as the initial query to retrieve documents to generate the first sentence\n_**s**_ \u02c61 = LM([ _D_ _**x**_ _,_ _**x**_ ]) to bootstrap the iterative generation process. For the following steps, the temporary forward-looking sentence is generated without\nretrieved documents.\n\n\n**Sentence tokenization.** For each step _t_, we generate 64 tokens which are longer than most sentences, and use NLTK sentence tokenizer [5] to extract the first sentence and discard the rest.\n\n\n**Efficiency** As shown in subsection 6.2, on average retrieval is triggered for 30% _\u223c_ 60% of sentences depending on downstream tasks. In comparision, KNN-LM (Khandelwal et al., 2020) retrieves\nevery token, RETRO or IC-RALM (Borgeaud et al.,\n2022; Ram et al., 2023) retrievers every 4 _\u223c_ 32 tokens, and IRCoT (Trivedi et al., 2022) retrieves\nevery sentence. Compared to single-time retrieval,\nhowever, interleaving retrieval and generation with\na naive implementation indeed increases overheads,\nwhich we discuss in the limitation section (section 9).\n\n\n**B** **Datasets and Settings**\n\n\nDatasets, metrics, and experimental settings are\nsummarized in Table"],"28":[" time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine"," action space design (i.e. models cannot really buy products on WebShop the research\nbenchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing\nmore extensive experiments in the future.\n\n\nREFERENCES\n\n\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita\nChhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim\nHarley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson, So\u02c7na Mokr\u00e1, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,\nDuncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,\n[2020. URL https:\/\/arxiv.org\/abs\/2012.05672.](https:\/\/arxiv.org\/abs\/2012.05672)\n\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\nAndy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL\n[https:\/\/arxiv.org\/abs\/2204.01691.](https:\/\/arxiv.org\/abs\/2204.01691)\n\n\nBen Alderson-Day and Charles Fernyhough"," improves model performance. We hypothesize that abstraction helps models to hallucinate\nless and reason better, probably reflecting the true nature of the model which are often hidden\nwhile responding to the original question without abstraction. We hope our work will inspire more\nhuman-inspired approaches to elicit the hidden potential of large language models.\n\n\n9\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nACKNOWLEDGEMENT\n\n\nWe thank Andrew Dai, Adams Yu and Hannah Rashkin of Google DeepMind for their insightful\nfeedback on this paper.\n\n\nREFERENCES\n\n\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv_\n_preprint arXiv:2305.10403_, 2023.\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. _Advances in neural information processing systems_, 33:1877\u20131901, 2020.\n\n\nWenhu Chen, Xinyi Wang, and William Yang Wang. A dataset for answering time-sensitive questions.\n_arXiv preprint arXiv:2108.06314_, 2021.\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n\n\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies. _Transactions of_\n_the Association for","aclanthology.org\/N18-1074)\n\n\n14\n\n\nPreprint.\n\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. _arXiv preprint arXiv:2307.09288_ [, 2023. URL https:\/\/arxiv.](https:\/\/arxiv.org\/abs\/2307.09288)\n\n[org\/abs\/2307.09288.](https:\/\/arxiv.org\/abs\/2307.09288)\n\n\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\nexploring the state of instruction tuning on open resources. _arXiv preprint arXiv:2306.04751_, 2023.\n[URL https:\/\/arxiv.org\/abs\/2306.04751.](https:\/\/arxiv.org\/abs\/2306.04751)\n\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In _International_\n_Conference on Learning Representations_ [, 2022. URL https:\/\/openreview.net\/forum?](https:\/\/openreview.net\/forum?id=gEZrGCozdqR)\n[id=gEZrGCozdqR.](https:\/\/openreview.net\/forum?id=gEZrGCozdqR)\n\n\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\nrewards for language model training. _arXiv preprint arXiv:2306.01693_ [, 2023. URL https:](https:\/\/arxiv.org\/abs\/2306","1.3_ _Data Curation_\n\nThe pursuit of high-quality and scalable data generation in\nknowledge distillation from LLMs has led to the emergence\n\n- f the Data Curation approach. This method arises in response to the limitations observed in both the Labeling and\nExpansion approaches. These methods often yield data of\nvariable quality and face constraints in quantity. In Labeling,\nthe seed knowledge is sourced from task datasets, leading\nto potential noise and dirty data. Meanwhile, in Expansion,\nthe input _x_ is derived from seed demonstrations, which\ncan result in homogeneous data when generated in large\nquantities. To overcome these challenges, the Data Curation\nmethod curates high-quality or large-scale data by extensive\nmeta-information as seed knowledge (Ding et al., 2023b;\nGunasekar et al., 2023; Li et al., 2023a; Mar, 2023; Liu et al.,\n2023d; Wei et al., 2023; Yu et al., 2024; Ye et al., 2022; Gao\net al., 2023a; Yang and Nicolai, 2023).\n\n\nA distinct feature of Data Curation is its approach\nto synthesize data from scratch. Numerous diverse metainformation, such as topics or knowledge points, could be\nincorporated into this process to generate controllable _x_\nand _y_ . Thus, this process can be meticulously controlled\nto yield datasets that are not only large in scale but also\n\n- f high quality. The formulation for Data Curation can be\nrepresented as:\n\n\n_D_ [(cur)] = _{_ ( _x, y_ ) _|x \u223c_ _pT_ ( _x|I \u2295_ _m_ ) _, y \u223c_ _pT_ ( _y|I \u2295_ _x_ ) _}._ (5)\n\n\nIn this formulation, _m_ represents the diverse metainformation used to guide the synthesis of _x_, and _I_ is the\ninstruction guiding teacher LLMs to generate _x_ - r _y_ .\nDifferent studies primarily vary in their source and\nmethod of leveraging meta-information. UltraChat (Ding\net al., 2023b) effectively demonstrates the process of curating\nboth high-quality and diverse data by distilled knowledge.\nThey collect extensive meta-information across three domains: _"],"29":[" as well as emergent abilities (Wei et al., 2022a) such as multi-step reasoning (Wei et al.,\n2022b; Zhou et al., 2022) and instruction following (Mishra et al., 2022b; Wei et al., 2021).\n\n\nFigure 1: Strong Performance of STEP-BACK PROMPTING: our proposed Abstraction-and-Reasoning\nscheme leads to a substantial improvement in a wide range of challenging tasks in STEM, Knowledge\nQA and Multi-Hop Reasoning requiring complex (often multi-hop) reasoning.\n\n\n_\u2217_ Equal Contribution\n\n\n1\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nDespite the great advancements, complex multi-step reasoning remains challenging for even the state\n- f-the-art LLMs. Lightman et al. (2023) show that process-supervision with step-by-step verification\nis a promising remedy to improve the correctness of intermediate reasoning steps. Techniques such\nas Chain-of-Thought (Wei et al., 2022b) were introduced to produce a coherent series of intermediate\nreasoning steps to increase the success rate of following the right decoding path. Inspired by the\nfact that when faced with challenging tasks humans often step back and do abstractions to arrive at\nhigh-level principles to guide the process, we propose STEP-BACK PROMPTING to ground reasoning\n\n- n abstractions to reduce the chance of making errors in the intermediate reasoning steps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of STEP-BACK PROMPTING with two steps of Abstraction and Reasoning\nguided by concepts and principles. _Top_ : an example of MMLU high-school physics (Hendrycks et al.,\n2020) where the first principle of Ideal Gas Law is retrieved via abstraction. _Bottom_ : an example\nfrom TimeQA (Chen et al., 2021) where the high-level concept of education history is a result of the\nabstraction. _Left_ : PaLM-2L (Anil et al., 2023) fails to answer the original question. Chain-of-Thought\nprompting (Wei et al., 2022b; Kojima et al., 2022) ran into errors during intermediate reasoning\nsteps (highlighted as red). _Right_ : PaLM-2L (Anil et al., 2023) successfully answers the question via"," 4 trials, the model scoring agrees with human ratings 97%, 98%, 99% and 99%\n\n- f the time.\n\n\nD PROMPTS AND FEW SHOT EXAMPLES\n\n\nD.1 STEM\n\n\nFor MMLU high-school Physics and Chemistry, we first prompt the model to generate the first\nprinciples behind the question. Using the generated first principles, we further prompt the model to\ngenerate the final answer through few-shot demonstrations The prompt generating first principles is\nshown in Table 7 for MMLU high-school Physics and Chemistry.\n\n\n15\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nMMLU Physics\/Chemistry Final Answer Prompt\n\n\nYou are an expert at Physics\/Chemistry. You are given a\nPhysics\/Chemistry problem and a set of principles involved in\nsolving the problem. Solve the problem step by step by following the\nprinciples. Here are a few examples:\n\n\nQuestion: _<_ Question Example1 _>_\nPrinciples: _<_ Principles Example1 _>_\nAnswer: _<_ Answer Example1 _>_\n\n...\nQuestion: _<_ Question Example5 _>_\nPrinciples: _<_ Principles Example5 _>_\nAnswer: _<_ Answer Example5 _>_\nQuestion: _<_ Question _>_\nPrinciples: _<_ Principles _>_\nAnswer:\n\n\nTable 8: Prompt of querying the model for final answer with first principles behind the question in\nMMLU high-school Physics and Chemistry.\n\n\nAfter extracting the first principles of solving a particular question, we formulate the prompt in\nTable 8 to query the model for the final answer.\n\n\nTables 9-10 show one demonstration exemplar of Question-Principles-Answer triplets for MMLU\nhigh-school Physics and Chemistry, respectively. For GSM8K, given the simplicity of the principles,\nwe directly combine the principles and the solution in the demonstration exemplar. Table 11 shows\nthe exemplar we used in the paper.\n\n\nD.2 KNOWLEDGE QA\n\n\nWe use the following prompting in Table 12 to demonstrate to the LLM on asking a step-back question\nfor TimeQA and SituatedQA including up to 5 exemplar demonstrations of pairs of Original Question\nand Step-back Question.\n\n\nTable 13 shows 5 exemplars from the Train split of TimeQA and SituatedQA as demonstrations of\nasking step-back questions.\n\n\nThe step-back"," MMLU and GSM8K for STEM tasks. MMLU (Hendrycks et al., 2020)\ncontains a series of benchmarks across diverse domains to evaluate the model\u2019s language\nunderstanding. We consider the high school physics and chemistry portions of MMLU\nbecause of the deep reasoning involved.\n\n    - **Knowledge QA** : We consider TimeQA (Chen et al., 2021) since it contains complex\nqueries that require challenging time-sensitive knowledge. We also experiment with SituatedQA (Zhang & Choi, 2021), another challenging open-retrieval QA dataset requiring the\nmodel to answer questions given temporal or geographical contexts.\n\n    - **Multi-Hop Reasoning** : We experiment with MuSiQue (Trivedi et al., 2022), a hard multihop\nreasoning dataset created via composable pairs of single-hop questions, and StrategyQA\n(Geva et al., 2021) with open-domain questions that demand some strategy to solve.\n\n\n3.2 MODELS\n\n\nWe use the following state-of-the-art LLMs: instruction-tuned PaLM-2L (Anil et al., 2023), GPT4 (OpenAI, 2023), and Llama2-70B (Touvron et al., 2023).\n\n\n3.3 EVALUATION\n\n\nConventional evaluation metrics such as accuracy, F1 score have limitations specifically for evaluating\nthe generations of state-of-the-art LLMs since these models often generate long-form answers which\nare hard to capture. We instead conduct an evaluation using the PaLM-2L model where we few-shot\nprompt the model to identify equivalence between target answers and the model predictions. Few-shot\nexamples, prompts and other details used for this evaluation are in Appendix C.\n\n\n3.4 BASELINE METHODS\n\n\n    - **PaLM-2L, PaLM-2L 1-shot** : PaLM-2L is either queried directly with the question or has a\nsingle demonstration exemplar of question-answer included in the prompt.\n\n    - **PaLM-2L + CoT, PaLM-2L + CoT 1-shot** : PaLM-2L model is queried with zero-shot\nCoT prompting (Kojima et al., 2022): \u201c _Let\u2019s think step by step_ \u201d is appended to the question.\nFor 1-shot, One demonstration example","b; Kojima et al., 2022) ran into errors during intermediate reasoning\nsteps (highlighted as red). _Right_ : PaLM-2L (Anil et al., 2023) successfully answers the question via\nSTEP-BACK PROMPTING.\n\n\nAmong many of the cognitive skills, abstraction (Lachmy et al., 2022) is ubiquitous to humans\u2019\nability to process vast amounts of information and derive general principles. For example, Kepler\ncompressed thousands of measurements into Kepler\u2019s three laws of planetary motion, which precisely\ndescribe the orbits of planets around the Sun (Russell, 1964). In critical decision-making, humans\nfind abstraction to be helpful since it provides a broader view of the environment. This work explores\nhow LLMs can tackle complex tasks involving many low-level details through a two-step process\n\n- f abstraction-and-reasoning. The first step is to show LLMs how to step back through in-context\nlearning \u2013 prompting them to derive high-level abstractions such as concepts and principles for\na specific example. The second step is to leverage the reasoning ability to reason on top of the\nhigh-level concepts and principles. We use few-shot exemplar demonstrations to execute STEP-BACK\nPROMPTING on LLMs.\n\n\nWe experiment across a range of tasks involving domain specific reasoning such as Physics and Chemistry, knowledge-intensive question answering requiring factual knowledge, multi-hop commonsense\nreasoning. We observe significant performance improvements (up to 27%) in PaLM-2L (Anil et al.,\n\n\n2\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\n2023) demonstrating the efficacy of STEP-BACK PROMPTING in tackling complex tasks, which are\n\n- therwise challenging due to the amount of details needed for reasoning. Figure 1 shows a summary\n\n- f all the key results presented in this paper. Some the tasks are very challenging: both PaLM-2L and\nGPT-4 achieve only _\u223c_ 40% accuracy on TimeQA and MuSiQue. Chain-of-Thought prompting leads\nto a minor improvement on a few tasks, while STEP-BACK PROMPTING improves the performance\n\n- f PaLM-2L across the board: 7% and 11% on MMLU Physics and Chemistry, 27% on TimeQA,\nand 7% on MuSiQue"," higher level of\nabstraction. For instance, instead of directly asking \u201c _which school Estella Leopold went to during a_\n_specific period_ \u201d, a step-back question (Figure 2 bottom) would ask about the \u201c _education history_ \u201d,\nwhich is a high-level concept encompasses the original question. Answering the step-back question\n\n- f \u201c _Estella Leopold\u2019s education history_ \u201d in this case will provide all the necessary information to\nreason about \u201c _which school Estella Leopold went to during a specific period_ \u201d. The premise is that\nthe step-back question is typically much easier. Grounding the reasoning on top of such abstractions\nhelps to avoid reasoning errors in the intermediate steps such as the example shown in Figure 2 (left)\nfrom Chain-of-Thought. In short, STEP-BACK PROMPTING consists two simple steps:\n\n\n- **Abstraction** : Instead of addressing the question directly, we first prompt the LLM to ask a generic\nstep-back question about a higher-level concept or principle, and retrieve relevant facts about the\nhigh-level concept or principle. The step-back question is unique for each task in order to retrieve\nthe most relevant facts.\n\n- **Reasoning** : Grounded on the facts regarding the high-level concept or principle, the LLM can\nreason about the solution to the original question. We term this as _Abstraction-grounded Reasoning_ .\n\n\nIn the following sections, we present an empirical study of STEP-BACK PROMPTING on a range of\nchallenging tasks covering STEM, Knowledge QA, and Multi-Hop Reasoning involving complex\nreasoning.\n\n\n3 EXPERIMENTAL SETUP\n\n\nHere we define the tasks and models we experiment with. We also describe our evaluation metric and\nthe baselines we consider.\n\n\n3.1 TASKS\n\n\nWe experiment with the following diverse tasks: (a) STEM, (b) Knowledge QA, and (c) Multi-Hop\nReasoning. We describe below the datasets we consider (see Appendix B for more details).\n\n\n3\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\n    - **STEM** : We evaluate MMLU and GSM8K for STEM tasks. MMLU (Hendrycks et al., 2020)\ncontains a series of benchmarks across diverse domains to evaluate the model\u2019s language\nunderstanding. We consider the high school physics"],"30":[" are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\n\n- pen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve comparable or better results on multiple QA datasets in the\n\n- pen-retrieval setting, compared to several, much\ncomplicated systems.\n\n\n**2** **Background**\n\n\nThe problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \u201c _Who first voiced Meg on Family_\n_Guy?_ \u201d or \u201c _Where was the 8th Dalai Lama born?_ \u201d, a\nsystem is required to answer it using a large corpus\n\n- f diversified topics. More specifically, we assume\n\n\n\nthe extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more passages in the corpus. Assume that our collection\ncontains _D_ documents, _d_ 1 _, d_ 2 _, \u00b7 \u00b7 \u00b7, dD_ . We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units [3] and get _M_\ntotal passages in our corpus _C_ = _{p_ 1 _, p_ 2 _, . . ., pM_ _}_,\nwhere each passage _pi_ can be viewed as a sequence\n\n- f tokens _w_ [(] _[i]_ [)]\n1 _[, w]_ 2 [(] _[i]_ [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _|_ [(] _p_ _[i]_ [)] _i|_ [. Given a question] _[ q]_ [,]\n\nthe task is to find a span _ws_ [(] _[i]_ [)] _[, w]_ _s_ [(] _[i]_ +1 [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _e_ [(] _[i]_ [)] from\n\n- ne of the passages _pi_ that can","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [","T:\n\n\n1. improves both retrieval and few-shot QA performance on several multi-step open-domain\nQA datasets, in both IID and OOD settings;\n2. reduces factual errors in generated CoTs; and\n3. improves performance with both large-scale\n(175B models) as well as smaller-scale models (Flan-T5-*, _\u2264_ 11B) without any training.\n\n\n**2** **Related Work**\n\n\n**Prompting for Open-Domain QA.** LLMs can\nlearn various tasks by simply using a few examples as prompts (Brown et al., 2020). They\u2019ve\nalso been shown to answer complex questions\nby producing step-by-step reasoning (chain-ofthoughts, or CoT) when prompted with a few or\nzero demonstrations (Wei et al., 2022; Kojima et al.,\n2022). Prompting has been applied to open-domain\nQA (Lazaridou et al., 2022; Sun et al., 2022; Yu\net al., 2023) but its value in improving retrieval and\nQA for multi-step open-domain questions remains\nrelatively underexplored.\n\n\n3We explain later (in the Metric section and Footnote 7)\nthe appropriateness of this metric in our setting as opposed to\nmore mainstream information recall metrics.\n\n\nRecently three approaches have been proposed\nfor multi-step open-domain QA. SelfAsk (Press\net al., 2022) prompts LLMs to decompose a question into subquestions and answers subquestions by\na call to Google Search API. DecomP (Khot et al.,\n2023) is a general framework that decomposes a\ntask and delegates sub-tasks to appropriate submodels. They also decompose questions but delegate retrieval to a BM25-based retriever. Both of\nthese approaches are not developed for CoT reasoning, do not focus on the retrieval problem, and require a single-hop QA model to answer the decomposed questions. Recently proposed ReAct (Yao\net al., 2022) system frames the problem as generating a sequence of reasoning and action steps. These\nsteps are much more complex, rely on much larger\nmodels (PaLM-540B), and require fine-tuning to\n\n- utperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be\neffective for smaller models without any training.\nWhile a direct comparison"," in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f","_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [2], indicating the needs\n\n- f improving retrieval.\n\n\n_\u2217_ Equal contribution\n1The code and trained models have been released at\n[https:\/\/github.com\/facebookresearch\/DPR.](https:\/\/github.com\/facebookresearch\/DPR)\n2For instance, the exact match score on SQuAD v1.1 drops\nfrom above 80% to less than 40% (Yang et al., 2019a).\n\n\n\nRetrieval in open-domain QA is usually implemented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords efficiently with an inverted index and can be seen\nas representing the question and context in highdimensional, sparse vectors (with weighting). Conversely, the _dense_, latent semantic encoding is _com-_\n_plementary_ to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n_\u201cWho is the bad guy in lord of the rings?\u201d_, which can\n\nbe answered from the context _\u201cSala Baker is best_\n\n_known for portraying the villain Sauron in the Lord_\n\n_of the Rings trilogy.\u201d_ A term-based system would\nhave difficulty retrieving such a context, while\na dense retrieval system would be able to better\nmatch _\u201cbad guy\u201d_ with _\u201cvillain\u201d_ and fetch the correct context. Dense encodings are also _learnable_\nby adjusting the embedding functions, which provides additional flexibility to have a task-specific\nrepresentation. With special in-memory data structures and indexing schemes, retrieval can be done\nefficiently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)).\n\nHowever, it is generally believed that learning a good dense vector representation needs a\nlarge number of labeled pairs of question and contexts. Dense retrieval methods have thus never\n\nbe shown to outperform TF-IDF\/BM25 for opendomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task ("],"31":[". Section 10.5 displays a few qualitative examples. We generate embeddings for randomly\nsampled documents from the TREC-Covid dataset and visualize their embeddings with PCA, where\nunique document inputs with different contextual embeddings are visualized in the same color. By\nchanging only the conditioning we reshape the embedding space and our model produces different\nembedding for the same text. Note that although the embeddings are clearly moving in response to\nchanging the contextual inputs, they still remain closer to each other than to different documents.\n\n\n19\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|||||Rand<br>TSP|om||\n||||||||\n||||||||\n||||||||\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Pre-training with TSP vs. random batching across cluster sizes.\n\n\n\nFigure 15: Each color indicates a single document input _d_ . Different points represent different values _\u03d5_ ( _d_ ; _D_ ) for different contexts.\n\n\n\nFigure 16: Performance of CDE model as the\nnumber of contextual examples increases.\n\n\n\nWe also consider how additional context is improving our model. Because the model includes an\n\n- ptional null token, we can supply any number of contextual inputs. We plot our model\u2019s performance\nacross context sizes in Figure 10.5. We see that our model is able to utilize partial context window\nsizes, and even perform reasonably with no context (i.e. all null token inputs) but offers the best\nperformance given a full context window size.\n\n\n10.6 CLUSTER TEXT EXAMPLES\n\n\nWe include random examples from a cluster gathered from our supervised dataset, shown in Table 4.\nThis particular cluster appears to be a combination of documents about county populations in the\nUntied States (in Kentucky, Iowa, Pennsylvania, etc.) and documents about criminal trials (mentioning\nhearings, depositions, and courts).\n\n\n10.7 TASK PREFIXES\n\n\nPrefixes are hand-written for each dataset in both meta-training sets. We follow the same prefix\nselection procedure as Nussbaum et al. (2024), inspired by Reimers et al. (2023):\n\n\n   - search ~~q~~ uery\n\n   - search ~~d~~   - cument\n\n   - classification\n\n\n20\n\n\nquery document\n\n\npopulation of breckenridge mi breckenridge, michigan. breckenridge is","2 is a second-stage encoder model, and _E_ is the token\nembedding matrix of _M_ 2 applied to each token in _d_ _[\u2032]_ . In practice, we parameterize both _M_ 1 and\n_M_ 2 using traditional bidirectional transformers, so our model is comprised of two biencoder-like\nbackbones called in sequence.\n\n\nThere is a similar contextualized model for the query encoder _\u03c8_ which is also given document context\n(as we do not have query context at test time):\n\n\n_\u03d5_ ( _q_ ; _D_ ) = _M_ 2( _M_ 1( _d_ [1] ) _, . . ., M_ 1( _d_ _[J]_ ) _, E_ ( _q_ 1) _, . . ., E_ ( _qT_ )) (6)\n\n\nWe note several implementation properties of this architecture. During training, computing contextual\nembeddings for each contextual document for each training instance would naively increase training\nby a computational factor proportional to _J_, the number of documents in context. This time increase\nwould not be tractable, since contrastive training can already take many days. We overcome this\ndifficulty by sharing context _d_ [1] _, ..., d_ _[J]_ within a batch of documents; this allows us to compute\nrepresentations just once per training step and reuse them between documents via computational\ngraph. [1]\n\n\nWhen indexing a new corpus _D_, first stage representations _M_ 1( _d_ [1] ) _...M_ 1( _d_ _[J]_ ) can be computed once\nand cached, so _M_ 1 does not add parameters or runtime to the search process. Query representations\ncan also use the cached context, which only require additional inputs to the encoder. (Our model does\nnot include contextualized queries, only documents, as we typically do not assume access to example\nqueries at test-time.)\n\n\n**Embedding** _**without**_ **context.** Individual corpora during training may not have sufficient or available\ncontext. To improve our model\u2019s generalization, we use _sequence dropout_, where we randomly replace\ncontext embeddings _M_ 1( _d_ _[\u2217]_ ) with some null token _v\u2205_","## CONTEXTUAL DOCUMENT EMBEDDINGS\n\n\n\n**John X. Morris**\nCornell University\njxm3@cornell.edu\n\n\n\n**Alexander M. Rush**\nCornell University\narush@cornell.edu\n\n\n\nABSTRACT\n\n\nDense document embeddings are central to neural retrieval. The dominant paradigm\nis to train and construct embeddings by running encoders directly on individual\ndocuments. In this work, we argue that these embeddings, while effective, are\nimplicitly out-of-context for targeted use cases of retrieval, and that a document\nembedding should take into account both the document and neighboring documents\nin context \u2013 analogous to contextualized word embeddings. We propose two complementary methods for contextualized document embeddings: first, an alternative\ncontrastive learning objective that explicitly incorporates document neighbors into\nthe intra-batch contextual loss; second, a new contextual architecture that explicitly\nencodes neighbor document information into the encoded representation. Results\nshow that both methods achieve better performance than biencoders in several\nsettings, with differences especially pronounced out-of-domain. We achieve state\n    - f-the-art results on the MTEB benchmark with no hard negative mining, score\ndistillation, dataset-specific instructions, intra-GPU example-sharing, or extremely\nlarge batch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n\n\n1 INTRODUCTION\n\n\nMachine learning approaches to text retrieval aim to learn an embedded representation for indexing\ndocuments. Classically, this area was dominated by statistical approaches using sparse lexical\nmatching methods based on n-gram frequencies such as BM25 (Robertson & Zaragoza, 2009).\nOnly recently have neural networks become competitive with state-of-the-art models on retrieval\ntasks (Karpukhin et al., 2020; Thakur et al., 2021). The primary neural method is a _dual encoder_\narchitecture that independently encodes both a document and query to a dense latent space for\nretrieval lookup. This document embedding space can improve upon a statistical model since it is\nlearned end-to-end for retrieval.\n\n\nHowever, there is at least one notable benefit of statistical approaches that is lost by neural models.\nStatistical models can easily incorporate prior corpus statistics such as inverse document frequency\n(IDF), into their representation. This prior term imparts context-dependence onto the model, since it\ncan be updated based on information specific to retrieval in a given domain at test time. We"," incorporate prior corpus statistics such as inverse document frequency\n(IDF), into their representation. This prior term imparts context-dependence onto the model, since it\ncan be updated based on information specific to retrieval in a given domain at test time. We contrast\nthis contextual formulation with neural document encoders that are by definition a function of the\ndocument itself. For example consider the following document:\n\n\nThe National Football League Draft is an annual event in which the National\nFootball League (NFL) teams select eligible college football players...\n\n\nDepending on the retrieval domain, e.g. Wikipedia search, sports articles, or televised events, IDF\nmay weight terms such as NFL, draft or annual higher; a neural document embedding model\nwould need to select a global weighting for this document.\n\n\nIn this work, we explore contextualization of document embeddings produced by dense encoders.\nThe goal is to produce embeddings that are better able to handle retrieval tasks in specific challenging\ncontexts. We propose two complementary changes to document encoders: a contextual training\nprocedure and architecture.\n\n\nFor contextual training, we aim to build a notion of neighboring documents directly into the contrastive\nlearning process. We propose a method that uses fast query-document clustering to produce a group\n\n\n1\n\n\nFigure 1: Overview of our system for contextual document embeddings (CDE). Our model operates\nin two stages: a first stage used to characterize the dataset from samples, and a second stage used to\nembed the final document.\n\n\n- f neighbors for each training batch. Each update for training is constructed purely from neighboring\ndocuments to ensure that embeddings can distinguish documents even in the most challenging\n\ncontexts.\n\n\nFor the architecture, we propose a new encoder that injects information about the contextual documents during embedding. The proposed architecture augments the standard BERT-style encoder with\nadditional conditioning that provides aggregated document-level information about neighboring documents. We call our method Contextual Document Embedding (CDE). Analogously to pre-computed\ncorpus-level statistics, this method provides a manner for the embedding to take into account the\nrelative frequency of terms in context. The final output is still an embedding of the same size, so this\ndoes not require any additional storage or other changes to the retrieval process. When indexing, we\nutilize information from the corpus to produce document and query embeddings that are specific to a\nparticular domain.\n\n\nExperiments compare these two extensions to standard approaches for training document embeddings.\nOur results show that contextual contrastive training improves","52.3 83.5 30.9 63.71\ngte-base-en-v1.5 77.2 46.8 85.3 57.7 54.1 82.0 31.2 64.11\n\n\ncde-small-v1\n\n[Random] 81.3 46.6 84.1 55.3 51.1 81.4 31.6 63.81\n\n[Contextual] 81.7 48.3 84.7 56.7 53.3 81.6 31.2 **65.00**\n\n\nTable 2: Performance of models with 250M or fewer parameters on the MTEB benchmark for\ntext embedding models. \u201cRandom\u201d indicates the performance of our model with random training\ndocuments included instead of per-domain contextual documents.\n\n\nmore difficult batches in contrastive learning form a better overall gradient approximation and learn\nmore effectively.\n\n\nSection 6 showcases model performance across batch and cluster sizes after both phases of training.\nWe observe that although a large batch and cluster size are useful when filtering is not enacted, when\nincluding filtering, smaller cluster (and harder) are clearly better, and large batches do not add much.\nWhen comparing filtered to non-filtered models (Figure 4), filtering false negatives clearly improves\nperformance.\n\n\n**Contextual architecture** In addition to adversarial batching, we compare our contextual architecture to a biencoder across the datasets of BEIR in Table 1 (full results in appendix). Our architecture\ngenerally matches or improves performance on all downstream datasets, with largest improvements\nin ArguAna and SciFact, two of the smaller and more out-of-domain datasets.\n\n\n**Full-scale training** Figure 5 shows our models\u2019 performance when trained for multiple epochs on\nthe supervised datasets, relative to the best similar-sized embedding model (dashed line). We find\nbest performance when training for four epochs on the BGE meta-datasets. Although our best model\ndoes use a single hard negative per query, we are still able to to achieve state-of-the-art performance\nwithout using _any_ hard negatives.\n\n\nFor our final model (cde-small-v1), we select the best of the supervised models, which comes\nfrom finetuning on the BGE dataset. On MTEB, cde-small-v1 obtains state-of-the-art results\ncompared to models"],"32":[" expansion method to a more powerful teacher LLM, textdavinci-003, to distill 52K high-quality data. To improve\nthe diversity and coverage during expansion, Wu et al.\n(2023c) and (Sun et al., 2024b) prompt the teacher LLM to\ngenerate instructions corresponding to some specific topics.\nXu et al. (2023a) propose an Evol-Instruct method to expand the instructions from two dimensions: difficulty (e.g.\nrewriting the question to be more complex) and diversity\n(e.g. generating more long-tailed instructions). This EvolInstruct method is domain-agnostic and has been used to\nexpand the distillation of coding (Luo et al., 2023a) and\nmath (Luo et al., 2023b). Additionally, expansion methods\ncan significantly augment NLP task datasets with similar\nsamples, thereby enhancing task performance. For instance,\nAugGPT (Dai et al., 2023a) leverages a teacher LLM to\nrephrase each sentence in the training samples into multiple conceptually similar, but semantically varied, samples\nto improve classification performance. Similarly, TDG (He\n\n\n\net al., 2023b) proposes the Targeted Data Generation (TDG)\nframework, which automatically identifies challenging subgroups within data and generates new samples for these\nsubgroups using LLMs through in-context learning.\nIn summary, the expansion method leverages the incontext learning strengths of LLMs to produce more varied and extensive datasets with both inputs and outputs.\nHowever, the quality and diversity of the generated data\nare heavily reliant on the teacher LLMs and the initial seed\ndemonstrations. This dependence can lead to a dataset with\ninherent bias from LLMs (Yu et al., 2023a; Wei et al., 2023)\nand a homogeneity issue where the generations may be\nprone to similarity ultimately, limiting the diversity this\nmethod seeks to achieve (Ding et al., 2023b). Moreover, the\nexpansion process may inadvertently amplify any biases\npresent in the seed data.\n\n\n_3.1.3_ _Data Curation_\n\nThe pursuit of high-quality and scalable data generation in\nknowledge distillation from LLMs has led to the emergence\n\n- f the Data Curation approach. This method arises in response to the limitations observed","\n\n\n\nInput Set\n\n\n\n\n\n\n\n\n\n\n|et|Col2|Col3|\n|---|---|---|\n|t|||\n|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|xtract|||\n|xtract|||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 5: An illustration of different knowledge elicitation methods from teacher LLMs. _Labeling_ : The teacher generates\nthe output from the input; _Expansion_ : The teacher generates samples similar to the given demonstrations through incontext learning; _Data Curation_ : The teacher synthesizes data according to meta-information, such as a topic or an entity;\n_Feature_ : Feed the data into the teacher and extract its internal knowledge, such as logits and features; _Feedback_ : The teacher\nprovides feedback on the student\u2019s generations, such as preferences, corrections, expansions of challenging samples, etc;\n_Self-Knowledge_ : The student first generates outputs, which is then filtered for high quality or evaluated by the student itself.\n\n\n\nIn this formulation, _x_ and _y_ represent the new input\n- utput pairs generated by the teacher LLM. The input _x_\nis generated based on a set of input-output demonstrations\n_c_ . The output _y_ is then generated in response to the new\ninput _x_ under the guidance of an instruction _I_ . Note that\nthe demonstrations could be predefined or dynamically\nupdated by adding the newly generated samples.\nExpansion techniques have been widely utilized to\nextract extensive instruction-following knowledge from\nteacher LLMs. Wang et al. (2022a) first introduces an iterative bootstrapping method, Self-Instruct, to utilize LLMs\nto generate a wide array of instructions based on several demonstrations sampled from 175 manually-written instructions. The newly generated instructions are then added\nback to the initial pool, benefiting subsequent expansion\niterations. Subsequently, Taori et al. (2023) applies this expansion method to a more powerful teacher LLM, textdavinci-003, to distill 52K high-quality data. To improve\nthe diversity and coverage during expansion, Wu et al.\n(2023c) and (Sun et",", 2023) proposes to distill the\nknowledge from both the GPT2 and LLaMA into the smallsize student models. Ensemble-Instruct (Lee et al., 2023b)\ntries to generate both instructions and responses ensembled\nfrom several different LLMs with RougeL as the indicator.\nFUSELLM (Wan et al., 2024a) externalizes the collective\nknowledge and unique strengths by leveraging the generative distributions of different LLMs aiming to train a student\nmodel beyond those of any individual source LLM. Despite\nthe recent progress in this topic, it still remains an underexplored topic.\n\n\n_**Explore Richer Knowledge from Teacher LLMs**_ As indicated\nin Table 3, the majority of teacher LLMs are closed-source\ndue to their advanced capabilities. Consequently, current\nmethodologies primarily focus on using the generations\nfrom these models as hard labels, training student models\nthrough simple supervised fine-tuning. However, beyond\n\n\nthe straightforward imitation of output behaviors via hard\nlabels, there is a growing interest in harnessing richer\nknowledge from teacher LLMs, including feedback and\nfeature knowledge, as well as exploring diverse combinations of knowledge elicitation methods. As highlighted in\nthe _Feedback_ section, teachers can provide various types of\nfeedback based on the student\u2019s outputs (Lee et al., 2023a;\nJiang et al., 2023b; Chen et al., 2023a). Similarly, the _Feature_\nsection discusses how knowledge based on features, such\nas logits serving as soft labels, can offer deeper, intrinsic\ninsights into the teacher model (Gu et al., 2024; Agarwal\net al., 2024). These explorations have demonstrated promising outcomes, suggesting that access to a broader spectrum\n\n- f knowledge can significantly enhance student model performance beyond what is achievable through simple SFT\ndistillation alone. This highlights the critical need for further\nresearch into varied knowledge extraction methods from\nteacher LLMs to augment the effectiveness of KD processes.\n\n\n_**Overcoming Catastrophic Forgetting During Distillation**_\nPrevious research has delved into the fine-tuning of LLMs\nto acquire the ability to follow instructions or transfer\nknowledge for forthcoming tasks, skills, or domains, leveraging advancements in LLM technology. Nevertheless, investigations have revealed that the continual fine-tuning of\nLLMs on"," . This method involves\ngathering 3.5K common topics from Wikipedia to serve as\nguidance during the generation process.\n\n\n_**Complex Instructions.**_ Some works promote students to\nsolve more complex instructions (Xu et al., 2023a; Luo et al.,\n2023b,a; Guo et al., 2023c). According to Xu et al. (2023a), instruction datasets derived from human-written seeds often\nexhibit low to moderate complexity. To enhance the complex instruction-following capabilities of smaller models,\nWizardLM (Xu et al., 2023a) introduces _Evol-Instruct_ . This\nmethod gradually transforms instructions into more complex forms through a multi-step evolution process, focusing\n\n- n both increasing difficulty levels and expanding the diversity of topics. They conducted four rounds of evolution\nusing the OpenAI ChatGPT API, resulting in a dataset of\n250k complex instructions. Subsequently, they trained the\nLLaMA 7B model, referred to as WizardLM, on this dataset.\nIn the high-difficulty section of test instructions, WizardLM\neven outperformed ChatGPT, achieving a win rate 7.9%\nhigher than ChatGPT. Zhao et al. (2023e) further conduct\npreliminary studies revealing the effectiveness of increasing\ninstruction complexity. Instruction Fusion (Guo et al., 2023c)\nfurther uses teacher LLMs to increase the complexity by\nfusing two distinct evolved instructions. Furthermore, this\nconcept of \u201cevolving\u201d instructions has been extended to\ndistill specific skills such as coding (Luo et al., 2023a) and\nmathematics (Luo et al., 2023b).\n\n\n_**Human Instructions.**_ In contrast to works that rely on generating instructions from ChatGPT, which may lack diversity\nand have gaps with real human instructions, Vicuna (Chiang\net al., 2023) and Koala (Geng et al., 2023) showcase impressive performance by using human conversations and natural instructions from community-contributed conversations.\nThese conversations, found in platforms like ShareGPT, provide a forum for users to share their interactions with ChatGPT. It\u2019s important to note, however, that models trained\n\n- n such natural conversations might mimic the style but\nmay not fully capture the reasoning process of the original\nteacher (G"," and Llama-2 models with 7B and 13B\nparameters across various benchmark evaluations.\n\n\n_**Improved Instructions.**_ Another line of work focuses on\nimproving the quality of existing instruction data, including\nboth the improvement of instruction and corresponding\nresponse. SelFee (Ye et al., 2023) utilizes the ChatGPT to iteratively improve the quality of responses. ExpertLLaMA (Xu\net al., 2023f) improves the quality of responses by augmenting vanilla instructions with specialized Expert Identity\ndescriptions. Reflection-Tuning (Li et al., 2023e) improves\nboth the instruction and response sequentially by reflecting\n\n- n specific criteria. DEITA (Liu et al., 2023h) proposes to\nenhance and score instructions in three directions including complexity, quality, and diversity to get high-quality\ndistillation data. MUFFIN (Lou et al., 2023) proposes to\nscale the instruction according to the input by diversifying\nthese tasks with various input facets. Selective ReflectionTuning (Li et al., 2024d) first involves the student model\nin the data improvement pipeline with a novel studentselection module, in which the student model is able to\ndecide the data learn from.\n\n\nIn summary, distilling instruction data from teachers\npresents a promising avenue for training cheap and reproducible instruction-following language models. Current small models have made strides in enhancing various aspects of instruction-following ability, like diversity, complexity and explanation. However, student models trained on instruction data expanded by ChatGPT often mimic ChatGPT\u2019s style without replicating its factual\naccuracy (Gudibande et al., 2023). Achieving a more capable instruction-following capability requires a stronger\nteacher LLM (Gudibande et al., 2023) and access to diverse, high-quality instruction data, such as the one used\nin Orca (Mukherjee et al., 2023; Mitra et al., 2023), which\nincorporates extensive task instructions from the Flan 2022\nCollection (Longpre et al., 2023).\n\n\n_4.1.2_ _Multi-turn Dialogue_\n\nWhile instruction following focuses on single-instance command execution, multi-turn dialogue extends this to comprehend and maintain context through ongoing interactions.\nThis skill is vital for models to engage meaningfully in\nhuman"],"33":["<br>89<br>9<br>**8**<br>**7**<br>**57.14**|\n\n\nTable 7: The result of counterfactual robustness. ACC is the\naccuracy (%) of LLMs without external documents. ACCdoc\nis the accuracy (%) of LLMs with counterfactual documents.\nED and ED _[\u2217]_ are error detection rates evaluated by exact\nmatching and ChatGPT, respectively. CR is the error correction rate.\n\n\nthought approach to break down complex problems (Zhou\net al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). However, these methods slow down the inference speed and cannot provide timely responses.\n\n\n**Results on Counterfactual Robustness testbed**\n\nIn order to ensure that LLMs possess relevant knowledge,\nwe assess their performance by directly asking them questions. However, we found that most LLMs struggle to answer them correctly. To ensure a more reasonable evaluation, we only consider LLMs that have an accuracy rate of\n\n- ver 70% as this threshold is relatively high and encompasses more LLMs. The results are shown in Table 7. We\npresent the following metrics: accuracy without any documents, accuracy with counterfactual documents, error detection rates, and error correction rates. We can see that It\nis hard for LLMs to identify and correct factual errors in the\ndocuments. This suggests that the model can be easily misled by documents containing incorrect facts.\nIt is important to note that retrieval-augmented generation\nis not designed to automatically address factual errors within\na given context, as this contradicts the underlying assumption that the model lacks knowledge and relies on retrieved\ndocuments for additional information. However, this issue is\ncrucial in practical applications due to the abundance of fake\nnews on the internet. Existing LLMs do not have a safeguard\nto handle inaccurate responses caused by misinformation. In\nfact, they heavily depend on the information they retrieve.\nEven when LLMs contain the internal knowledge about the\nquestions, they often trust false information that is retrieved.\nThis presents significant a challenge for the future development of RAG in LLMs.\n\n\n**Conclusion**\n\nIn this paper, we evaluated four abilities of retrievalaugmented generation in LLMs: noise robustness, negative rejection, information integration, and counterfactual\nrobustness"," in improving\nfactuality and citation accuracy for long-form generations relative to these models. [1]\n\n\n1 INTRODUCTION\n\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces **Self-Reflective Retrieval-augmented Generation (SELF-RAG)** to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., _reflection tokens_ ). Reflection tokens are categorized into _retrieval_ and _critique_ tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a **retrieval** token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and then **generating** corresponding task outputs (Step\n2). It then generates critique tokens to **criticize** its own output and choose best one (Step 3) in terms\n\n- f factuality and overall quality. This process differs from conventional RAG (","ufficient information\u201d or other rejection signals.\n\n - **Information Integration**, which evaluates whether\nLLMs can answer complex questions that require integrating information from multiple documents. For the instance in Figure 1, for the question \u201cWhen were the ChatGPT app for iOS and ChatGPT api launched?\u201d, LLMs\nare expected to provide information of the launch dates\nfor both the ChatGPT iOS app and ChatGPT API. The\ntestbed for information integration contains instances\nthat can only be answered using multiple documents.\n\n - **Counterfactual Robustness**, which evaluates whether\nLLMs can identify risks of known factual errors in the\nretrieved documents when the LLMs are given warnings\nabout potential risks in the retrieved information through\ninstruction. The testbed for counterfactual robustness includes instances that can be answered directly by the\nLLMs, but the external documents contain factual errors.\n\n\n\nBased on RGB, we conduct evaluation on 6 state-ofthe-art large language models including ChatGPT (OpenAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM26B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\nQwen-7B-Chat (QwenLM 2023), BELLE-7B (Yunjie Ji\n2023). We found that even though RAG can improve the response accuracy of LLMs, they still suffer from the abovementioned challenges significantly. Specifically, we found\nthat even though LLMs demonstrate some level of noise robustness, they tend to confuse similar information and frequently generate inaccurate answers when relevant information exists. For example, when faced with a question about\nthe 2022 Nobel Prize in Literature, if there are noisy documents about the 2021 Nobel Prize in Literature in external\ndocuments, LLMs may become confused and provide inaccurate answers. Besides, LLMs frequently fail to reject answering and generate incorrect answers when none of the\nexternal documents contain relevant information. Furthermore, LLMs lack the ability to summarize from multiple\ndocuments, and therefore if multiple documents are needed\nto answer a question, LLMs often fail to provide accurate\nanswer. Finally, we found that even when the LLMs contain\nthe required knowledge and are given warnings about potential risks in the retrieved information through instruction,\nthey still tend to trust and prioritize"," one negative document are shown. The responses\nare generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,\nwhile the red text highlights the non-matching parts.\n\n\n\nstructions to inform the model.). If the model generates this\ncontent, it indicates that the model has detected erroneous\ninformation in the document.\n\n**Error correction rate** measures whether the model can\nprovide the correct answer after identifying errors for counterfactual robustness. The model is asked to generate the correct answer after identifying the factual errors. If the model\ngenerates the correct answer, it indicates that the model is\ncapable of correcting errors in the document.\nConsidering that the model may not fully adhere to instructions, for rejection rate and error detection rate, we\nalso use ChatGPT to conduct additional evaluation of the\nanswers. Specifically, we assess the model\u2019s responses by\nusing instructions and demonstrations to determine if they\ncan reflect information that is not present in the document or\nidentify any factual errors.\n\n\n**Experiments**\n\n\nIn this section, we evaluate the performance of various\nLLMs, analyze and discuss the results, summarizing the\nmain challenges that existing LLMs encounter when using\nexternal knowledge.\n\n\n**Settings**\n\n\n**Task formats.** Due to contextual limitations, we provide 5\nexternal documents for each question. In our experiments\n\n- n noise robustness, we evaluate scenarios with noise ratios ranging from 0 to 0.8. To comprehensively evaluate the\n\n- verall capabilities, we have adopted a unified instruction\nfor each language, as shown in Figure 3. The experiments\nwere conducted using an NVIDIA GeForce RTX 3090.\n\n\n\n**Models** We conduct evaluation on 6 state-of-the-art\nlarge language models which can generate both English and Chinese including ChatGPT (OpenAI 2022) [3],\nChatGLM-6B (THUDM 2023a), ChatGLM2-6B (THUDM\n2023b), Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7BChat (QwenLM 2023), BELLE-7B-2M (Yunjie Ji 2023).\n\n\n**Results on Noise Robustness**\n\nWe evaluated the accuracy based on the different noise ratios\nin external documents, and the results are shown in Table "," et al., 2023b, 2024b), etc. For detailed\nprogress on trustworthy LLMs, please further refer to Yao\net al. (2023a); Liu et al. (2023i); Sun et al. (2024a).\nThough slightly under-explored, aligning LLMs with\nhuman values by distilling is still possible (Bai et al., 2022a;\n\n\nCui et al., 2023a; Yang et al., 2024; Sun et al., 2024b). For\ninstance, Bai et al. (2022a) propose RLAIF, utilizing AIgenerated labels to interactively improve both helpfulness\nand harmlessness. Sun et al. (2024b) prompt the student\nmodel with 16 principles as guidelines for generating helpful, ethical, and reliable responses. Similarly, both harmless\nand harmful generations could be elicited by modifying\nthe prompts, and then are used to train the preference\nmodel (Yang et al., 2024). Cui et al. (2023a) utilize GPT4 to rank generations regarding helpfulness, truthfulness,\nand honesty. Liu et al. (2023b) advance the alignment of\nLLMs with societal values by incorporating simulated social\ninteractions into the training process. This approach encompasses a range of elements, including demonstrations that\nare both in alignment and in conflict with social norms, as\nwell as collective ratings, in-depth feedback, and responses\nthat are revised iteratively.\n\n\n**4.3** **Agent**\n\n\n_4.3.1_ _Tool Using_\n\nWhile recent LLMs have shown proficiency in solving various tasks, they still tend to make mistakes when handling\nlarge numerical values or executing intricate mathematical\ncalculations (Qian et al., 2022; She et al., 2023; Manikandan\net al., 2023; Liang et al., 2023b; Mialon et al., 2023). Thus\nequipping LLM agents with the capability to utilize tools\nhas been increasingly focused on. Commonly used methods\nmainly relied on human-curated data for training (Parisi\net al., 2022; Nakano et al., 2022; Qin et al., 2023c; Song\net al., 2023b) or prompt designing(Cai et"],"34":["ING on challenging Multi-Hop reasoning benchmark MuSiQue\n(Trivedi et al., 2022) and StrategyQA (Geva et al., 2021). We follow the same protocol as Section 5\nto implement STEP-BACK PROMPTING.\n\n\nTable 3 shows performance of various baselines on the dev set of MuSiQue and StrategyQA.\nBaseline performance of PaLM-2L and GPT-4 are low (35 _._ 5% and 38 _._ 5% for PaLM-2L and GPT-4\nrespectively) in MuSiQue since it is a hard multihop reasoning benchmark. In contrast, StrategyQA\nhas stronger baselines (82 _._ 8% and 78 _._ 3% for PaLM-2L and GPT-4 respectively) probably because\nit is a binary classification task. CoT and TDB improve model performance a bit in the case of\n\n_\u223c_\nMuSiQue ( 3% and 3.5% respectively) which can be attributed to the inherent reasoning nature\n\n- f this task where these methods are shown to be helpful. In the case of StrategyQA, there is no\nsignificant performance gain with CoT and TDB which could be due to the high baseline performance\nin this task, with limited scope for these prompting methods to improve performance. Often, 1-shot\nperformance is significantly lower than their zero-shot methods, which could be attributed to potential\nexample bias (Zhao et al., 2021; Parmar et al., 2023). RAG improves model performance ( _\u223c_ 4%\nand 2% for MuSiQue and StrategyQA respectively.) STEP-BACK PROMPTING with the power\n\n- f abstraction produces the best performance of all methods: 42 _._ 8% in MuSiQue and 86 _._ 4% in\nStrategyQA, significantly outperforming GPT-4 on both tasks. We present a detailed error analysis\n\n- n StrategyQA in Appendix A.3.\n\n\n7 DISCUSSION\n\n\nAbstraction helps humans to solve complex tasks by removing irrelevant details and distilling highlevel concepts and principles to guide the problem-solving process. STEP-BACK PROMPTING breaks\n\n\n8\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ncomplex tasks such as knowledge-intensive QA, multi-hop reasoning,"," points.\n\n\n\nWe speculate DecomP performs well on 2WikiMultihopQA because it has only a few easy-to-predict\ndecomposition patterns, which DecomP\u2019s question\ndecomposition can leverage. The lack of such patterns in HotpotQA and MuSiQue causes it to underperform compared to IRCoT. Lastly, it will be\nuseful to assess whether DSP, which is hardcoded\nfor 2-hop questions like that of HotpotQA, will\nwork well for a dataset with a varied number of\n\nhops like that of MuSiQue. We leave this further\ninvestigation to future work.\n\n\n**D** **Additional CoT Generation Examples**\n\n\nTable 5 provides illustrations, in addition to the\n\n- nes provided in Table 2, for how the CoT generations for NoR QA, OneR QA, and IRCoT QA\nmethods vary. This gives an insight into how IRCoT improves QA performance. Since NoR relies completely on parametric knowledge, it often\nmakes a factual error in the first sentence, which derails the full reasoning chain. Some of this factual\ninformation can be fixed by OneR, especially information closest to the question (i.e., can be retrieved\nusing the question). This is insufficient for fixing\n\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nWhat was the\n\nisland, - n\nwhich Marinelli\n\nGlacier is located, formerly\nknown as?\n\n\nWho is the\n\nchild - f the\ndirector of film\nMukhyamantri\n(1996 Film)?\n\n\nWhen did the\nGovernorship\nend - f the\ncity where the\nauthor of Sac\nerdotii Nostri\n\nPrimordia died?\n\n\n\n_\u21d2_ Marinelli Glacier is\n\nlocated on the island of\nTierra del Fuego.\n_\u21d2_ The island of Tierra\ndel Fuego was formerly\nknown as Isla de Xativa.\n\n\n_\u21d2_ The director - f\nMukhyamantri (1996\nfilm) is Anjan Choudhury.\n_\u21d2_ Anjan Choudhury\nhas a child named\nSandip Choudhury.\n\n\n_\u21d2_ Sacerdotii Nostri Primordia was written by\nPope John XXIII.\n_\u21d2_ Pope"," we use an LLM with ground truth answers to classify\nresponses as Correct ( **%C** ), Abstention ( **%A** ), or Hallucination ( **%H** ).\n\n\n**Fine-tuning Results and Discussion.** Table 3 shows our experimental results. We verify that the FT\nvariants have a higher rate of generating correct answers ( **%C** ) compared to closed-book and Vanilla\nRAG for Musique but not for HotPotQA. On the other hand, refuting our hypothesis, Data Mix 2 and\n3 do not lead to more abstentions than Vanilla RAG. But, they do abstain more than with Data Mix 1,\nshowing the impact of adding \u201cI don\u2019t know\u201d in the training set. In general, FT models using RAG\n\n- utput incorrect answers ( **%H** ) much of the time, and often more than they abstain ( **%A** ).\n\n\nB.2 PERFORMANCE BREAKDOWN BY SUFFICIENT CONTEXT\n\n\nWe explore RAG performance by different models for various RAG benchmark datasets. Here,\nthe first column shows performance without RAG (closed-book) while the second column shows\nperformance with RAG (open-book). To better understand RAG performance, we use our sufficient\ncontext autorater to stratify the retrieval augmented generation (RAG) datasets into sufficient and\ninsufficient context. The third and fourth columns show the performance of the second column\nstratified by sufficient vs insufficient context respectively.\n\n\n17\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Correct, hallucination, and abstention fractions across models for dataset FreshQA, stratified\nby sufficient context. FreshQA includes hand-curated source URLs, which explains the larger\npercentage of sufficient context (77.4%). FreshQA also specifically explores questions with answers\nthat change based on the question\u2019s timestamp, which may explain the frequent abstentions without\nRAG (100% for Gemini 1.5 Pro).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Correct, hallucination, and abstention fractions across models for dataset HotpotQA,\nstratified by sufficient context. HotpotQA includes questions that are more likely to be answerable\nwithout context (e.g., yes or no questions, multiple choice questions, or questions with answers that\nmight be answerable due to pre-training). This explains the higher fraction of correct answers without","% for Gemma 27B on HotpotQA\nin the highest accuracy regions, and gains of over 5% for Gemini 1.5 Pro on the same dataset near the\n70% coverage region. These gains are less pronounced on datasets with lower overall accuracy, such\nas when using Gemma 27B on Musique. In this scenario, the low overall performance (18.4%) likely\nmeans that most of the predictive gains are seen by using the self-rated confidence to predict errors\nfor the majority of samples. As a result, there is no added benefit from the sufficient context signal.\n\n\n**Discussion.** As expected, we see a downward trend in which higher coverage leads to lower selective\naccuracy for both methods. We conclude that the selective generation mechanism with sufficient\ncontext has an added benefit for accuracy-coverage trade-offs compared to self-rated confidence\nalone. As a prerequisite for our method, models should have a non-trivial accuracy on sufficient and\ninsufficient context instances. Then, intuitively, we can prioritize answering questions the model is\nlikely to get right before those the model struggles with. While ordering examples is impossible in\nreal settings, we can estimate a coverage level and use a threshold to choose when to answer.\n\n\n5.2 FINE-TUNING\n\n\nWe also consider fine-tuning models to increase their ability to abstain instead of outputting an\nincorrect answer. To do so, we train the models with some examples that contain \u201cI don\u2019t know\u201d\ninstead of their original ground truth answer. The intuition here is that training explicitly on such\n\n\n9\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n|Col1|Col2|P(Correct<br>P(Correct|)<br>) + Suff . C|onte|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n\n\n|Col1|Col2|P(Correct<br>P(Correct|)<br>) + Suff .|Conte|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n\n\n|Col1|Col2|Col3|P(True<br>P(True|)<br>) + Suff .|Contex|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Col2|P(Correct<br>P(Correct","Claude 3.5 Sonnet 9.6 53.8 36.5\n\nGemini 1.5 Flash 7.7 73.1 19.2\n\nGemma 27B 9.6 55.8 34.6\n\n\n19\n\n\nPublished as a conference paper at ICLR 2025\n\n\nB.3 COMPARISON OF QA EVALUATION METRICS\n\n\nWe compare two the LLM-based QA Evaluator (LLMEval) used in the paper with a deterministic\nlexical matching metric (Contains Answer). The Contains Answer metric labels responses based on\nwhether they contain the exact ground truth answer, while LLMEval uses an LLM to assess semantic\n\ncorrectness.\n\n\nTable 5 presents model performance across three datasets (FreshQA, Musique, HotpotQA), split by\n\n- ur sufficient context autorater. The results show Contains Answer is generally stricter than LLMEval,\nthough both metrics reveal similar patterns in model behavior.\n\n\nTable 5: **Comparison of evaluation metrics across models and datasets.** We show results for\nchecking whether the response contains one of the ground truth answer strings (\"Contains\"), where\nwe report the % of responses that contain an answer. We compare this to our LLMEval method that\nuses an LLM to evaluate if the response is correct, abstain, or hallucinated, and we report % correct.\n\n\nFreshQA Musique HotpotQA\n\n\nModel Context Contains LLMEval Contains LLMEval Contains LLMEval\n\n\nGemini 1.5 Pro Suff 80.3% 89.1% 60.1% 83.4% 47.6% 67.5%\n\nInsuff 31.4% 41.2% 33.6% 49.5% 34.2% 49.4%\n\nGPT-4 Suff 84.3% 89.1% 64.6% 83.4% 52.4% 71.9%\n\nInsuff 36.3% 44.1% 44.4% 61.4% 46.1% 59.5%\n\nGemma 27B Suff 26.9% 27.5% 10.8% 23.3% 40.7% 64.1%\n\nInsuff 11.8% 6.9% 7.2%"],"35":["## - TAKE A STEP BACK: EVOKING REASONING VIA AB\n\n### STRACTION IN LARGE LANGUAGE MODELS\n\n**Huaixiu Steven Zheng** _[\u2217]_ **Swaroop Mishra** _[\u2217]_ **Xinyun Chen** **Heng-Tze Cheng**\n**Ed H. Chi** **Quoc V Le** **Denny Zhou**\n\n\nGoogle DeepMind\n\n\nABSTRACT\n\n\nWe present STEP-BACK PROMPTING, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning\npath towards the solution. We conduct experiments of STEP-BACK PROMPTING\nwith PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM,\nKnowledge QA, and Multi-Hop Reasoning. For instance, STEP-BACK PROMPTING improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%\nand 11% respectively, TimeQA by 27%, and MuSiQue by 7%.\n\n\n_The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be_\n_absolutely precise. \u2014 Edsger W. Dijkstra_\n\n\n1 INTRODUCTION\n\n\nThe field of natural language processing (NLP) is witnessing a ground-breaking revolution because\n\n- f the Transformer-based (Vaswani et al., 2017) large language models (LLMs) (Devlin et al., 2018;\nRaffel et al., 2020; Brown et al., 2020; Anil et al., 2023). Scaling up the model size and pre-training\ncorpus (Hoffmann et al., 2022; Chowdhery et al., 2022) has brought remarkable improvement in model\ncapabilities and sample efficiency with insights from the scaling law (Kaplan et al., 2020; Hoffmann\net al., 2022), as well as emergent abilities (Wei et al., 2022a) such as multi-step reasoning (Wei et al.,\n2022b; Zhou et al., 2022) and instruction following (Mishra et al., 202",", and Wen-tau Yih. Task-aware retrieval with instructions. In _Findings of the Associ-_\n_ation for Computational Linguistics_ [, 2023b. URL https:\/\/aclanthology.org\/2023.](https:\/\/aclanthology.org\/2023.findings-acl.225)\n[findings-acl.225.](https:\/\/aclanthology.org\/2023.findings-acl.225)\n\n\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob\nEisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering:\nEvaluation and modeling for attributed large language models. _arXiv preprint arXiv:2212.08037_,\n[2022. URL https:\/\/arxiv.org\/abs\/2212.08037.](https:\/\/arxiv.org\/abs\/2212.08037)\n\n\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt\u2019s behavior changing over time? _arXiv_\n_preprint arXiv:2307.09009_ [, 2023. URL https:\/\/arxiv.org\/abs\/2307.09009.](https:\/\/arxiv.org\/abs\/2307.09009)\n\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\n_arXiv preprint arXiv:1803.05457_ [, 2018. URL https:\/\/arxiv.org\/abs\/1803.05457.](https:\/\/arxiv.org\/abs\/1803.05457)\n\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-\u00b4\nefficient exact attention with io-awareness. In _Advances in Neural Information Processing Systems_,\n[2022. URL https:\/\/openreview.net\/forum?id=H4DqfPSibmx.](https:\/\/openreview.net\/forum?id=H4DqfPSibmx)\n\n\nShehzaad Dhuliawala, Mojtaba Komeili, Jing","b; Kojima et al., 2022) ran into errors during intermediate reasoning\nsteps (highlighted as red). _Right_ : PaLM-2L (Anil et al., 2023) successfully answers the question via\nSTEP-BACK PROMPTING.\n\n\nAmong many of the cognitive skills, abstraction (Lachmy et al., 2022) is ubiquitous to humans\u2019\nability to process vast amounts of information and derive general principles. For example, Kepler\ncompressed thousands of measurements into Kepler\u2019s three laws of planetary motion, which precisely\ndescribe the orbits of planets around the Sun (Russell, 1964). In critical decision-making, humans\nfind abstraction to be helpful since it provides a broader view of the environment. This work explores\nhow LLMs can tackle complex tasks involving many low-level details through a two-step process\n\n- f abstraction-and-reasoning. The first step is to show LLMs how to step back through in-context\nlearning \u2013 prompting them to derive high-level abstractions such as concepts and principles for\na specific example. The second step is to leverage the reasoning ability to reason on top of the\nhigh-level concepts and principles. We use few-shot exemplar demonstrations to execute STEP-BACK\nPROMPTING on LLMs.\n\n\nWe experiment across a range of tasks involving domain specific reasoning such as Physics and Chemistry, knowledge-intensive question answering requiring factual knowledge, multi-hop commonsense\nreasoning. We observe significant performance improvements (up to 27%) in PaLM-2L (Anil et al.,\n\n\n2\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\n2023) demonstrating the efficacy of STEP-BACK PROMPTING in tackling complex tasks, which are\n\n- therwise challenging due to the amount of details needed for reasoning. Figure 1 shows a summary\n\n- f all the key results presented in this paper. Some the tasks are very challenging: both PaLM-2L and\nGPT-4 achieve only _\u223c_ 40% accuracy on TimeQA and MuSiQue. Chain-of-Thought prompting leads\nto a minor improvement on a few tasks, while STEP-BACK PROMPTING improves the performance\n\n- f PaLM-2L across the board: 7% and 11% on MMLU Physics and Chemistry, 27% on TimeQA,\nand 7% on MuSiQue","aud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\nImproving language models by retrieving from trillions of tokens. In _International conference on_\n_machine learning_ [, pp. 2206\u20132240. PMLR, 2022. URL https:\/\/arxiv.org\/abs\/2112.](https:\/\/arxiv.org\/abs\/2112.04426)\n[04426.](https:\/\/arxiv.org\/abs\/2112.04426)\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\nZiegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neu-_\n_ral Information Processing Systems_, volume 33, pp. 1877\u20131901. Curran Associates, Inc.,\n\n\n10\n\n\nPublished as a conference paper at ICLR 2024\n\n\n2020. [URL https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2020\/](https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2020\/file\/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)\n[file\/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.](https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2020\/file\/1457c0d6bfcb4967418"," 23-27, we show one example for each of the 5 error categories we identified through error\nanalysis on STEP-BACK PROMPTING.\n\n\nE.2 EXAMPLE WINS FROM STEP-BACK PROMPTING\n\n\nTables 28- 30, 31 32, 33 and 34 illustrate the some successful examples of STEP-BACK PROMPTING\n\n- n MMLU-Physics, MMLU-Chemistry, TimeQA, SituatedQA, and StrategyQA respectively.\n\n\n21\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset Question Answer\n\n\n\nMMLU A point charge, Q1 = +1 mC, is fixed\nPhysics at the origin. How much work is\nrequired to move another charge, Q2\n= +8 _\u00b5_ C, from the point (0, 4 meters)\nto the point (3 meters, 0)?\n0) 3.5 J 1) 6.0 J 2) 22.5 J 3) 40 J\n\n\nMMLU A sample of an unknown chloride\nChemistry compound was dissolved in water, and then titrated with excess\nPb(NO3)2 to create a precipitate.\nAfter drying, it is determined there\nare 0.0050 mol of precipitate present.\nWhat mass of chloride is present in\nthe original sample?\n0) 0.177 g 1) 0.355 g 2) 0.522 g 3)\n0.710 g\n\n\nGSM8K Bella bought stamps at the post office. Some of the stamps had a\nsnowflake design, some had a truck\ndesign, and some had a rose design.\nBella bought 11 snowflake stamps.\nShe bought 9 more truck stamps\nthan snowflake stamps, and 13 fewer\nrose stamps than truck stamps. How\nmany stamps did Bella buy in all?\n\n\n\nThe work required to move a charge in an\nelectric field is equal to the change in potential\nenergy of the charge. The potential energy of a\ncharge in an electric field is given by: U = k *\nQ1 * Q2 \/ r where: k is the Coulomb constant,\nQ1 and Q2 are the charges, r is the distance\nbetween the charges. In this case, the initial\npotential energy of Q2 is: U1 = k * Q1 * Q"],"36":[" for simplicity in instantiating and to mirror previous\nwork (i.e. NQ, HotpotQA, etc. [Kwiatkowski et al., 2019, Yang et al., 2018]).\n\n\nOur last step is to choose a qrel matrix to instantiate these attributes. Although we could not prove\nthe hardest qrel matrix definitively with theory (as the sign rank is notoriously hard to prove), we\nspeculate based on intuition that our theoretical results imply that the more interconnected the qrel\nmatrix is (e.g. dense with all combinations) the harder it would be for models to represent. [8] Following\nthis, we use the qrel matrix with the highest number of documents for which all combinations would\nbe just above 1000 queries for a top- _\ud835\udc58_ - f 2 (46 docs, since [\ufffd][46] 2 - is 1035, the smallest above 1k).\n\n\nWe then assign random natural language attributes to the queries, adding these attributes to their\nrespective relevant documents (c.f. Figure 1). We give each document a random first and last name\nfrom open-source lists of names. Finally, we randomly sample new attributes for each document until\nall documents have the same number of attributes. As this setup has many more documents than\nthose that are relevant to any query (46 relevant documents, 49.95k non-relevant to any query) we\nalso create a \u201csmall\u201d version with only the 46 documents that are relevant to one of the 1000 queries.\n\n\n8See Appendix 10 for specific metrics that show the difference between LIMIT and other IR datasets.\n\n\n8\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n\n\n\n\n\n\n\n\n|Reca|ll@2|\n|---|---|\n|||\n|||\n|||\n\n\n|Recal|l@10|\n|---|---|\n|||\n|||\n|||\n\n\n|Recal|l@100|\n|---|---|\n|||\n|||\n\n\n\n\n\n\n\n\n\n\n\nFigure 3 | Scores on the LIMIT task. Despite the simplicity of the task we see that SOTA models\nstruggle. We also see that the dimensionality of the model is a limiting factor and that as the\ndimension increases, so does performance. Even multi-vector models struggle. Lexical models like\nBM25 do very well due to their higher dimensionality. Stars indicate models trained with MRL.\n\n\n"," right operators if the documents are\nsufficiently expressive (i.e. non-trivial). Thus, that existing datasets choose to only instantiate some\n\n- f these combinations is mainly for practical reasons and not because of a lack of existence.\n\n\nIn contrast to these previous works, we seek to build a dataset that evaluates all combinations of\ntop- _\ud835\udc58_ sets for a small number of documents. Rather than using difficult query operators like QUEST,\nBrowseComp, etc. (which are already difficult for reasons outside of the qrel matrix) we choose very\nsimple query and documents to highlight the difficulty of representing all top- _\ud835\udc58_ sets themselves.\n\n\n**5.2. The LIMIT Dataset**\n\n\n**Dataset Construction** In order to have a natural language version of this dataset, we need some\nway to map combinations of documents into something that could be retrieved with a query. One\nsimple way to do this is to create a synthetic version with latent variables for queries and documents\nand then instantiate it with natural language. For this mapping, we choose to use attributes that\nsomeone could like (i.e. Jon likes Hawaiian pizza, sports cars, etc. ) as they are plentiful and don\u2019t\npresent issues w.r.t. other items: one can like Hawaiian pizza but dislike pepperoni, all preferences\nare valid. We then enforce two constraints for realism: (1) users shouldn\u2019t have too many attributes,\nthus keeping the documents short (less than 50 per user) and (2) each query should only ask for one\nitem to keep the task simple (i.e. \u201cwho likes X\u201d). We gather a list of attributes a person could like\nthrough prompting Gemini 2.5 Pro. We then clean it to a final 1850 items by iteratively asking it to\nremove duplicates\/hypernyms, while also checking the top failures with BM25 to ensure no overlap.\n\n\nWe choose to use 50k documents in order to have a hard but relatively small corpus and 1000\nqueries to maintain statistical significance while still being fast to evaluate. For each query, we choose\nto use two relevant documents (i.e. _\ud835\udc58_ =2), both for simplicity in instantiating and to mirror previous\nwork (i.e. NQ, HotpotQA, etc. [Kwiatkowski et al., 2019, Yang et al., 2018]).\n\n\nOur last step is to",". Metrics Measuring Qrel Graph Density**\n\n\nWe show two metrics that treat the qrel matrix as a graph and show that LIMIT has unique properties\ncompared to standard IR datasets (Table 1). We call these metrics Graph Density and Average Query\nStrength and describe them below.\n\n\n**Graph Density** We use the qrel matrix to construct the graph, where nodes are documents and an\nedge exists between two documents if they are both relevant to at least one common query.\n\n\nFor a given graph _\ud835\udc3a_ = ( _\ud835\udc49, \ud835\udc38_ ) with _\ud835\udc49_ being the set of nodes and _\ud835\udc38_ being the set of edges, the graph\ndensity is defined as the ratio of the number of edges in the graph to the maximum possible number\n\n- f edges. For an undirected graph, the maximum possible number of edges is [|] _[\ud835\udc49]_ [|] [(|] _[\ud835\udc49]_ 2 [|] [\u2212][1][)] . Thus, the\n\ndensity _\ud835\udf0c_ is calculated as:\n\n\n\n_\ud835\udc38_ | 2| _\ud835\udc38_ |\n\n=\n\n_\ud835\udc49_ | - 1) | _\ud835\udc49_ |(| _\ud835\udc49_ | \u2212 1)\n\n2\n\n\n\n| _\ud835\udc38_ |\n_\ud835\udf0c_ =\n\n\n\n| _\ud835\udc49_ | (| _\ud835\udc49_ | - 1)\n\n\n\nThis metric indicates how connected the graph is; a density of 1 signifies a complete graph (all\npossible edges exist), while a density close to 0 indicates a sparse graph. For a qrel dataset, the\n\n\n**Average Query Strength** In a query-query graph where nodes are queries and edges represent\nsimilarity between queries (e.g., Jaccard similarity of their relevant documents), the _strength_ - f a\nquery node _\ud835\udc56_, denoted _\ud835\udc60\ud835\udc56_, is defined as the sum of the weights of all edges incident to it. If _\ud835\udc64\ud835\udc56\ud835\udc57_ is the\nweight of the edge between query _\ud835\udc56_ and query _\ud835\udc57_, and _\ud835\udc41_ ( _\ud835\udc56_ ) is the set of neighbors of query _\ud835\udc56_, then the\nstrength is:\n\n\n_\ud835\udc60\ud835\udc56_ = \ufffd"," [\u00d7] _[\ud835\udc5b]_ in a row-wise order-preserving manner in\n_\ud835\udc51_ dimensions, this implies a bound on the sign rank of 2 _\ud835\udc34_   - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ . In particular, this suggests\na _practical mechanism_ for determining an upper-bound on sign-rank for matrices via gradient\ndescent optimization of free embedding representations.\n\n##### **4. Empirical Connection: Best Case Optimization**\n\n\nWe have now established a theoretical limitation of embedding models based on the sign-rank of the\nqrel matrix and their embedding dimension _\ud835\udc51_ . Now we seek to show that this empirically as well.\n\n\nTo show the strongest optimization case possible, we design experiments where the vectors\nthemselves are directly optimizable with gradient descent. [5] We call this \u201cfree embedding\u201d optimization,\nas the embeddings are free to be optimized and not constrained by natural language, which imposes\nconstraints on any realistic embedding model. Thus, this shows whether it is feasible for **any**\n**embedding model** to solve this problem: if the free embedding optimization cannot solve the\nproblem, real retrieval models will not be able to either. It is also worth noting that we do this by\ndirectly optimizing the embeddings over the target qrel matrix (test set). This will not generalize to a\nnew dataset, but is done to show the highest performance that could possibly occur.\n\n\n**Experimental Settings** We create a random document matrix (size _\ud835\udc5b_ ) and a random query matrix\nwith top- _\ud835\udc58_ sets (of all combinations, i.e. size _\ud835\udc5a_ = [\ufffd] _[\ud835\udc5b]_ _\ud835\udc58_ - ), both with unit vectors. We then directly optimize\nfor solving the constraints with the Adam optimizer [Kingma and Ba, 2014]. [6] Each gradient update\nis a full pass through all correct triples (i.e. full dataset batch-size) with the InfoNCE loss function\n\n\n5This could also be viewed as an embedding model where each query\/doc are a separate vector via a lookup table.\n6We found similar results with SGD, but we use Adam for speed and similarity with existing training methods.\n\n\n6\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n[Oord et al., 2018], [","I\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So\n\nthe answer is: 1894.\n\n\nQ: When was Neville A. Stanton's employer founded?\nA: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862.\n\nSo the answer is: 1862.\n\n\nQ: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to?\nA: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located\n\nin the Prince County. So the answer is: Prince County.\n\n\nQ: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America\u2212\n\nLite: How Imperial Academia Dismantled Our Culture's author?\nA: The author of America\u2212Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was\n\neducated at the Yale University. The city in Connecticut that has the highest number of Zagat\u2212rated restaurants is New\nHaven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale\nHerald.\n\n\nQ: What is the headquarters for the organization who sets the standards for ISO 21500?\nA: The standards for ISO 21500 were set by International Organization for Standardization. The International Organization\n\nfor Standardization has headquarters in Geneva. So the answer is: Geneva.\n\n\nQ: What did the publisher of Banjo\u2212Tooie rely primarily on for its support?\nA: The publisher of Banjo\u2212Tooie is Nintendo. Nintendo relied primarily for its support on first\u2212party games. So the answer is:\n\nfirst\u2212party games.\n\n\nQ: In which county was the birthplace of the Smoke in tha City performer?\nA: The performer of Smoke in tha City is MC Eiht. MC Eiht's birthplace is Compton. Compton is located in the county of Los\n\nAngeles County. So the answer is: Los Angeles County.\n\n\nQ: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?\n\n\nA: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central\n\nJakarta.\n\n\nQ:"],"37":[" and the Family in May\n2002. She held this position until 2005.\n**Stepback Question** : What positions did Carmel Hanna hold?\n**Final Answer [Correct]** : From the context, we can see that Carmel\nHanna was named minister of Employment and Learning (2001). We\ncan then infer that she held the position of Minister for Employment and\nLearning in May 2002.\n\n\nTable 32: Illustration of wins of STEP-BACK PROMPTING on the TimeQA dataset.\n\n\n36\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nSituatedQA **Original Question** : when was the last time miss texas won miss america\nas of November 29, 2010\n**Original Answer [Incorrect]** : Laura Leigh Covington was the last Miss\nTexas to win the Miss America competition. This happened in 2000.\n**Stepback Question** : which years did miss texas win miss america\n**Final Answer [Correct]** : According to Wikipedia, the last time Miss\nTexas won Miss America was in 1975.\n\n\nSituatedQA **Original Question** : when is the last time congress passed a complete\nbudget as of 2021\n**Original Answer [Incorrect]** : The last time Congress passed a complete\nbudget as of 2021 was on January 19, 2018.\n**Stepback Question** : what are the years that congress passed a complete\nbudget as of 2021\n**Final Answer [Correct]** : The last time Congress passed a complete budget as of 2021 was December 27, 2020, the Consolidated Appropriations\nAct, 2021.\n\n\nSituatedQA **Original Question** : when was the last time unc did not make the ncaa\ntournament as of 2003\n**Original Answer [Incorrect]** : The last time UNC did not make the\nNCAA tournament was 1990.\n**Stepback Question** : which years did unc not make the ncaa tournament\nas of 2003\n**Final Answer [Correct]** : The correct answer is 2002-03. The context\nshows the standings of North Carolina Tar Heels men\u2019s basketball. From\nthe standings, we can see that this team has 19 wins and 16 losses. 19-16\nis not good enough to make the NCAA tournament, which is mentioned\nin the context.","doi.org\/10.48550\/arXiv.2304.04358)\n[Learning to generate factually correct articles for](https:\/\/doi.org\/10.48550\/arXiv.2304.04358)\n[queries by grounding on large web corpus.](https:\/\/doi.org\/10.48550\/arXiv.2304.04358) _CoRR_,\nabs\/2304.04358.\n\n\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\n[Liu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-](https:\/\/doi.org\/10.48550\/arXiv.2305.06849)\n[teractive web search for chinese long-form question](https:\/\/doi.org\/10.48550\/arXiv.2305.06849)\n[answering.](https:\/\/doi.org\/10.48550\/arXiv.2305.06849) _CoRR_, abs\/2305.06849.\n\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\n[Dario Amodei, and Ilya Sutskever. 2019. Language](https:\/\/d4mucfpksywv.cloudfront.net\/better-language-models\/language-models.pdf)\n[models are unsupervised multitask learners.](https:\/\/d4mucfpksywv.cloudfront.net\/better-language-models\/language-models.pdf) _OpenAI_\n_Blog_, 1(8).\n\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented language models. _arXiv preprint arXiv:2302.00083_ .\n\n\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\n\n[How much knowledge can you pack into the param-](https:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.437)\n[eters of a language model? In](https:\/\/doi.org\/","might not be necessary. ELI5 (Fan et al., 2019)\nis a long-form QA dataset requiring in-depth answers to open-ended questions. Due to issues mentioned in Krishna et al. (2021) such as difficulties\n\n- f grounding generation in retrieval and evaluation, both single-time retrieval and FLARE did not\nprovide significant gains over not using retrieval.\nFrom an engineering perspective, interleaving generation and retrieval with a naive implementation\nincreases both overheads and the cost of generation.\nLMs need to be activated multiple times (once for\neach retrieval) and a caching-free implementation\nalso requires recomputing the previous activation\neach time after retrieval. This issue can be potentially alleviated with special architectural designs\nthat encode the retrieved documents _D_ _**q**_ _t_ and the\ninput\/generation ( _**x**_ \/ _**y**_ _<t_ ) independently.\n\n\n**Acknowledgements**\n\n\nThis work was supported in part by a grant from\nthe Singapore Defence Science and Technology\nAgency and the IBM PhD Fellowship. We thank\nChunting Zhou, Amanda Bertsch, Uri Alon, Hiroaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo\nSchick, Kaixin Ma, Shuyan Zhou, and Songwei Ge\nfor their insightful discussions and help with the\nexperiments.\n\n\n**References**\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n[2022. Improving language models by retrieving from](https:\/\/proceedings.mlr.press\/v162\/borgeaud22a.html)\n[trillions of tokens. In](https:\/\/proceedings.mlr.press\/v162\/borgeaud22a.html) _International Conference on_\n_Machine Learning, ICML ","11386)\n\n[16] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\nGoharian, and Ophir Frieder. 2020. Expansion via Prediction of Importance with\n\n\n\nContextualization. _Proceedings of the 43rd International ACM SIGIR Conference on_\n_Research and Development in Information Retrieval_ [(Jul 2020). https:\/\/doi.org\/10.](https:\/\/doi.org\/10.1145\/3397271.3401262)\n[1145\/3397271.3401262](https:\/\/doi.org\/10.1145\/3397271.3401262)\n\n[17] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\n[arXiv:1901.04085 [cs.IR]](https:\/\/arxiv.org\/abs\/1901.04085)\n\n[18] Rodrigo Nogueira and Jimmy Lin. 2019. From doc2query to docTTTTTquery.\n\n[19] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\n[Expansion by Query Prediction. arXiv:1904.08375 [cs.IR]](https:\/\/arxiv.org\/abs\/1904.08375)\n\n[20] Biswajit Paria, Chih-Kuan Yeh, Ian E. H. Yen, Ning Xu, Pradeep Ravikumar, and\nBarnab\u00e1s P\u00f3czos. 2020. Minimizing FLOPs to Learn Efficient Sparse Representa[tions. arXiv:2004.05665 [cs.LG]](https:\/\/arxiv.org\/abs\/2004.05665)\n\n[21] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPyTorch: An Imperative Style, High-Performance Deep Learning Library.. In\n_NeurIPS_ .\n\n[22] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In _Proceed"," 23-27, we show one example for each of the 5 error categories we identified through error\nanalysis on STEP-BACK PROMPTING.\n\n\nE.2 EXAMPLE WINS FROM STEP-BACK PROMPTING\n\n\nTables 28- 30, 31 32, 33 and 34 illustrate the some successful examples of STEP-BACK PROMPTING\n\n- n MMLU-Physics, MMLU-Chemistry, TimeQA, SituatedQA, and StrategyQA respectively.\n\n\n21\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset Question Answer\n\n\n\nMMLU A point charge, Q1 = +1 mC, is fixed\nPhysics at the origin. How much work is\nrequired to move another charge, Q2\n= +8 _\u00b5_ C, from the point (0, 4 meters)\nto the point (3 meters, 0)?\n0) 3.5 J 1) 6.0 J 2) 22.5 J 3) 40 J\n\n\nMMLU A sample of an unknown chloride\nChemistry compound was dissolved in water, and then titrated with excess\nPb(NO3)2 to create a precipitate.\nAfter drying, it is determined there\nare 0.0050 mol of precipitate present.\nWhat mass of chloride is present in\nthe original sample?\n0) 0.177 g 1) 0.355 g 2) 0.522 g 3)\n0.710 g\n\n\nGSM8K Bella bought stamps at the post office. Some of the stamps had a\nsnowflake design, some had a truck\ndesign, and some had a rose design.\nBella bought 11 snowflake stamps.\nShe bought 9 more truck stamps\nthan snowflake stamps, and 13 fewer\nrose stamps than truck stamps. How\nmany stamps did Bella buy in all?\n\n\n\nThe work required to move a charge in an\nelectric field is equal to the change in potential\nenergy of the charge. The potential energy of a\ncharge in an electric field is given by: U = k *\nQ1 * Q2 \/ r where: k is the Coulomb constant,\nQ1 and Q2 are the charges, r is the distance\nbetween the charges. In this case, the initial\npotential energy of Q2 is: U1 = k * Q1 * Q"],"38":[" pages and extracting relevant information from noisy\nretrieved data. Results for Qwen2-VL demonstrate stronger document understanding capabilities,\n\n- utperforming MiniCPM-V 2.6 in these tasks.\n\n\nH RETRIEVAL EFFICIENCY\n\n\nIn this experiment, we evaluate the retrieval efficiency of VisRAG-Ret and MiniCPM (OCR) by\nmeasuring two key components: offline document parsing and encoding latency, and online query\nencoding and search latency. Query and document encoding are conducted on an NVIDIA A100\n40G GPU with a batch size of 1, while document parsing is performed on a single core of an Intel\nXeon Platinum 8350C CPU. The reported latencies are averaged over the queries and documents\nfrom the PlotQA dataset. The results are summarized in Table 12.\n\n\nAs shown in the table, although VisRAG-Ret, a VLM-based model, requires more time for document\nencoding compared to MiniCPM (OCR), it bypasses the time-consuming parsing stage required by\n\n\n24\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 12: Retrieval efficiency (ms). We report offline latencies per document, including document\nparsing and encoding latencies, as well as online latencies per query, including query encoding and\nsearch latencies.\n\n|Col1|OfflineLatencyperDocument<br>Parsing Encoding Total|OnlineLatencyperQuery<br>Encoding Search Total|\n|---|---|---|\n|MiniCPM (OCR)<br>VisRAG-Ret|284<br>28<br>312<br>\u2013<br>121<br>121|28<br>26<br>54<br>28<br>26<br>54|\n\n\n\nMiniCPM (OCR). This leads to a 58% reduction in total document processing time for VisRAG-Ret.\nFor online query processing, the latencies of VisRAG-Ret and MiniCPM (OCR) are nearly identical,\nas the queries consist solely of textual inputs.\n\n\nI RETRIEVAL PERFORMANCE ON TEXT RETRIEVAL BENCHMARKS\n\n\nTable 13: Retrieval performance on subsets of the text retrieval benchmark BEIR (Thakur et al.,\n2021) in NDCG@10. VisRAG-Ret performs retrieval on rendered document screenshots.\n\n\n**Model** **SciFact** **NFCorpus** **Scidocs**\n\n\nMiniCP"," than VisRAG-Ret, indicating that directly encoding\nwith VLMs works better than using VLMs for parsing. This can be attributed to the inevitable\ninformation loss when multi-modality information is transcribed into text.\n\n\nFurther analysis reveals that MiniCPM (OCR) and SigLIP perform differently across datasets:\nSigLIP excels in ArxivQA and ChartQA, while MiniCPM (OCR) significantly outperforms SigLIP\nin DocVQA and InfographicsVQA. This may be due to the different focuses of the two models:\nMiniCPM focuses on text, while SigLIP focuses on visual signals. VisRAG-Ret, built on top of\nMiniCPM-V 2.0, with a SigLIP encoder and a MiniCPM language model, combines the merits of\nboth and performs well across all datasets, capturing more holistic information from a document.\n\n\nCompared to ColPali, a multi-vector document page embedding model, VisRAG-Ret not only maintains superior performance but also achieves much better memory efficiency. ColPali represents a\npage with 256KB of data distributed across 1030 128-dim vectors (Faysse et al., 2024), whereas\nVisRAG-Ret uses just 4.5KB in a single 2304-dimensional vector. This makes VisRAG-Ret more\nsuitable for scaling to millions or billions of documents in real-world applications.\n\n\n**Generation Performance.** In this experiment, we apply a series of text- and vision-based generators and methods on top of the same retriever VisRAG-Ret to study their effectiveness in generating\nthe answer given the query and retrieved documents. Table 3 shows the performance of (a) text-based\ngeneration (TextRAG-Gen), (b) generation using the VLM MiniCPM-V 2.0 which only accepts a\nsingle image as input, and (c) generation using VLMs which accept multiple images as input.\n\n\nWhen models are provided with only the ground-truth documents (\u201cOracle\u201d), VisRAG-Gen models,\nwhich process the document image directly, significantly outperform TextRAG-Gen models, which\n\n\n8\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nFigure 3: Pipeline performance of (a) TextRAG and (b) VisRAG on InfographicsVQA. We visualize\nthe portion","Gen models, which\n\n\n8\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nFigure 3: Pipeline performance of (a) TextRAG and (b) VisRAG on InfographicsVQA. We visualize\nthe portion of queries that have the positive document retrieved at the top-1 position (\u201cCorrect Retrieval\u201d), and that are answered correctly given the top-1 retrieved document (\u201cCorrect Generation\u201d).\n\n\nrely solely on extracted text. For instance, MiniCPM-V 2.0 achieves 30% higher performance than\nMiniCPM (OCR) when using ground-truth documents. This underscores the importance of visual\nclues in extracting answers from documents.\n\n\nIn practical scenarios where models receive the top-1 to 3 retrieved documents, which may include noise, VisRAG-Gen consistently outperforms TextRAG-Gen within the same model series.\nSpecifically, for MiniCPM-V 2.0, capable of processing only a single image, the weighted selection\napproach demonstrates better performance than page concatenation when handling 2 or 3 retrieved\ndocuments. However, neither method shows a performance improvement as the number of retrieved\ndocuments increases, a trend commonly observed in TextRAG pipelines (Zhu et al., 2024). In contrast, MiniCPM-V 2.6 and GPT-4o, both capable of processing multiple images as input, exhibit\na notable performance gain as the number of retrieved documents increases, suggesting that only\nVLMs pre-trained on multi-image data can effectively reason over multiple retrieved pages.\n\n\n**End-to-end Performance.** In this experiment, we study the effectiveness of the VisRAG _pipeline_,\nby comparing it with the TextRAG pipeline. We construct TextRAG using MiniCPM (OCR) and\nMiniCPM-V 2.6 (OCR) for retrieval and generation, respectively, and VisRAG using VisRAG-Ret\nfor retrieval and MiniCPM-V 2.6 for generation. The performance on InfographicsVQA is visually\nrepresented in Figure 3. Notebly, VisRAG achieves a higher rate of accurately retrieving documents than TextRAG, and demonstrates a significantly improved rate of correct answer generation\nfrom accurately retrieved documents. The cumulative improvements in both retrieval and generation phases result in an overall accuracy increment from 25% to 51%. Across the six evaluation\ndatasets, VisRAG shows","br>44.96 (56.1%)<br>80.19 (100%)|55.58 (85.1%)<br>58.99 (90.4%)<br>58.63 (89.8%)<br>65.29 (100%)|58.12 (80.0%)<br>58.32 (80.2%)<br>59.03 (81.0%)<br>73.35 (100%)|\n\n\n\nIn this section, we present supplementary evaluation results for both retrieval and generation on our\ndataset.\n\n\nTable 10 shows additional retrieval results obtained by applying reciprocal rank fusion (RRF) (Cormack et al., 2009) to combine the outputs of MiniCPM (OCR) and SigLIP. It is a straightforward\nmethod to integrate textual information extracted from the page with its visual clues. The results\nindicate that fusing text and image modalities provides a meaningful performance boost over individual modality baselines. However, this approach still falls short of the performance achieved\nby our VisRAG-Ret model (71.49 for out-of-domain, 77.91 for in-domain). This underscores the\nsuperior capability of VisRAG-Ret in understanding both modalities within a unified architecture.\n\n\nTable 11 provides additional generation results using top-6 and top-10 retrieved documents from\nVisRAG-Ret. For these experiments, we evaluate the performance of MiniCPM-V 2.0 using the\npage concatenation method and MiniCPM-V 2.6 with direct feeding. We also report the performance of another SOTA VLM, Qwen2-VL-7B-Instruct (Wang et al., 2024). The results indicate\nsignificant performance degradation when handling a larger number of retrieved pages, for both\npage concatenation (MiniCPM-V 2.0) and multi-page input (MiniCPM-V 2.6). MiniCPM-V 2.6\nexhibits greater robustness to increasing context compared to MiniCPM-V 2.0. Open-source VLMs\nstill face challenges in reasoning over multiple pages and extracting relevant information from noisy\nretrieved data. Results for Qwen2-VL demonstrate stronger document understanding capabilities,\n\n- utperforming MiniCPM-V 2.6 in these tasks.\n\n\nH RETRIEVAL EFFICIENCY","1344x1344) at any aspect ratio. We use MiniCPM-V 2.0 to\nbuild VisRAG-Ret (Table 2) and VisRAG-Gen (Table 3(b)), as well as the document parsing model.\n\n\n**MiniCPM-V 2.6** (OpenBMB, 2024b; Yao et al., 2024) is an upgrade of MiniCPM-V 2.0 and\nMiniCPM-Llama3-V 2.5 (Yao et al., 2024). It is built upon SigLIP-400M and Qwen2-7B (Yang\net al., 2024) with a total of 8.5B parameters, exihibiting a significant performance improvement\n\n- ver MiniCPM-Llama3-V 2.5 (Yao et al., 2024). Different from previous models, MiniCPM-V\n2.6 can accept multiple images as the input and perform multi-modal in-context learning. It also\ndemonstrates stronger OCR capabilities. We use MiniCPM-V 2.6 to build VisRAG-Gen (Table 3)\nand a text-based generation baseline MiniCPM-V 2.6 (OCR) (Figure 3, Figure 5).\n\n\nNote that, MiniCPM-Llama3-V 2.5 (Yao et al., 2024) is not used in this paper.\n\n\n**GPT-4o** (OpenAI, 2024) is OpenAI\u2019s latest multi-modal model, capable of processing any combination of text, audio, image, and video inputs and generating outputs in text, audio, and image\nformats. We use GPT-4o to construct VisRAG-Gen (Table 3) and to synthesize training data.\n\n\nD RETRIEVAL PERFORMANCE IN RECALL@10\n\n\nTable 6 presents the retrieval performance in Recall@10.\n\n\nE PROMPTS FOR GENERATION\n\n\nWe present the prompts of VisRAG-Gen and TextRAG-Gen in Table 7.\n\n\n2https:\/\/huggingface.co\/HuggingFaceM4\/siglip-so400m-14-980-flash-attn2-navit\n\n\n20\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 6: Overall retrieval performance in Recall@10.\n\n\nTable 7: Prompt templates for generation. \u201c"],"39":[" typed as above. We sample new entities uniform randomly from the 5% least common entities\nin NewsCrawl ( _I_ _[RandArg][\u2193]_ ), and the 5% most common ( _I_ _[RandArg][\u2191]_ ). We insert the sampled entities\nwhile preserving the rest of each statement.\n\n\n**4** **Querying Models with Prompts**\n\n\n**4.1** **Models**\n\n\n**LLaMA** is a recent LLM model family which\nrivals or surpasses GPT-3 performance while being\n\n- pen to scientific study. A range of model sizes\n\n\nare provided, and we test the largest **LLaMA-65B**\nmodel. LLaMA is not fine-tuned. In preliminary\nexperiments on the Levy\/Holt dataset, we found\ntwo popular fine-tuned LLaMA variants, Alpaca\n(Taori et al., 2023) and Vicuna (Chiang et al., 2023),\nperform similarly to LLaMA base models and underperform LLaMA-65B, so we leave them out of\nfurther experiments.\n\n\n**GPT-3 Series** models are closed to deep scientific review (Brown et al., 2020), though they are a\nwidely-used comparison for their performance, and\nhave been reasonably well-studied. We evaluate on\n**text-davinci-003 (GPT-3.5)**, as it is the largest, and\nhas undergone instruction- and RLHF-finetuning,\nenabling interesting comparisons.\n\n\n**PaLM** is larger than GPT-3, which often claims\nstate-of-the-art on evaluation datasets. We use the\n\nlargest **PaLM-540B** base model, which is also only\npretrained, so it serves as a further comparison\npoint to LLaMA.\n\n\nLater GPT models (like text-davinci-003 in our\nexperiments) have been pre-trained and fine-tuned,\nwhile base LLaMA and PaLM have only undergone pre-training, so their contrast indicates what\nstage of training is responsible for the phenomena\nwe study. Our aim is not to judge which LLM\nis superior, but to show the common sources of\nhallucination they share.\nWe also omit models superseded in performance\nby LLaMA (e.g. OPT, GPT-J, etc"," beyond direct code generation to include\nthe provision of external knowledge and data, which is\ncrucial in distilling their expertise into smaller, more efficient models. Several works have successfully distilled code\nknowledge from LLMs into those compact and specialized\ncode models (Chaudhary, 2023; Rozi`ere et al., 2023; Gunasekar et al., 2023; Wei et al., 2023; Chen et al., 2023a;\nLiu et al., 2023d; Yu et al., 2024; Jain et al., 2023; Su and\nMcMillan, 2023; Guo et al., 2023d).\nA primary focus in these student code models is on\ncode generation, a task of both common utility and practical\nsignificance. For instance, Code Alpaca (Chaudhary, 2023)\nfine-tunes Llama using self-instruct with ChatGPT-distilled\ninstructions specifically for code generation tasks. Similarly,\nCode Llama-instruct (Rozi`ere et al., 2023) is fine-tuned via\nself-instruct, prompting Llama-2 (Touvron et al., 2023) with\ncoding problems, and further refined with unit tests. Phi1 (Gunasekar et al., 2023) aims to enhance the quality of distilled code data by extracting \u201ctextbook quality\u201d data from\na teacher LLM, incorporating Python textbook and exercise\ndata. Magicoder (Wei et al., 2023) addresses potential biases\nin teacher LLMs by referencing a wealth of open-source\ncode, yielding more diverse and grounded data for code\ngeneration. To consider the capability of the student model\n\n\nand leverage the feedback of the teacher, PERsD (Chen et al.,\n2023a) introduces a Personalized Distillation method where\nthe teacher LLM refines the student\u2019s generated code based\n\n- n the execution feedback of the executor.\nHowever, these models primarily target the code generation task, lacking generalizability across a broader range\n\n- f code-related tasks. To address this issue, MFTCoder (Liu\net al., 2023d) utilizes self-instruct to distill diverse code data\nfrom teacher LLMs for various tasks, such as code completion and text-to-code generation, training a student model\nvia"," life. By analyzing vast datasets of biological sequences and structures,\nLLMs can predict the three-dimensional shapes of proteins,\npotential functions, and interactions at a scale and speed\nbeyond traditional computational methods. This capability\nis critical for unraveling the complexities of biological systems, advancing drug discovery by identifying targets and\ndesigning molecules with high precision, and understanding genetic diseases through the interpretation of genomic\nvariations.\n\nProt2Text (Abdine et al., 2023) introduces a novel multimodal framework for generating protein function descriptions in free text by combining GNNs and LLMs. This\napproach, which integrates structural and sequential protein\ninformation, highlights the transformative impact of knowledge distillation through the fusion of GNNs and LLMs\nfor accurate protein function prediction, potentially revolutionizing research in bioinformatics and biological sciences.\nBioMedGPT (Luo et al., 2023e) introduces a multimodal\ngenerative pre-trained transformer specifically designed for\nthe biomedicine domain, emphasizing the significance of\naligning molecular, protein, and natural language modalities to enhance biomedical question-answering, molecule,\nand protein QA tasks. This framework showcases the critical\nrole of knowledge distillation in bridging the gap between\ncomplex biological data and human language, thereby facilitating groundbreaking advancements in drug discovery\nand therapeutic target identification. xTrimoPGLM (Chen\net al., 2024e), a unified 100B-scale pre-trained transformer\nmodel, addresses both protein understanding and generation tasks by integrating autoencoding and autoregressive\npre-training objectives. Its significant advancements over\nexisting models in 18 protein understanding benchmarks\nand its capability in de novo protein sequence generation\nhighlight the model\u2019s importance in advancing the field of\nprotein science through knowledge distillation.\n\n\n_**Geography, Geology, and Environmental Science.**_ The integration of LLMs into Geography, Geology, and Environmental Science is revolutionizing these fields by enhancing data\nanalysis, predictive modeling, and interdisciplinary research\n(Roberts et al., 2023; Lin et al., 2023b; Wang et al., 2023l).\nK2 (Deng et al., 2023), the first-ever LLM specialized in\nthe geoscience domain, demonstrates the significant impact\n\n\n- f knowledge distillation in vertical domain specialization.\nBy adapting the general-domain LLaMA-7B model with a\n5"," (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao et al. 2020; Raunak, Menezes,\nand Junczys-Dowmunt 2021; Ji et al. 2023), knowledge outdating (He, Zhang, and Roth 2022), and the lack of domainspecific expertise (Li et al. 2023c; Shen et al. 2023).\nIncorporating external knowledge via information retrieval, i.e., Retrieval-Augmented Generation (RAG), has\nbeen regarded as a promising way to resolve the above challenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\n\n\n  - Corresponding authors.\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\n2022; Izacard et al. 2022). With the help of external knowledge, LLMs can generate more accurate and reliable responses. The most common method is to use a search engine\nas a retriever such as New Bing. Due to the vast amount of\ninformation available on the Internet, using a search engine\ncan provide more real-time information.\n\nHowever, Retrieval-Augmented Generation brings not\n\n- nly positive effects to LLMs (Liu, Zhang, and Liang 2023;\nMaynez et al. 2020). On one hand, there is a significant\namount of noise information even fake news in the content\navailable on the Internet, which poses challenges for search\nengines in accurately retrieving desirable knowledge. On the\n\n- ther hand, LLMs suffer from unreliable generation challenge. LLMs can be misled by incorrect information contained in the context (Bian et al. 2023) and also suffer from\nhallucination during the generation (Adlakha et al. 2023),\nresulting in generating content that goes beyond external in\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nformation. These challenges result in LLMs being unable to\nconsistently generate reliable and accurate responses. Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how\ncould each model survives from these drawbacks and improvement"," efficiency of research workflows and\nfostering interdisciplinary collaborations, LLMs are poised\nto accelerate the pace of scientific discovery and innovation\nacross various fields. To distill knowledge from an LLM,\nDARWIN Series (Xie et al., 2023a) utilizes a semi selfinstruct for instruction generation for science papers, which\nis then used to fine-tune an LLM. SciGLM (Zhang et al.,\n2024) proposes to train a scientific LLM, which prompts a\nteacher LLM to generate detailed answers for unlabelled\nscientific questions, as well as a self-reflective critic-andrevise to improve data quality. Besides the above knowledge\ndistillation methods to adapt LLMs to science, we will also\ndelve into how the distillation happens in sub-domains, e.g.,\nmathematics, astronautics, chemistry, etc.\n\n\n_**Mathematics.**_ The application of LLMs within the subdomain of mathematics heralds a transformative era in\nmathematical research, education, and problem-solving\n(Azerbayev et al., 2023; Yu et al., 2023b). LLMs in mathematics facilitate the exploration and understanding of complex\nmathematical theories and problems by providing intuitive\nexplanations, proofs, and solutions that can bridge the\ngap between advanced mathematical concepts and learners at various levels. These models have shown potential\nin conjecturing new mathematical theorems and patterns,\nthus opening new avenues for research and discovery that\nmight not have been readily accessible to humans alone.\nIn education, they serve as personalized tutors, offering\nstudents step-by-step guidance through mathematical problems and adapting explanations to the learner\u2019s level of understanding. This democratizes access to high-quality mathematical education and fosters a deeper appreciation and\nunderstanding of mathematics among a broader audience.\nBy enhancing collaborative efforts through the generation\n\n- f new ideas and the simplification of complex concepts,\nLLMs are poised to significantly advance the field of mathematics, making it more accessible, efficient, and innovative. WizardMath (Luo et al., 2023b) enhances the mathematical reasoning capabilities of Llama-2 by applying the\n\n\nnovel Reinforcement Learning from Evol-Instruct Feedback\n(RLEIF) method, significantly outperforming other opensource LLMs on the GSM8k and MATH benchmarks, as\nwell as surpassing several closed-source LLM"],"40":[", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop\/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation"," 0.61 0.44 0.56 0.67\n\n25 0.44 0.89 0.56 0.44 0.44 0.56\n\n\nTable 3: **Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall\u2019s Tau** : The Kendall\u2019s tau values represent\nthe correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same\nexperimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human\npreference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their\naccuracies in context relevance and answer relevance (C.R. and A.R., respectively).\n\n\n**ARES Ranking of Pseudo RAG Systems using GPT-4 Labels**\n\n\nNQ ReCoRD MultiRC\n\n\nContext Answer Context Answer Context Answer\n\nRelevance Relevance Relevance Relevance Relevance Relevance\n\n\nKendall\u2019s Tau 0.78 1.0 0.78 0.72 0.89 0.78\n\n\nKendall\u2019s Tau of\n0.94 1.0 0.83 0.89 0.94 0.89\nHuman Labeled Approach\n\n\nAverage PPI Range 9.2% 6.8% 8.2% 9.0% 7.7% 8.3%\n\n\nAccuracy on\n79.3% 96.7% 88.4% 78.3% 85.8% 82.5%\nRAG Evaluation Sets\n\n\nTable 4: **GPT-4 Labels vs. Human Labels** : We wanted to explore the practicality of using GPT-4 generated\nlabels instead of human annotations for our human preference validation set in ARES. In the experiments, we\ngenerated 500 GPT-4 labels as replacements for human labeling using few-shot prompts (see Sections A.2, A.3,\nand A.4). While GPT-4 generated labels decreased Kendall\u2019s tau in most settings by 0.05 to 0.30, the ability to\ncheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of\nannotations to less than ten for few-shot prompts. Additionally, the efficacy of P"," confidence set of the ML\nmodel\u2019s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence\ninterval for the performance of the evaluated RAG\nsystem (e.g. its context relevance, answer faithfulness, or answer relevance accuracy individually)\ncompared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the human preference validation set with the much larger\nset of datapoints with ML predictions, PPI can develop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\n\n\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confidence bounds for the success and failure rates of the\n\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (probability) for our confidence interval.\n\n\nWith the accuracy confidence interval for each\ncomponent of the RAG, we find the midpoint of\neach confidence interval and use the midpoints to\nrank the RAG systems. With our ranking, we can\ncompare different RAG systems, as well as different configurations of the same RAG system, to find\nthe best-performing approach for a given domain.\n\n\n**4** **Experiments**\n\n\n**4.1** **Models**\n\n\nFor our fine-tuned judges, ARES relies on generating cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use OpenAI\u2019s _gpt-3.5-turbo-16k_, version 10\/23, (Brown\net al., 2020) in a zero\/few-shot setting. For similarity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand Open",".05 to 0.30, the ability to\ncheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of\nannotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we\ngenerate more GPT-4 generated labels. In the table, we define PPI range as the number of percentage points from\nthe lower number to the upper number of the PPI confidence bounding. Additionally, we use the fine-tuned LLM\njudge (DeBERTa-v3-Large) for evaluation.\n\n\n**ARES Ranking of Real RAG Systems**\n\n\nNQ WoW FEVER\n\n\nC.R. A.R. C.R. A.R. C.R. A.R.\n\n\nKendall\u2019s Tau for\n0.73 0.78 0.73 0.73 0.73 0.82\nSampled Annotations\n\nKendall\u2019s Tau\n0.82 0.82 0.73 0.82 0.73 0.87\nfor RAGAS\n\n\nKendall\u2019s Tau\n0.82 0.87 0.82 0.82 0.64 0.87\nfor GPT-3.5 Judge\n\nKendall\u2019s Tau\n0.91 **0.96** **0.91** **1.0** 0.73 0.87\nfor ARES LLM Judge\n\nKendall\u2019s Tau\n**1.0** **0.96** **0.91** **1.0** **0.82** **1.0**\nfor ARES\n\n\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\n\n\nTable 5: **ARES Ranking on Real-World RAG Systems** : For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against"," evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges). We compare against RAGAS (version 0.0.18) and a baseline few-shot prompted GPT3.5 judge ( _gpt-3.5-turbo-16k_ ). For the few-shot\nGPT-3.5 judge, we provide few-shot examples for\nguiding predictions; the prompts are included in\nAppendices A.2, A.3, and A.4. For both ARES\nand the GPT-3.5 judge baseline, we augment the\nLLM with PPI, using a 300-datapoint human preference validation set to rectify the ML predictions\nand produce confidence intervals.\n\n\n\nAcross almost all settings across the datasets\nfrom KILT and SuperGLUE, ARES provides a\nmore accurate ranking of RAG systems than RAGAS. ARES averages a Kendall\u2019s _\u03c4 0.065 higher_\n_for context relevance_ and _0.132 higher for answer_\n_relevance than RAGAS_ . Additionally, the LLMjudge is substantially more accurate than RAGAS\nat predicting context relevance and answer relevance of a query-passage-answer triple. For context relevance, ARES with a fine-tuned LLM-judge\nis _59.9 percentage points higher than RAGAS_ while\nfor answer relevance, our system is _14.4 percent-_\n_age points higher than RAGAS_ . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompting and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall\u2019s _\u03c4_ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall\u2019s _\u03c4_ for ARES is\n0.08 higher"]},"response":{"0":"The authors of the paper \"Dense Passage Retrieval for Open-Domain Question Answering\" are Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih <document id=\"2004.04906_dpr_karpukhin:chunk_0\">.","1":"The margin value used in triplet loss is set to 1 <document id=\"2004.04906_dpr_karpukhin:chunk_13500\">.","2":"The potential reasons for lower performance on SQuAD can be summarized as follows:\n\n1. **High Lexical Overlap**: The annotators wrote questions after viewing the passages, resulting in a significant lexical overlap between the passages and questions. This gives traditional retrieval methods like BM25 a clear advantage over Dense Passage Retrieval (DPR) on SQuAD <document id=\"2004.04906_dpr_karpukhin:chunk_5400\">.\n\n2. **Limited Training Data**: SQuAD is derived from a small set of over 500 Wikipedia articles, which introduces a bias in the distribution of training examples. This limited scope can adversely affect the retrieval performance compared to other datasets <document id=\"2004.04906_dpr_karpukhin:chunk_5400\">. \n\nThese factors contribute significantly to DPR's underperformance relative to BM25 in the context of the SQuAD dataset <document id=\"2104.05740_hybrid_retrieval_ma:chunk_4950\">.","3":"In the context of information retrieval, a corpus refers to a structured set of documents or data that can be searched or analyzed for information. It serves as a source of knowledge that retrieval systems can query against to find relevant information. For example, one study mentions utilizing a Wikipedia dump as a retrieval corpus for answering questions in multi-document question answering tasks <document id=\"2307.03172_lost_in_middle_liu:chunk_13500\">. Additionally, the retrieval sources for information can come from training corpora, external datasets, or unsupervised data <document id=\"2202.01110_rag_survey_li:chunk_1350\">.","4":"I don't have enough information in the provided materials to answer this question.","5":"Antoine Bordes' work in machine learning primarily focuses on enhancing the capabilities of retrieval-augmented generation (RAG) models for open-domain question answering. His contributions involve integrating retrieval mechanisms with generative models, as demonstrated in the paper \"Reading Wikipedia to answer open-domain questions,\" where he collaborates with Danqi Chen and others in the exploration of how to leverage external knowledge sources effectively to improve question answering capabilities <document id=\"2007.01282_fid_izacard:chunk_4050\">. \n\nMoreover, Bordes has been involved in the development of frameworks that utilize dense retrieval techniques, which improve performance on question answering benchmarks, suggesting that effective retrieval can surpass traditional methods in these tasks <document id=\"2004.04906_dpr_karpukhin:chunk_10350\">. His research contributes significantly to the intersection of model learning and external knowledge integration, aiming to enhance the overall efficiency and accuracy of knowledge-intensive natural language processing tasks <document id=\"2005.11401_rag_lewis:chunk_14850\">.","6":"The main contribution of Facebook AI's research paper at ICLR 2021 is the introduction of a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions. This method achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Notably, it does not require access to corpus-specific information like inter-document hyperlinks or human-annotated entity markers, allowing it to be applied to any unstructured text corpus. Furthermore, it provides a superior efficiency-accuracy trade-off, achieving the best published accuracy on HotpotQA while being ten times faster at inference time compared to previous methods <document id=\"2009.12756_multihop_qi:chunk_0\">.","7":"On the HotpotQA dev set, the performance comparison between the BERT-wwm reader and the ELECTRA reader shows that the ELECTRA reader generally outperforms the BERT-wwm reader. Specifically, the results are as follows:\n\n- **ELECTRA Reader**:\n  - Top 50: EM 61.7, F1 74.3\n  - Top 250: EM 63.4, F1 76.2\n\n- **BERT-wwm Reader**:\n  - Top 250: EM 61.5, F1 74.7\n\nThe ELECTRA reader is reported to be 1-2% better than the BERT-wwm reader when sufficient passages are used (<document id=\"2009.12756_multihop_qi:chunk_6750\">).","8":"The purpose of HotpotQA in information retrieval is to evaluate and enhance the ability to answer multi-hop questions using documents from Wikipedia, as opposed to fixed knowledge bases. It includes a set of 113k multi-hop questions, enabling a broader range of topics and allowing for the assessment of intermediate retrieval performance through ground truth support passages provided with each question. This design encourages the development of models that can retrieve and process information from multiple documents effectively (<document id=\"2009.12756_multihop_qi:chunk_2700\">).","9":"The author of \"She Walks These Hills\" is Sharyn McCrumb, who was born on February 26, 1948 <document id=\"2009.12756_multihop_qi:chunk_13950\" metadata>.","10":"In the retrieval process described, downstream models serve several critical roles. Firstly, they are designed to process a limited number of retrieved passage sequences efficiently, aiming to enhance performance while ensuring that the retrieval steps remain manageable within real-world applications. Specifically, the retriever provides the top *k* scoring sequences from large text corpora, from which downstream models can derive relevant information necessary for answering complex questions effectively <2009.12756_multihop_qi:chunk_1350>.\n\nMoreover, downstream models can utilize information from previously retrieved documents to refine their queries iteratively. For example, they may generate new queries based on earlier retrieved results to obtain additional relevant information, thus supporting a multi-hop retrieval mechanism <2009.12756_multihop_qi:chunk_1350>.\n\nAdditionally, the integration of pre-trained language models (PLMs) is emphasized for learning term weights and enhancing retrieval robustness, especially in contexts where specialized domain knowledge is crucial <2312.10997_rag_survey_gao:chunk_8550>. Fine-tuning models can also be guided by feedback from language models to improve their performance by aligning retrieval outputs more closely with generative tasks <2312.10997_rag_survey_gao:chunk_8550>.\n\nOverall, downstream models play a crucial role in interpreting, refining, and leveraging information retrieved from the corpus to optimize the answering of complex queries.","11":"The conference paper that introduced WebQuestions is \"WebQuestions\" by Sergey Berant, John D. McCarthy, and others, published in 2013 <document id=\"2004.04906_dpr_karpukhin:chunk_4050\">.","12":"Structured state space models (SSMs) are a class of sequence models that relate to traditional recurrent neural networks (RNNs) and convolutional networks (CNNs), expanding upon classical state space frameworks. They are designed to address limitations of existing models, particularly in handling long sequences efficiently while maintaining strong performance, especially for tasks involving discrete data such as language modeling.\n\nOne significant advancement is the introduction of **selective state space models**, which improve upon earlier SSMs by introducing a mechanism that allows the SSM parameters to be dependent on the input. This design helps the model selectively propagate or forget information based on the current token, enhancing its ability to perform content-based reasoning, a known weakness in prior SSMs <document id=\"2312.00752_rag_roadmap_asai:chunk_0\">.\n\nAdditionally, structured SSMs eliminate the requirement for time-invariance, a core limitation of previous SSM architectures. By employing a hardware-aware algorithm for computation, they achieve linear scaling in sequence length while avoiding inefficiencies related to state materialization across varying memory levels in GPUs <document id=\"2312.00752_rag_roadmap_asai:chunk_28350\">. \n\nIn essence, structured state space models harness this selective mechanism to outperform traditional Transformers in terms of efficiency and versatility, particularly in contexts with lengthy inputs or where inference speed is critical <document id=\"2312.00752_rag_roadmap_asai:chunk_1800\">.","13":"Mamba demonstrates a strong capability in solving the induction heads task. Specifically, its selective SSM (Selective State Space Model) layer allows it to perfectly solve the task by selectively remembering relevant tokens while ignoring others. This selective attention capability is particularly notable as Mamba generalizes perfectly to sequences as long as 1 million tokens, which is significantly longer than what it encountered during training (4000 times longer) <document id=\"2312.00752_rag_roadmap_asai:chunk_10800\">.","14":"ARES differs from other retrieval-augmented generation (RAG) evaluation systems, such as Data ChatGPT and LLaMA Expansion SFT, primarily in its accuracy and efficiency in evaluating RAG systems. ARES requires 78% fewer annotations than traditional evaluation methods while still providing a more accurate ranking of systems, averaging a Kendall\u2019s tau 0.06 higher than a GPT-3.5 judge in terms of context and answer relevance <document id=\"2311.09476_ares_saad_falcon:chunk_5850\">. Additionally, ARES employs a fine-tuned LLM judge that can distinguish between RAG systems more precisely, thereby guiding decisions on document splitting, retriever selection, and generative LLM choice, which is a distinct advantage over standard approaches <document id=\"2311.09476_ares_saad_falcon:chunk_5850\">.","15":"I don't have enough information in the provided materials to answer this question.","16":"In the context of the Mamba model, the speed and memory benchmarks for the SSM scan operation are as follows:\n\n1. **Speed**: \n   - The efficient SSM scan is reported to be **20-40 times faster** than a standard scan implementation in PyTorch. \n   - Specifically, it is up to **7 times faster** than attention at sequence lengths of 32K and faster than the best attention implementation known (FlashAttention-2) beyond sequence length 2K (<document id=\"2312.00752_rag_roadmap_asai:chunk_31500\">).\n\n2. **Memory Usage**:\n   - Mamba\u2019s memory footprint, when compared to an optimized Transformer with FlashAttention-2, shows the following memory consumption benchmarks for different batch sizes:\n     - Batch size 1: Mamba uses **4.8GB**, Transformer uses **4.6GB**.\n     - Batch size 2: Mamba **5.8GB**, Transformer **5.2GB**.\n     - Batch size 4: Mamba **7.3GB**, Transformer **6.9GB**.\n     - Batch size 8: Mamba **12.3GB**, Transformer **11.5GB**.\n     - Batch size 16: Mamba **23.1GB**, Transformer **20.7GB**.\n     - Batch size 32: Mamba **38.2GB**, Transformer **34.5GB** (<document id=\"2312.00752_rag_roadmap_asai:chunk_39150\">).\n\nThese metrics highlight Mamba's efficiency in both speed and memory usage for SSM scan operations.","17":"FreshQA plays a crucial role in evaluating time-sensitive information as it uses up-to-date URLs that directly support answers to queries. This capability allows FreshQA to maintain a high percentage of sufficient context in its dataset, with about 77.4% of instances labeled as having sufficient context. This is significantly higher than both HotPotQA and Musique, which have only 46.2% and 44.6% respectively <document id=\"2411.06037_sufficient_context_joren:chunk_7200\">.\n\nIn comparison, HotPotQA is a Wikipedia-based QA dataset that features both single- and multi-hop questions. It has a lower sufficient context percentage because it contains questions that can often be answered without extensive context, leading to a different performance profile <document id=\"2411.06037_sufficient_context_joren:chunk_7650\">. Musique, on the other hand, includes multi-hop questions constructed from interconnected single-hop questions, but it also shows a low sufficient context ratio. Its context is made up of fixed supporting text snippets, which may not provide as robust context as FreshQA's curated URLs <document id=\"2411.06037_sufficient_context_joren:chunk_7650\">.\n\nOverall, FreshQA's design for handling time-sensitive inquiries and its substantial reliance on current, relevant context distinguish it from HotPotQA and Musique in the setting of retrieval-augmented generation systems.","18":"Dialogue generation plays a crucial role in expanding and improving language models like LLaMA by addressing the diversity and complexity of conversational responses. In the context of chit-chat dialogue systems, dialogue response generation is particularly challenging due to the \"one-to-many\" problem, where a single dialogue history can lead to numerous potential responses. Effective dialogue generation involves integrating external knowledge that may not be present in the dialogue history, preventing responses from being safe but boring (<document id=\"2202.01110_rag_survey_li:chunk_2700\">).\n\nModern dialogue systems can be divided into retrieval-based and generation-based models. While retrieval-based models offer informative and grammatically correct responses by copying from existing data, they struggle with novel dialogues. Conversely, generation-based models create new utterances, providing better generalization but often yielding less informative responses (<document id=\"2202.01110_rag_survey_li:chunk_3150\">).\n\nRecent advancements have focused on combining the strengths of both retrieval and generation models through approaches like shallow and deep integration, which enhance the quality of responses generated. Deep integration methods, for example, utilize retrieved \"skeletons\" to guide the generation process and improve coherence and relevance (<document id=\"2202.01110_rag_survey_li:chunk_900\">). \n\nThis synthesis of retrieval-augmented dialogue generation contributes to improving language models like LLaMA by enabling them to construct more contextually appropriate and diverse responses, ultimately enhancing their conversational abilities (<document id=\"2202.01110_rag_survey_li:chunk_4050\">).","19":"Open-domain QA models using dense retrieval techniques include models like Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and multi-hop dense retrieval models (Xiong et al., 2021). These models leverage dense passage retrieval methods that utilize learned embeddings to better capture semantic matching compared to traditional sparse IR methods such as TF-IDF and BM25, which primarily rely on term matches.\n\nStudies show that dense retrieval significantly outperforms traditional IR methods, achieving improvements of 9%-19% in top-20 passage retrieval accuracy on various datasets (Karpukhin et al., 2020). Dense methods typically employ large-scale pretraining or fine-tuning based on human-labeled data to create robust question and passage representations, positioning them advantageously for tasks requiring better semantic understanding and relevance (Xiong et al., 2020).\n\nIn contrast, traditional IR methods tend to focus on simple n-gram overlaps, which can miss nuanced semantic relations (Xiong et al., 2020). The introduction of dense retrieval has shown to enhance the capabilities of QA systems significantly, marking a shift from conventional methods to models that prioritize semantic comprehension and context over mere keyword matching (<document id=\"2004.04906_dpr_karpukhin:chunk_0\">, <document id=\"2009.12756_multihop_qi:chunk_8550\">).","20":"Selective State Space Models (SSMs) relate to linear attention approximations by representing a balance between expressivity and computational efficiency. In particular, linear attention methods, such as those generalizing S4, utilize recurrent mechanisms that can be interpreted as a degenerate form of SSMs. These models enhance the expressiveness of sequence processing while managing computational demands. \n\nSelective SSMs can utilize selections based on the input, allowing for an adaptable propagation of information across sequence lengths, which linear attention also seeks to achieve. This relationship highlights a shared goal of optimizing performance and handling long sequences effectively while maintaining fast inference capabilities (Gu & Dao, 2023) <document id=\"2312.00752_rag_roadmap_asai:chunk_3600\">.","21":"The generate-and-retrieve method is a strategy used in semantic parsing where predictions made during the generation phase are used to improve the retrieval process. This approach involves generating potential outputs that can guide the retrieval system in finding more relevant information or instances from a knowledge base <document id=\"2305.06983_active_rag_jiang:chunk_18000\" \/>. \n\nBy integrating generation with retrieval, the method enhances the information retrieval process, allowing for improved context and accuracy in generating answers. This synergistic approach ensures that the responses are informed by both generated and retrieved data, thereby enhancing the quality and relevance of the semantic parsing results <document id=\"2404.16130_graphrag_edge:chunk_21600\" \/>.","22":"I don't have enough information in the provided materials to answer this question.","23":"The Mamba model achieves significantly better performance benchmarks compared to other models. For instance, in terms of negative log-likelihood (NLL) and Fr\u00e9chet Inception Distance (FID), the Mamba model with 6.1 million parameters has an NLL of 1.852 and FID of 0.94, which is superior to models like SampleRNN and WaveNet. Even when comparing larger models, the 24.3 million parameter version of Mamba has an NLL of 1.860 and an even lower FID of 0.67, maintaining high metrics on inception scores and mean inception scores as well (<document id=\"2312.00752_rag_roadmap_asai:chunk_15750\">).\n\nIn terms of speed and memory, Mamba's efficient state space model (SSM) scan operation is reported to be 40 times faster than a standard implementation in PyTorch, and Mamba achieves 4-5 times higher inference throughput than a Transformer of similar size. This performance is attributed to the model's architecture, which allows for larger batch sizes without the key-value (KV) cache constraints present in Transformers (<document id=\"2312.00752_rag_roadmap_asai:chunk_16650\">).","24":"The process of **Data ChatGPT LLaMA Expansion SFT** involves enhancing large language models (LLMs) like LLaMA through supervised fine-tuning (SFT). This process typically incorporates diverse datasets that include human-written instructions and feedback to improve the model's ability to follow instructions accurately. In particular, the **LLaMA Expansion** aspect refers to expanding the training data with additional examples and adapting to various instruction forms. This fine-tuning approach helps the models akin to ChatGPT and LLaMA better understand and fulfill user queries.\n\nThe contribution to the **democratization of AI** lies in the accessibility of advanced LLMs that have been fine-tuned to perform well across a broad range of tasks. By leveraging SFT and incorporating feedback mechanisms, models become more adept at understanding user input in varied forms. This democratization makes high-performance AI technologies available to a wider audience, reducing barriers to entry for users without extensive technical knowledge or resources <document id=\"2402.13116_long_context_rag_xu:chunk_17550\">.","25":"Some examples of downstream models that have been expanded and fine-tuned using the Data ChatGPT LLaMA Expansion SFT method include:\n\n1. **Lion** (Jiang et al., 2023b)\n2. **BabyLlama** (Timiryasov and Tastet, 2023)\n3. **Vicuna** (Chiang et al., 2023)\n4. **Koala** (Geng et al., 2023)\n5. **Baize** (Xu et al., 2023b)\n6. **UltraChat** (Ding et al., 2023b)\n7. **Gorilla** (Patil et al., 2023)\n8. **WizardMath** (Luo et al., 2023b)\n\nThese models have utilized the LLaMA Labeling and Expansion process as part of their fine-tuning efforts <document id=\"2402.13116_long_context_rag_xu:chunk_17550\"\/>.","26":"RAFT, or Retrieval-Augmented Fine Tuning, is a training methodology designed to enhance the performance of pre-trained Large Language Models (LLMs) in domain-specific Retrieval-Augmented Generation (RAG) tasks. The core idea of RAFT is to instruct the model to ignore extraneous information (distractor documents) while effectively utilizing relevant documents to answer questions.\n\nKey features of RAFT include:\n\n1. **Distraction Management**: During training, RAFT encourages the model to pinpoint and cite only the useful portions of relevant documents, disregarding those that do not contribute to answering the question. This process helps the model to focus on pertinent information, which is crucial for effective reasoning in RAG scenarios (<document id=\"2403.10131_raft_zhang:chunk_0\">).\n\n2. **Performance Improvements**: RAFT has demonstrated significant performance gains on various datasets, like HotpotQA and PubMed. For instance, it has shown up to a 35.25% improvement on Hotpot QA compared to its baselines and outperformed other models, including GPT-3.5, in specific tasks (<document id=\"2403.10131_raft_zhang:chunk_4500\">).\n\n3. **Chain-of-Thought Style**: The model is also trained to respond in a chain-of-thought style, which facilitates better reasoning capabilities. This approach helps the model to construct logical sequences in its responses, further enhancing its ability to solve complex questions (<document id=\"2403.10131_raft_zhang:chunk_0\">).\n\n4. **Robustness Against Irrelevant Text**: By incorporating distractor documents alongside relevant ones during training, RAFT improves the model\u2019s ability to sift through irrelevant information. This is especially important for ensuring high recall in retrieval tasks where irrelevant content can easily mislead the model (<document id=\"2403.10131_raft_zhang:chunk_6750\">).\n\nOverall, RAFT represents a significant advancement in adapting LLMs for specialized domain tasks, allowing them not only to retrieve but also to effectively process and utilize information from multiple sources.","27":"The Flare framework enhances information retrieval through recursive retrieval, which iteratively refines search queries based on previous results. This iterative process aims to improve the depth and relevance of search outcomes by creating a feedback loop, allowing the search to converge on the most pertinent information. Flare automates retrieval timing by monitoring the confidence of the generation process, activating retrieval when the probability of generated terms falls below a certain threshold. This targeted approach not only improves efficiency but also ensures that the retrieval aligns more closely with the evolving context of the generation process. Additionally, Flare's integration of recursive retrieval and adaptive retrieval techniques enables the system to autonomously decide when and what to retrieve, thereby increasing the relevance and quality of the generated responses <document id=\"2312.10997_rag_survey_gao:chunk_10800\"> and <document id=\"2305.06983_active_rag_jiang:chunk_9000\">.","28":"I don't have enough information in the provided materials to answer this question.","29":"An example of a multi-hop reasoning task in MMLU high-school Physics involves applying the Ideal Gas Law. In this instance, the question requires retrieving the first principle of the Ideal Gas Law via abstraction, which serves as a foundational concept guiding the reasoning process. The use of abstraction leads to a more structured approach to solving complex problems that necessitate detailed reasoning steps <document id=\"2310.06117_step_back_zheng:chunk_450\" \/>.","30":"In the context of open-domain question answering (QA), retrieval units refer to the text passages that are created from a corpus of documents. Specifically, the corpus is split into passages of equal lengths, which become the basic retrieval units. When a factoid question is posed, the system's task is to locate a relevant span of text (the answer) within one of these passages. In essence, each passage serves as a discrete unit from which potential answers can be drawn during the retrieval phase of the QA process <document id=\"2004.04906_dpr_karpukhin:chunk_1350\" \/>.","31":"Different contextual inputs affect the embeddings generated for documents in the TREC-Covid dataset significantly. According to the material, when varying the contextual inputs while keeping the document text constant, the model reshapes the embedding space, leading to distinct embeddings being produced for the same text. This is evidenced by the visualization of embeddings where documents with different contextual embeddings are represented in the same color, showing that while the embeddings change with differing contexts, they remain closer to each other than to embeddings of different documents <document id=\"2410.02525_contextual_embeddings_nussbaum:chunk_16650\">.\n\nFurthermore, the model performance improves with a larger number of contextual examples. It was observed that the model performs reasonably even with no context but reaches optimal performance when a full context window is provided. This reflects the model's ability to utilize the contextual information effectively <document id=\"2410.02525_contextual_embeddings_nussbaum:chunk_16650\">. The architecture employed incorporates neighboring document information during embedding, allowing for a more nuanced representation that is sensitive to the context in which a document resides <document id=\"2410.02525_contextual_embeddings_nussbaum:chunk_450\">.","32":"Wu et al. proposed a method to enhance the diversity and coverage of instruction-following knowledge extracted from teacher LLMs by prompting the teacher LLM to generate instructions corresponding to specific topics. This approach was applied to a powerful teacher LLM, textdavinci-003, as part of their efforts to distill high-quality data <document id=\"2402.13116_long_context_rag_xu:chunk_8550\" \/>.","33":"Xu et al. proposed the **Self-Reflective Retrieval-augmented Generation (SELF-RAG)** approach to enhance the performance of large language models (LLMs) in handling factual errors. This method involves training the LLM to reflect on its generation process while using on-demand retrieval. It employs two types of tokens\u2014**retrieval tokens**, which prompt the model to retrieve relevant passages, and **critique tokens**, which allow the model to evaluate and select the best output based on factual accuracy and quality. This self-reflection mechanism aims to improve factual accuracy without sacrificing the model's versatility, explicitly addressing the issue of retrieving and utilizing the most relevant and accurate information from external documents (Asai et al., 2023).","34":"The researchers evaluated the performance of their models using the following datasets: MuSiQue (Trivedi et al., 2022) and StrategyQA (Geva et al., 2021) <document id=\"2310.06117_step_back_zheng:chunk_6750\">.","35":"Anil et al. contributed to the field of large language models through their work on the \"STEP-BACK PROMPTING\" technique. This method enables large language models (LLMs) to perform abstractions that help derive high-level concepts and principles from examples containing specific details. By guiding reasoning through these abstractions, LLMs demonstrate significantly improved performance on various reasoning-intensive tasks.\n\nIn their experiments with models like PaLM-2L, GPT-4, and Llama2-70B, the authors observed substantial performance gains, such as a 27% improvement on TimeQA and 11% on MMLU for Physics and Chemistry tasks. Their research illustrates how using abstraction can enhance the reasoning capabilities of LLMs, thus addressing complex tasks that involve detailed information processing (<document id=\"2310.06117_step_back_zheng:chunk_0\">).","36":"In Yang et al.'s work, the authors use a specific qrel matrix for instantiating attributes, which is characterized by having a high number of documents that exceed 1000 queries for a given task. They selected a qrel matrix with 46 relevant documents to ensure the complexity was manageable while still being statistically significant. The reasoning is that the more interconnected the qrel matrix, which is dense with combinations, the harder it would be for models to represent effectively <2508.21038_embedding_limit_weller:chunk_8550>.","37":"I don't have enough information in the provided materials to answer this question.","38":"MiniCPM-V 2.6 and VisRAG-Ret show notable differences in retrieval efficiency and performance on text retrieval benchmarks.\n\n1. **Retrieval Efficiency**:\n   - VisRAG-Ret exhibits a significant decrease in total document processing time (58% reduction) compared to MiniCPM (OCR), despite requiring more time for document encoding. MiniCPM (OCR) has an offline latency of 312 ms per document, while VisRAG-Ret simplifies the process by bypassing the parsing stage, showing a total online latency for query processing nearly identical to MiniCPM (OCR) at 54 ms per query <document id=\"2410.10594_vision_rag_yu:chunk_25200\">.\n\n2. **Retrieval Performance on Benchmarks**:\n   - VisRAG-Ret demonstrates superior document retrieval capabilities across various datasets, consistently outperforming MiniCPM-V 2.6. For instance, it performs better in parsing and understanding multi-modality information compared to MiniCPM (OCR) and has shown significant performance improvements in benchmarks such as SciFact and NFCorpus <document id=\"2410.10594_vision_rag_yu:chunk_9450\">.\n\nOverall, while MiniCPM-V 2.6 enhances multi-modal input handling and OCR capabilities, VisRAG-Ret leverages its architecture to yield better performance in both retrieval efficiency and performance on various benchmarks.","39":"LLaMA (Large Language Model Meta AI) is significant in the field of advanced models for information retrieval and understanding due to its strong performance, which rivals or surpasses that of established models like GPT-3, while being accessible for scientific study. The largest variant, LLaMA-65B, has shown superior performance compared to popular fine-tuned variants such as Alpaca and Vicuna in preliminary tests, emphasizing its effectiveness without fine-tuning (McKenna et al. 2023).\n\nAdditionally, LLaMA's architecture supports the integration of external knowledge through methodologies like Retrieval-Augmented Generation (RAG), which is seen as a promising approach to improve the accuracy and reliability of responses generated by LLMs. This integration addresses challenges like factual hallucination and knowledge outdating, further solidifying LLaMA's relevance in the context of information retrieval and understanding (Chen et al. 2023). Thus, LLaMA's attributes contribute to its role as a powerful tool in advancing research and applications in natural language processing and related fields.","40":"The use of DeBERTa-v3-Large in evaluating Retrieval-Augmented Generation (RAG) systems enhances their accuracy significantly. As part of the ARES framework, DeBERTa-v3-Large is utilized as a fine-tuned LLM judge to rank RAG systems effectively. This model allows for evaluations that do not depend on external APIs and can operate solely with few-shot prompts, improving the efficiency and cost-effectiveness of the evaluation process.\n\nIn comparison to other models such as RAGAS and GPT-3.5, ARES with DeBERTa-v3-Large averaged higher accuracy metrics in ranking context relevance and answer relevance. Specifically, it outperformed RAGAS by 59.9 percentage points for context relevance and 14.4 percentage points for answer relevance across various datasets from KILT and SuperGLUE. This indicates that DeBERTa-v3-Large contributes significantly to the precision of evaluations by applying domain-adaptive techniques and employing the PPI (Performance Per Instance) feature to enhance prediction accuracy (<document id=\"2311.09476_ares_saad_falcon:chunk_5400\">)."},"reference":{"0":"The authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering' are Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.","1":"The margin value of the triplet loss is set to 1.","2":"The lower performance on SQuAD is due to two main reasons. First, annotators wrote questions after seeing the passage, leading to high lexical overlap between passages and questions, which gives BM25 an advantage. Second, the data was collected from only 500+ Wikipedia articles, resulting in a biased distribution of training examples.","3":"A corpus in the context of information retrieval refers to a large collection of diversified topics, which is used for training and evaluating algorithms, particularly in tasks like question answering.","4":"Yoav Goldberg contributed to information retrieval through his work on Multi-passage BERT and Break it down: A question understanding benchmark.","5":"Antoine Bordes worked on developing models for reading Wikipedia to answer open-domain questions.","6":"The main contribution of Facebook AI's research paper at ICLR 2021 is proposing a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, their method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Their system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.","7":"The BERT-wwm reader is 1-2% worse than the ELECTRA reader when using enough passages. However, ELECTRA still outperforms the results in (Asai et al., 2020) which also uses BERT-wwm for answer prediction.","8":"HotpotQA is used for developing advanced models to improve information retrieval and understanding.","9":"Sharyn McCrumb was born on February 26, 1948.","10":"Downstream models are used to process the sequence candidates retrieved by the retriever and determine which one has the desired qualities.","11":"WebQuestions was published as a conference paper at ICLR 2021.","12":"Structured state space models (SSMs) are a type of architecture developed to improve information retrieval and understanding. They address the limitations of Transformers by allowing content-based reasoning, selectively propagating or forgetting information along the sequence length dimension depending on the current token.","13":"Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the task perfectly because of its ability to selectively remember the relevant token while ignoring everything else in between.","14":"ARES, an Automated RAG Evaluation System, evaluates RAG systems along dimensions of context relevance, answer faithfulness, and answer relevance. It finetunes lightweight LM judges to assess the quality of individual RAG components and utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Data ChatGPT LLaMA Expansion SFT is not explicitly mentioned in the provided context.","15":"Temporalwiki is a lifelong benchmark for training and evaluating ever-evolving language models. It relates to the development of language models like LLaMA by providing a platform for continuous learning and improvement, ensuring that these models can adapt and evolve over time.","16":"The speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16) was faster than the best attention implementation that we know of (FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation in PyTorch. Mamba achieved 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the KV cache it can use much higher batch sizes.","17":"FreshQA evaluates time-sensitive information and has up-to-date URLs that should support an answer to the queries. It exhibits high sufficient context compared to HotPotQA and Musique, which have lower sufficient context even with 6000 tokens.","18":"Dialogue generation plays a crucial role in expanding and improving language models like LLaMA by providing labeled data for supervised fine-tuning (SFT). This process involves using human conversations to train models, enhancing their ability to understand and generate natural language responses.","19":"Open-domain QA models that use dense retrieval techniques typically employ dense passage retrieval methods, which better capture semantic matching beyond simple n-gram overlaps. These methods either conduct large-scale pretraining with self-supervised tasks close to the underlying question-passage matching or directly finetune pretrained masked language models using human-labeled question-passage pairs. On single-hop information-seeking QA datasets like NaturalQuestions or WebQuestions, these dense methods have achieved significant improvements over traditional IR methods. Prior to these model-based methods, Das et al. (2019) used RNN encoders for generating dense representations of questions and passages, involving an iterative retrieval process and reformulating the query representation based on reader model's hidden states. However, their method required an initial round of TF-IDF\/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Unlike Feldman & El-Yaniv (2019)'s bi-attentive reformulation component applied on token-level representations, our approach uses a straightforward query reformulation strategy by concatenating the original query and previous retrieval as inputs to the query encoder. Together with stronger pretrained encoders and more effective training methods, MDR doubles the accuracy of their system.","20":"Selective State Space Models (SSMs) are related to linear attention approximations such as SSMs through their ability to handle time-varying data efficiently. They can be viewed as an architecture with an SSM sandwiched by two gated connections, which allows for parallelizable computation paths and the use of simpler mechanisms like multi-head attention instead of convolutions.","21":"The generate-and-retrieve method involves using predictions to enhance retrieval for semantic parsing. This approach was introduced in a paper titled 'Generate-and-retrieve: Use your predictions to improve retrieval for semantic parsing' by Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit Shanghai, and Fei Sha. The method leverages iterative retrieval and generation techniques to expand the capabilities of existing models like ChatGPT and LLaMA for better performance in semantic parsing tasks.","22":"The fine-tuning approach used in Temporalwiki involves training and evaluating ever-evolving language models over time.","23":"The Mamba model achieved 4-5\u00d7 higher inference throughput than a Transformer of similar size, as it can use much higher batch sizes without the KV cache. It was faster than the best attention implementation that we know of (FlashAttention-2) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation in PyTorch.","24":"Data ChatGPT LLaMA Expansion SFT involves expanding a pre-trained model like LLaMA using a dataset, which then undergoes fine-tuning. This process contributes to the democratization of AI by making advanced capabilities more accessible to smaller entities and individual researchers.","25":"Some examples of downstream models that have been expanded and fine-tuned using the Data ChatGPT LLaMA Expansion SFT method include MiniLLM, Self-Align, Self-Rewarding, STaR, Llama-GPT4, Reflection-Tuning, Selective Reflection-Tuning, Vicuna, Koala, Baize, UltraChat, and Orca2.","26":"RAFT (Retrieval Augmented Fine Tuning) is a training recipe that improves the model's ability to answer questions in 'open-book' in-domain settings. In training RAFT, given a question and a set of retrieved documents, the model is trained to ignore distractor documents and cite verbatim the right sequence from relevant documents to help answer the question. This method consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets when used in domain-specific RAG tasks.","27":"The Flare framework utilizes recursive retrieval by iteratively refining search queries based on previous results, aiming to enhance the depth and relevance of search outcomes through a feedback loop. This process involves summarizing sections of a document or lengthy PDF before performing a retrieval based on this summary, followed by a secondary refinement within the document.","28":"Anil et al. faced challenges in developing their model, particularly with complex multi-step reasoning. They overcame these challenges by proposing STEP-BACK PROMPTING, which grounds reasoning in abstractions to reduce errors in intermediate steps.","29":"An example of a multi-hop reasoning task in MMLU high-school Physics is the original question about the pressure of an ideal gas when temperature and volume are changed. The correct answer requires applying the Ideal gas law twice, first to find the initial pressure and then to find the final pressure after the changes.","30":"Retrieval units in the context of open-domain QA refer to text passages of equal lengths used as basic units for information retrieval. These passages are split from documents and form a corpus, which is then used to answer factoid questions by finding relevant spans within these passages.","31":"Different contextual inputs reshape the embedding space, causing the model to produce different embeddings for the same text. Although the embeddings move in response to changing contextual inputs, they remain closer to each other than to different documents.","32":"Wu et al. proposed a method to enhance the diversity and coverage of instruction-following knowledge extracted from teacher LLMs by dynamically updating the initial pool of demonstrations with newly generated samples during expansion iterations.","33":"Xu et al. proposed a thought approach to break down complex problems, which involves using methods like Zhou et al. (2023a), Xu et al. (2023b), and Drozdov et al. (2023). These methods aim to enhance the performance of LLMs by providing relevant knowledge and addressing issues such as slow inference speed and delayed responses.","34":"The researchers used FreshQA and HotpotQA datasets to evaluate the performance of their models for Musique.","35":"Anil et al. contributed to the development of PaLM-2L, a large language model that successfully answers questions via step-back prompting and abstraction-and-reasoning schemes.","36":"The qrel matrix used in Yang et al.'s work for instantiating attributes has the highest number of documents for which all combinations would be just above 1000 queries for a top- _\ud835\udc58_ - f 2 (46 docs, since [\ufffd][46] 2 - is 1035, the smallest above 1k).","37":"Chunting Zhou made significant contributions to the field of information retrieval through his work on summarization with pointer-generator networks and in-context pretraining. His research has helped advance language models by retrieving from trillions of tokens, improving their performance on various tasks.","38":"MiniCPM-V 2.6 outperforms VisRAG-Ret in retrieval efficiency, demonstrating stronger document understanding capabilities. VisRAG-Ret requires more time for document encoding compared to MiniCPM (OCR), but it bypasses the time-consuming parsing stage required by MiniCPM (OCR). This leads to a 58% reduction in total document processing time for VisRAG-Ret. On text retrieval benchmarks, VisRAG-Ret performs well on subsets of the text retrieval benchmark BEIR.","39":"LLaMA, or Large Language Model Meta AI, is a significant advancement in the field of advanced models for information retrieval and understanding. It has been used in various expansions, fine-tuning methods, and applications such as data chat, labeling, and self-knowledge SFT to improve the performance of other models like ChatGPT and GPT-4.","40":"The use of DeBERTa-v3-Large as the pretrained basis for the judges in evaluating RAG systems contributes to their accuracy by providing a fine-tuned language model that enhances the system's ability to score context relevance and answer relevance. This leads to higher Kendall\u2019s Tau values, indicating successful pairwise comparisons across various performance gaps between RAG systems."},"answer_correctness":{"0":0.9976729155,"1":0.9854301497,"2":0.9334146225,"3":0.6117449802,"4":0.1891485866,"5":0.5466091475,"6":0.9399007706,"7":0.608574025,"8":0.4994601998,"9":0.7237899486,"10":0.3046322553,"11":0.2249167549,"12":0.35442706,"13":0.4850691172,"14":0.8556887414,"15":0.1763206349,"16":0.4468414312,"17":0.9868654565,"18":0.2879989545,"19":0.80568733,"20":0.6143304176,"21":0.82407215,"22":0.1821935731,"23":0.4154990044,"24":0.65674179,"25":0.5373888308,"26":0.5572889158,"27":0.929999899,"28":0.187646592,"29":0.3895530032,"30":0.9923238122,"31":0.6296650246,"32":0.5358628173,"33":0.3133789532,"34":0.4724196127,"35":0.7065729804,"36":0.4665593412,"37":0.1852179993,"38":0.5255593974,"39":0.4142201891,"40":0.8444390052},"context_precision":{"0":0.45,"1":0.5,"2":0.8666666666,"3":0.0,"4":0.0,"5":0.7,"6":0.7,"7":0.75,"8":1.0,"9":0.9999999999,"10":1.0,"11":0.0,"12":1.0,"13":1.0,"14":0.8666666666,"15":0.0,"16":0.95,"17":0.8875,"18":0.7,"19":1.0,"20":1.0,"21":0.8333333333,"22":0.0,"23":1.0,"24":1.0,"25":0.9999999999,"26":1.0,"27":0.8875,"28":0.0,"29":0.8875,"30":1.0,"31":1.0,"32":0.95,"33":0.9999999999,"34":0.3666666666,"35":0.8333333333,"36":1.0,"37":0.0,"38":1.0,"39":0.8666666666,"40":1.0},"context_recall":{"0":1.0,"1":1.0,"2":1.0,"3":1.0,"4":0.0,"5":1.0,"6":1.0,"7":1.0,"8":1.0,"9":1.0,"10":1.0,"11":0.0,"12":0.5,"13":1.0,"14":1.0,"15":0.0,"16":1.0,"17":1.0,"18":1.0,"19":1.0,"20":1.0,"21":0.6666666667,"22":0.0,"23":1.0,"24":0.5,"25":1.0,"26":1.0,"27":1.0,"28":0.0,"29":1.0,"30":1.0,"31":1.0,"32":1.0,"33":0.5,"34":1.0,"35":1.0,"36":1.0,"37":0.0,"38":0.75,"39":0.5,"40":1.0}}