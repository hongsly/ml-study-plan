[
  {
    "user_input": "Who are the authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering'?",
    "retrieved_contexts": [
      "## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar",
      "## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [",
      "QA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20\nparagraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these\ndatasets into an open-domain setting, we make two\ncorpora, one for each dataset, by combining all\nsupporting and non-supporting paragraphs for all\nits questions in the train, development, and test\nsets. IIRC is originally a mix between reading\ncomprehension and an open-domain setting. Each\nquestion is grounded in one main paragraph, which\ncontains links to multiple Wikipedia pages with\nseveral paragraphs each. We create a corpus out\n\n- f all the paragraphs from all the Wikipedia pages\npresent in the dataset. [18] We do assume the availability of the main passage which doesn\u2019t need\nto be retrieved and is always present. We don\u2019t\nassume the availability of Wikipedia links in the\nmain passage, however, to keep the retrieval problem challenging. [19]\n\n\n**B** **Special Handling of Models for IIRC**\n\n\nIIRC is slightly different from the other datasets,\nin that the question is grounded in the main passage and other supporting paragraphs come from\nthe Wikipedia pages of entities mentioned in this\npassage. We modify the retrievers and readers to\naccount for this difference: (i) We always keep the\nmain passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all\nthe retrieval methods, we first prompt the model to\ngenerate a list of Wikipedia page titles using the\nmain passage and the question. We map these generated titles to the nearest Wikipedia page titles in\nthe corpus (found using BM25), and then the rest\n\n- f the paragraph retrieval queries are scoped within\n\n- nly those Wikipedia pages.\nTo prompt the model to generate Wikipedia page\ntitles using the main passage and the question for\n\n\n18Following are the corpus sizes for the datasets: HotpotQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e",
      " for GPT-2 [50] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against\nmisleading content and automated spam/phishing.\n\n\n**Acknowledgments**\n\n\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\n\nprogram.\n\n\n**References**\n\n\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. _arXiv:1611.09268 [cs]_, November 2016. URL `[http:](http://arxiv.org/abs/1611.09268)`\n`[//arxiv.org/abs/1611.09268](http://arxiv.org/abs/1611.09268)` . arXiv: 1611.09268.\n\n\n[2] Petr Baudi\u0161 and Jan \u0160edivy. Modeling of the question answering task in the yodaqa system. In`\n_International Conference of the Cross-Language Evaluation Forum for European Languages_,\npages 222\u2013228. Springer, 2015. URL `[https://link.springer.com/chapter/10.1007%](https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20)`\n`[2F978-3-319-24027-5_20](https://link.springer.com/chapter/",
      " time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine"
    ],
    "response": "The authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering' are Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih <document id=\"2004.04906_dpr_karpukhin:chunk_0\" />.",
    "reference": "The authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering' are Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.",
    "answer_correctness": 0.9978550777,
    "context_precision": 0.5,
    "context_recall": 1.0
  },
  {
    "user_input": "What is the margin value used in triplet loss?",
    "retrieved_contexts": [
      "A** **Distant Supervision**\n\n\nWhen training our final DPR model using Natural\nQuestions, we use the passages in our collection\nthat best match the gold context as the positive\npassages. As some QA datasets contain only the\nquestion and answer pairs, it is thus interesting\nto see when using the passages that contain the\nanswers as positives (i.e., the distant supervision\nsetting), whether there is a significant performance\ndegradation. Using the question and answer together as the query, we run Lucene-BM25 and pick\nthe top passage that contains the answer as the positive passage. Table 5 shows the performance of\nDPR when trained using the original setting and\nthe distant supervision setting.\n\n\n**B** **Alternative Similarity Functions &**\n**Triplet Loss**\n\n\nIn addition to dot product (DP) and negative loglikelihood based on softmax (NLL), we also experiment with Euclidean distance (L2) and the triplet\nloss. We negate L2 similarity scores before applying softmax and change signs of question-topositive and question-to-negative similarities when\napplying the triplet loss on dot product scores. The\nmargin value of the triplet loss is set to 1. Table 6 summarizes the results. All these additional\n\nexperiments are conducted using the same hyperparameters tuned for the baseline (DP, NLL).\nNote that the retrieval accuracy for our \u201cbaseline\u201d\nsettings reported in Table 5 (Gold) and Table 6\n(DP, NLL) is slightly better than those reported in\nTable 3. This is due to a better hyper-parameter\nsetting used in these analysis experiments, which\nis documented in our code release.\n\n\n**C** **Qualitative Analysis**\n\n\nAlthough DPR performs better than BM25 in general, the retrieved passages of these two retrievers\nactually differ qualitatively. Methods like BM25\nare sensitive to highly selective keywords and\nphrases, but cannot capture lexical variations or semantic relationships well. In contrast, DPR excels\nat semantic representation, but might lack sufficient\ncapacity to represent salient phrases which appear\nrarely. Table 7 illustrates this phenomenon with\ntwo examples. In the first example, the top scoring passage from BM25 is irrelevant, even though\nkeywords such as _England_ and _Ireland_ appear multiple times. In comparison, DPR is able to return\n\n\n\n**Top-1** **Top-5** **Top-20",
      " top scoring passage from BM25 is irrelevant, even though\nkeywords such as _England_ and _Ireland_ appear multiple times. In comparison, DPR is able to return\n\n\n\n**Top-1** **Top-5** **Top-20** **Top-100**\n\n\nGold 44.9 66.8 78.1 85.0\n\nDist. Sup. 43.9 65.3 77.1 84.4\n\n\nTable 5: Retrieval accuracy on the development set of\nNatural Questions, trained on passages that match the\ngold context (Gold) or the top BM25 passage that contains the answer (Dist. Sup.).\n\n\n**Sim Loss** **Retrieval Accuracy**\nTop-1 Top-5 Top-20 Top-100\n\n\nNLL **44.9** **66.8** **78.1** **85.0**\nDP\nTriplet 41.6 65.0 77.2 84.5\n\n\nNLL 43.5 64.7 76.1 83.1\nL2\nTriplet 42.2 66.0 **78.1** 84.9\n\n\nTable 6: Retrieval Top- _k_ accuracy on the development\nset of Natural Questions using different similarity and\nloss functions.\n\n\nthe correct answer, presumably by matching _\u201cbody_\n\n_of water\u201d_ with semantic neighbors such as _sea_ and\n_channel_, even though no lexical overlap exists. The\nsecond example is one where BM25 does better.\nThe salient phrase _\u201cThoros of Myr\u201d_ is critical, and\nDPR is unable to capture it.\n\n\n**D** **Joint Training of Retriever and**\n**Reader**\n\n\nWe fix the passage encoder in our joint-training\nscheme while allowing only the question encoder\nto receive backpropagation signal from the combined (retriever + reader) loss function. This allows\nus to leverage the HNSW-based FAISS index for\nefficient low-latency retrieving, without reindexing\nthe passages during model updates. Our loss function largely follows ORQA\u2019s approach, which uses\nlog probabilities of positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically",
      "N: number of negative\nexamples, IB: in-batch training. G.+BM25 [(1)] and\nG.+BM25 [(2)] denote in-batch training with 1 or 2 additional BM25 negatives, which serve as negative passages for all questions in the batch.\n\n\nOur experiments on Natural Questions show that\nswitching to distantly-supervised passages (using\nthe highest-ranked BM25 passage that contains the\nanswer), has only a small impact: 1 point lower\ntop- _k_ accuracy for retrieval. Appendix A contains\nmore details.\n\n\n**Similarity and loss** Besides dot product, cosine\nand Euclidean L2 distance are also commonly used\nas decomposable similarity functions. We test these\nalternatives and find that L2 performs comparable to dot product, and both of them are superior\nto cosine. Similarly, in addition to negative loglikelihood, a popular option for ranking is triplet\nloss, which compares a positive passage and a negative one directly with respect to a question (Burges\net al., 2005). Our experiments show that using\ntriplet loss does not affect the results much. More\ndetails can be found in Appendix B.\n\n\n**Cross-dataset generalization** One interesting\nquestion regarding DPR\u2019s discriminative training\nis how much performance degradation it may suffer from a non-iid setting. In other words, can\nit still generalize well when directly applied to\na different dataset without additional fine-tuning?\nTo test the cross-dataset generalization, we train\nDPR on Natural Questions only and test it directly\n\n- n the smaller WebQuestions and CuratedTREC\ndatasets. We find that DPR generalizes well, with\n3-5 points loss from the best performing fine-tuned\nmodel in top-20 retrieval accuracy (69.9/86.3 vs.\n75.0/89.1 for WebQuestions and TREC, respectively), while still greatly outperforming the BM25\nbaseline (55.0/70.9).\n\n\n**5.3** **Qualitative Analysis**\n\n\nAlthough DPR performs better than BM25 in general, passages retrieved by these two methods differ qualitatively. Term-matching methods like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason",
      " 48.4 _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nReAct _\u2212_ | _\u2212_ 35.1 | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nSelfAsk _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ 40.1 | _\u2212_ 15.2 | _\u2212_\n\nDecomP _\u2212_ | 50.0 _\u2212_ | _\u2212_ _\u2212_ | 59.3 _\u2212_ | _\u2212_\n\n\nIRCoT QA **45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8**\n\n\nTable 1: Comparison with other LLM-based ODQA\nsystems on EM and F1 scores. \u2018 _\u2212_ \u2019: score is unavailable. HpQA [Br] : Bridge questions subset of HotpotQA.\nMQ [2H] : MuSiQue 2-hop questions. IRCoT QA with\nGPT3 (ours) outperforms other systems by a large margin. Note: Comparisons aren\u2019t head-to-head as discussed in the text. App. \u00a7C reports updated SOTA numbers, including contemporaneous and newer works.\n\n\nAs shown in Table 1, IRCoT QA significantly\n\n- utperforms all of these recent systems by a large\nmargin, setting a new state of the art in terms of\nwhat\u2019s achievable via retrieval-augmented LLMs\n(without supervised training).\n\n\n**6** **Conclusions**\n\n\nChain-of-thought prompting has significantly improved LLMs\u2019 ability to perform multi-step reason\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nThe tap dancing\nsoloist in My\nFeet Keep\nDancing also\nstarred in what\n\n1935 MGM\n\nmusical?\n\n\nWho is the\n\nfather-in-law\n\n- f Queen Hyojeong?\n\n\nWhat is the\n\nname - f the\n\ncastle in the\ncity where the\nperformer - f\nA Collection\n\n1984\u20131989 was\n\nborn?\n\n\n\n_\u21d2_ The tap dancing\nsoloist in My Feet\nKeep Dancing is Fayard\nNicholas.\n_\u21d2_ Fayard Nicholas\nstarred in the 1935\n\nM",
      " reaction is\nChemistry spontaneous?\n0) at equilibrium there are more products than reactants\n1) the value of \u2206 _G_ [0] is greater than zero\n2) the value of \u2206 _S_ [0] is greater than zero\n3) the value of Keq is less than one\n**Original Answer [Incorrect]** : 2) the value of \u2206S- is greater than zero.\nA reaction is spontaneous if the value of \u2206S- is greater than zero. This is\nbecause a positive \u2206S- indicates that the reaction results in an increase in\nentropy, which is a measure of disorder. A spontaneous reaction is one that\n\n          - ccurs without the input of energy, and an increase in entropy is a natural\ntendency of the universe.\n**Stepback Question** : What are the chemistry principles behind this question?\n**Principles** :\nSpontaneous reactions: A spontaneous reaction is a reaction that occurs without\nthe input of energy.\n**Final Answer** : A reaction is spontaneous if the change in Gibbs free energy is\nnegative (\u2206 _G <_ 0). The value of \u2206 _G_ [0] is greater than zero indicates that the\nreaction is not spontaneous.\n\n\nThe correct answer is 0) at equilibrium there are more products than reactants.\n\n\nMMLU- **Original Question** : Potassium-40 is a minor isotope found in naturally ocChemistry curring potassium. It is radioactive and can be detected on simple radiation\ncounters. How many protons, neutrons, and electrons does potassium-40 have\nwhen it is part of K2SO4?\n0) 21 neutrons, 19 protons, 18 electrons\n1) 20 neutrons, 19 protons, 19 electrons\n2) 21 neutrons, 19 protons, 19 electrons\n3) 19 neutrons, 19 protons, 19 electrons\n**Original Answer [Incorrect]** : The correct answer is 2).\nPotassium-40 has 19 protons and 21 neutrons. When it is part of K2SO4, it has\n19 electrons.\n\n**Stepback Question** : What are the chemistry principles behind this question?\n**Principles** :\nAtomic number: The atomic number of an element is the number of protons in\nthe nucleus of an atom of that element"
    ],
    "response": "The margin value used in triplet loss is set to 1 <document id=\"2004.04906_dpr_karpukhin:chunk_13500\">.",
    "reference": "The margin value of the triplet loss is set to 1.",
    "answer_correctness": 0.9854301497,
    "context_precision": 0.9999999999,
    "context_recall": 1.0
  },
  {
    "user_input": "What are the potential reasons for lower performance on SQuAD?",
    "retrieved_contexts": [
      " BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25( _q_, _p_ ) + _\u03bb \u00b7_ sim( _q, p_ ) as the ranking function.\nWe used _\u03bb_ = 1 _._ 1 based on the retrieval accuracy in\nthe development set.\n\n\n**5.1** **Main Results**\n\n\nTable 2 compares different passage retrieval systems on five QA datasets, using the top- _k_ accuracy\n( _k \u2208{_ 20 _,_ 100 _}_ ). With the exception of SQuAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when _k_ is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\n\n- n Natural Questions). When training with mul\n\n8SQuAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\n\nmore in Section 5.1.\n\n9\n[Lucene implementation. BM25 parameters](https://lucene.apache.org/) _b_ = 0 _._ 4 (document length normalization) and _k_ 1 = 0 _._ 9 (term frequency\nscaling) are tuned using development sets.\n\n\n\nFigure 1: Retriever top- _k_ accuracy with different numbers of training examples used in our dense passage retriever vs BM25. The results are measured on the de\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n\n\ntiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and",
      "-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\n\n- nly 500+ Wikipedia articles and thus the distribution of training examples is extremely biased, as\nargued previously by Lee et al. (2019).\n\n\n**5.2** **Ablation Study on Model Training**\n\n\nTo understand further how different model training\n\n- ptions affect the results, we conduct several additional experiments and discuss our findings below.\n\n\n\n\n\n\n**Sample efficiency** We explore how many training examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top- _k_\nretrieval accuracy with respect to different numbers of training examples, measured on the devel\n- pment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 examples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question\u2013passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently.\n\n\n**In-batch negative training** We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3.\nThe top block is the standard 1-of- _N_ training setting, where each question in the batch is paired\nwith a positive passage and its own set of _n_ negative passages (Eq. (2)). We find that the choice\n\n- f negatives \u2014 random, BM25 or gold passages\n(positive passages from other questions) \u2014 does\nnot impact the top- _k_ accuracy much in this setting\nwhen _k \u2265_ 20.\n\nThe middle bock is the in-batch negative training\n(Section 3.2) setting. We find that using a similar\nconfiguration (7 gold negative passages), in-batch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\n\n- r from the whole training set. Effectively, in-batch\nnegative training is an easy and memory",
      " train the model for 10k gradient steps,\nwith a batch size of 64, using 64 Tesla V100 32Gb.\nWe evaluate models every 500 steps and select the\nbest one on the validation set based on the Exact\n\nMatch score. During training on NaturalQuestions\n\n\n4github.com/huggingface/transformers\n\n\n\nand SQuAD, we sample the target among the list\n\n- f answers, while for TriviaQA, we use the unique\nhuman-generated answer. For TriviaQA, answers\nin uppercase are normalized by converting all letters in lowercase except the first letter of each word,\nusing the title Python string method. For both\ntraining and testing, we retrieve 100 passages (unless said otherwise), and truncate them to 250 word\npieces. Following the results of Karpukhin et al.\n(2020), passages are retrieved with DPR for NQ\nand TriviaQA, and with BM25 for SQuAD. We\ngenerate answers by using greedy decoding.\n\n\n**Comparison to state-of-the-art.** In table 1, we\ncompare the results obtained by Fusion-in-Decoder\nwith existing approaches for open domain question answering. We observe that while conceptually simple, this method outperforms existing work\n\n- n the NaturalQuestion and TriviaQA benchmarks.\nIn particular, generative models seem to perform\nwell when evidence from multiple passages need to\nbe aggregated, compared to extractive approaches.\nOur method also performs better than other generative models, showing that scaling to large number\n\n- f passages and processing them jointly leads to\nimprovement in accuracy. Second, we observe that\nusing additional knowledge in generative models\nby using retrieval lead to important performance\ngains. On NaturalQuestions, the _closed book_ T5\nmodel obtains 36.6% accuracy with 11B parameters, while our approach obtains 44.1% with 770M\nparameters plus Wikipedia with BM25 retrieval.\nBoth methods use roughly the same amount of\nmemory to store information, indicating that text\nbased explicit memories are competitive for knowledge retrieval tasks.\n\n\n**Scaling with number of passages.** In Figure 3,\nwe report the performance with respect to the\n\n\nNaturalQuestions TriviaQA\nTraining Passages w/o finetuning w/ finetuning w/o finetuning w/ finetuning\n\n\n5 37.8 45.0 58.1 64.",
      "Ab|rrect<br>Ab|sta|in<br>Hal|in<br>Hal|rect<br>Abst|rect<br>Abst|ain<br>Hall|\n\n\n\nFigure 3: **Model Performance on Datasets Stratified by Sufficient Context.** Given sufficient\ncontext, models have a higher correct percentage on these challenging datasets. Performance drops,\nbut the models are still able to answer a large portion of questions correct without sufficient context.\nOne prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient\ncontext. The smallest model Gemma 27B struggles to avoid hallucinations given insufficient context.\n\n\nappropriate. Without RAG, Claude 3.5 Sonnet abstains on 84.1% questions, while with RAG, the\nfraction of abstentions drops to 52%. Similarly, GPT 4o\u2019s abstention fraction moves from 34.4%\nto 31.2% and Gemini 1.5 Pro\u2019s drops from 100% to 18.6%. This phenomenon may arise from the\nmodel\u2019s increased confidence in the presence of any contextual information, leading to a higher\npropensity for hallucination rather than abstention.\n\n\n**Models Hallucinate with Both Sufficient and Insufficient Context.** Considering Figure 3, models\ngenerally achieve higher accuracy with sufficient context (higher **green bars**, top row) than without\nsufficient context (lower **green bars**, bottom row). However, looking at each row separately, we\ndiscover several findings. First, in the sufficient context case (top row), we see that models hallucinate\nmore than they abstain ( **red bars** are higher than **blue bars**, usually). The trend holds across all\nthree datasets. Moving to insufficient context (bottom row), we find a different distribution of model\nresponses, with more abstentions and hallucinations. This tendency varies notably across different\nmodels. For instance, Claude abstains more (higher **blue bars** ) with insufficient context, but answers\nfewer questions correctly (lower **green bars** ) than Gemini and GPT. These differences underscore\nthe potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\nmuch more hallucinations (higher **red bars** ) than the other models, except for HotPotQA, where we\nattribute the higher accuracy to the smaller retrieved contexts.\n\n\n4.3 QUALITATIVELY ANALYZING",
      "ING on challenging Multi-Hop reasoning benchmark MuSiQue\n(Trivedi et al., 2022) and StrategyQA (Geva et al., 2021). We follow the same protocol as Section 5\nto implement STEP-BACK PROMPTING.\n\n\nTable 3 shows performance of various baselines on the dev set of MuSiQue and StrategyQA.\nBaseline performance of PaLM-2L and GPT-4 are low (35 _._ 5% and 38 _._ 5% for PaLM-2L and GPT-4\nrespectively) in MuSiQue since it is a hard multihop reasoning benchmark. In contrast, StrategyQA\nhas stronger baselines (82 _._ 8% and 78 _._ 3% for PaLM-2L and GPT-4 respectively) probably because\nit is a binary classification task. CoT and TDB improve model performance a bit in the case of\n\n_\u223c_\nMuSiQue ( 3% and 3.5% respectively) which can be attributed to the inherent reasoning nature\n\n- f this task where these methods are shown to be helpful. In the case of StrategyQA, there is no\nsignificant performance gain with CoT and TDB which could be due to the high baseline performance\nin this task, with limited scope for these prompting methods to improve performance. Often, 1-shot\nperformance is significantly lower than their zero-shot methods, which could be attributed to potential\nexample bias (Zhao et al., 2021; Parmar et al., 2023). RAG improves model performance ( _\u223c_ 4%\nand 2% for MuSiQue and StrategyQA respectively.) STEP-BACK PROMPTING with the power\n\n- f abstraction produces the best performance of all methods: 42 _._ 8% in MuSiQue and 86 _._ 4% in\nStrategyQA, significantly outperforming GPT-4 on both tasks. We present a detailed error analysis\n\n- n StrategyQA in Appendix A.3.\n\n\n7 DISCUSSION\n\n\nAbstraction helps humans to solve complex tasks by removing irrelevant details and distilling highlevel concepts and principles to guide the problem-solving process. STEP-BACK PROMPTING breaks\n\n\n8\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ncomplex tasks such as knowledge-intensive QA, multi-hop reasoning,"
    ],
    "response": "The potential reasons for lower performance on SQuAD include:\n\n1. **High Lexical Overlap**: The questions in SQuAD were written after the annotators saw the passages, which led to a high degree of lexical overlap between the questions and the passages. This overlapping vocabulary gives a clear advantage to the BM25 retrieval method over others like Dense Passage Retrieval (DPR) <document id=\"2004.04906_dpr_karpukhin:chunk_5850\">.\n\n2. **Limited Document Scope**: The dataset was collected from only about 500 Wikipedia articles. This limited source results in a biased distribution of training examples, which negatively impacts the model\u2019s performance <document id=\"2004.04906_dpr_karpukhin:chunk_5850\">.",
    "reference": "The lower performance on SQuAD is due to two main reasons. First, annotators wrote questions after seeing the passage, leading to high lexical overlap between passages and questions, which gives BM25 an advantage. Second, the data was collected from only 500+ Wikipedia articles, resulting in a biased distribution of training examples.",
    "answer_correctness": 0.987366084,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What is a corpus in the context of information retrieval?",
    "retrieved_contexts": [
      ") in a single context window.\n\n\n    - _Map community answers_ . Intermediate answers are generated in parallel. The LLM is also\nasked to generate a score between 0-100 indicating how helpful the generated answer is in\nanswering the target question. Answers with score 0 are filtered out.\n\n\n    - _Reduce to global answer_ . Intermediate community answers are sorted in descending order\n\n     - f helpfulness score and iteratively added into a new context window until the token limit\nis reached. This final context is used to generate the global answer returned to the user.\n\n\n**3.2** **Global Sensemaking Question Generation**\n\n\nTo evaluate the effectiveness of RAG systems for global sensemaking tasks, we use an LLM to\ngenerate a set of corpus-specific questions designed to asses high-level understanding of a given\ncorpus, without requiring retrieval of specific low-level facts. Instead, given a high-level description\n\n- f a corpus and its purposes, the LLM is prompted to generate personas of hypothetical users of\nthe RAG system. For each hypothetical user, the LLM is then prompted to specify tasks that this\nuser would use the RAG system to complete. Finally, for each combination of user and task, the\nLLM is prompted to generate questions that require understanding of the entire corpus. Algorithm\n1 describes the approach.\n\n\n6\n\n\n**Algorithm 1: Prompting Procedure for Question Generation**\n\n\n1: **Input:** Description of a corpus, number of users _K_, number of tasks per user _N_, number of\nquestions per (user, task) combination _M_ .\n2: **Output:** A set of _K \u2217_ _N \u2217_ _M_ high-level questions requiring global understanding of the corpus.\n3: **procedure** GENERATEQUESTIONS\n4: Based on the corpus description, prompt the LLM to:\n\n1. Describe personas of _K_ potential users of the dataset.\n\n2. For each user, identify _N_ tasks relevant to the user.\n\n3. Specific to each user & task pair, generate _M_ high-level questions that:\n\n\n      - Require understanding of the entire corpus.\n\n      - Do not require retrieval of specific low-level facts.\n\n\n5: Collect the generated questions to produce _K \u2217_ _N \u2217_ _M_ test questions for the dataset.\n6: **end procedure**\n\n\nFor our evaluation, we set _K_",
      " Transformers for longer sequences. In\n_Proc. of NeurIPS_ .\n\n\n**A** **Ambiguity in Multi-Document QA**\n**Distractor Documents**\n\n\nFollowing past work on NaturalQuestions-Open\n(Izacard et al., 2021; Izacard and Grave, 2021, _inter_\n_alia_ ), we use a Wikipedia dump from late 2018\nas our retrieval corpus. However, this standard\nWikipedia dump has a small amount of temporal\nmismatch with the NaturalQuestions annotations.\n\nFor example, consider the question \u201cwhat nfl\nteam does robert griffin iii play for\u201d. The NaturalQuestions annotated answer is \u201ccurrently a free\nagent\u201d. However, the Wikipedia retrieval corpus\ncontains the information that he plays for the \u201cBaltimore Ravens\u201d, since he was released from the team\nbetween the Wikipedia dump\u2019s timestamp and the\nNaturalQuestions annotation process.\nWe use the ambiguity annotations of Min et al.\n(2020) to create a subset unambiguous questions.\nExperiments on this unambiguous subset of the\ndata show similar results and conclusions as the\n\nexperiments on the full questions collection (Figure 12).\n\n\n|(~4K tokens unambi<br>,|Col2|Col3|Col4|guousques|Col6|tions)|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\nFigure 12: Language model performance on a unambiguous subset of questions.\n\n\n**B** **Random Distractors in**\n\n**Multi-Document QA**\n\n\nWe also run multi-document question answering\nexperiments with random Wikipedia documents as\ndistractors, which allows us to ablate the impact\n\n- f retrieved distractors (hard negatives). Note that\nin this setting, the the document containing the answer can often be identified with simple heuristics\n(e.g., lexical overlap with the query). Figure 13\npresents the results of this experiment. Although\nall models have higher absolute accuracy in this\nsetting, they surprisingly still struggle to reason\n\n- ver their entire input context, indicating that their\nperformance degradation is not solely due to an\ninability to identify relevant documents.\n\n\n**C** **Randomizing Distractor Order in**\n**Multi-Document QA**\n\n\nOur prompt instructs the language model to use\nthe provided search results to answer the question.\nThere may",
      " time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine",
      " focus on her initial transformation. The difference in retrieved information\n\n\n19\n\n\nPublished as a conference paper at ICLR 2024\n\n\nsignificantly impacts downstream tasks. When GPT-4 is provided with RAPTOR\u2019s context, it generates a detailed answer: \u201cCinderella finds a happy ending when the Prince searches for the owner\n\n- f the lost glass slipper and discovers it belongs to Cinderella. They eventually marry, transforming Cinderella\u2019s life for the better.\u201d In contrast, using DPR\u2019s context, GPT-4 states: \u201cBased on the\ngiven context, it is not possible to determine how Cinderella finds a happy ending, as the text lacks\ninformation about the story\u2019s conclusion.\u201d\n\n\nThe second question we examine is \u201cWhat is the central theme of the story?\u201d, a thematic question\nthat requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for\nthis question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of\nall the major parts of the story, whereas the text retrieved by DPR contains detailed descriptions of\na narrow subset of the story. Again, the difference in retrieval mechanisms affects the performance\n\n- f GPT-4 when answering the question. Given DPR\u2019s context, it outputs \u201cThe central theme of\nthe story is transformation and the power of inner beauty, as Cinderella, a kind and humble girl, is\nmagically transformed into a beautiful princess, capturing the attention and admiration of the Prince\nand others at the ball.\u201d This answer only takes into account the first portion of the story, up until\nCinderella first meets the prince. In contrast, given RAPTOR\u2019s context, GPT-4 outputs \u201cThe central\ntheme of the story is transformation and overcoming adversity, as Cinderella, with the help of her\nFairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident\nyoung woman who ultimately finds happiness and love with the Prince.\u201d This is a more complete\nanswer, demonstrating a comprehensive understanding of the story.\n\n\nThis qualitative analysis indicates that RAPTOR outperforms prior retrieval mechanisms because\nthe information that it retrieves is more relevant and exhaustive, allowing for better performance on\ndownstream tasks.\n\n\nWe also created a 2600-word story along with questions about its narrative and theme. An excerpt\n[from the story is present below and the full PDF of this story is linked here",
      " extreme, we could allow the\nencoder full access to the entire corpus, through some form of cross attention. The latter approach\nhas been explored on a small scale in methods like neural processes (Garnelo et al., 2018); however,\nit would be difficult to scale to larger datasets.\n\n\nWe opt for a middleground that allows the model to learn corpus statistics, but is also relatively\nefficient to compute, shown in Figure 1. Specifically, we note that document embeddings retain\na surprising amount of lexical information even after embedding (Morris et al., 2023). Therefore,\nif we pre-embed a subset of the corpus, we believe we can still dynamically calculate key dataset\ninformation during encoding.\n\n\n5\n\n\n\n\n\n\n\n\n - _m_ (( _d, q_ ) _, m_ _[b]_ ) (3)\n\n( _d,q_ ) _\u2208B_ _[b]_\n\n\n\n\n\n\n\n_b_\n\n\nWe produce contextualized embeddings via a two-stage process:\n\n\n_**First stage:**_ _Gather and embed context._ Given context documents _d_ [1] _, ..., d_ _[J]_ _\u2208D_, we embed each\nusing a unique embedding model and concatenate embeddings into a sequence _M_ 1( _d_ [1] ) _...M_ 1( _d_ _[J]_ ).\n\n\n_**Second stage:**_ _Embed document with additional context tokens._ To compute _\u03d5_ for document _d_ _[\u2032]_ we\nintegrate contextual embedding sequence at the input of second-stage embedding model _M_ 2:\n\n\n_\u03d5_ ( _d_ _[\u2032]_ ; _D_ ) = _M_ 2( _M_ 1( _d_ [1] ) _, . . ., M_ 1( _d_ _[J]_ ) _, E_ ( _d_ _[\u2032]_ 1 [)] _[, . . ., E]_ [(] _[d][\u2032]_ _T_ [))] (5)\n\n\nHere _M_ 1 is the first-stage encoder model, _M_ 2 is a second-stage encoder model, and _E_ is the token\nembedding matrix of _M_ 2 applied to each token in _d_ _[\u2032]_ . In practice, we parameterize both _M_ 1"
    ],
    "response": "In the context of information retrieval, a corpus refers to a collection of texts or documents that are utilized for retrieving information. It serves as the foundational dataset on which retrieval algorithms operate, allowing for the extraction of relevant information in response to queries. The corpus is often specific to a domain or a topic, and it is critical for evaluating retrieval systems or for generating questions that assess understanding of a given corpus <document id=\"2404.16130_graphrag_edge:chunk_4950\" metadata=\"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\">.",
    "reference": "A corpus in the context of information retrieval refers to a large collection of diversified topics, which is used for training and evaluating algorithms, particularly in tasks like question answering.",
    "answer_correctness": 0.7058497319,
    "context_precision": 0.5,
    "context_recall": 1.0
  },
  {
    "user_input": "What is the main contribution of Facebook AI's research paper at ICLR 2021?",
    "retrieved_contexts": [
      "Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set. Until recently, retrieval models were dependent\n\n- n traditional term-based information retrieval (IR) methods, which fail to capture the semantics of\nthe question beyond lexical matching and remain a major performance bottleneck for the task. Recent\nwork on dense retrieval methods instead uses pretrained encoders to cast the question and documents\ninto dense representations in a vector space and relies on fast maximum inner-product search (MIPS)\nto complete the retrieval. These approaches (Lee et al., 2019; Guu et al., 2020; Karpukhin et al.,\n2020) have demonstrated significant retrieval improvements over traditional IR baselines.\n\n\nHowever, such methods remain limited to _simple_ questions, where the answer to the question\nis explicit in a single piece of text evidence. In contrast, _complex_ questions typically involve\naggregating information from multiple documents, requiring logical reasoning or sequential (multihop) processing in order to infer the answer (see Figure 1 for an example). Since the process for\nanswering such questions might be sequential in nature, single-shot approaches to retrieval are\ninsufficient. Instead, iterative methods are needed to recursively retrieve new information at each\nstep, conditioned on the information already at hand. Beyond further expanding the scope of existing\ntextual open-domain QA systems, answering more complex questions usually involves _multi-hop_\n_reasoning_, which poses unique challenges for existing neural-based AI systems. With its practical\n\n\n_\u2217_ Equal Contribution\n1\n[https://github.com/facebookresearch/multihop_dense_retrieval.](https://github.com/facebookresearch/multihop_dense_retrieval)\n\n\n1\n\n\nPublished as a conference paper at ICLR 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: An overview of the multi-hop dense retrieval approach.\n\n\nand research values, multi-hop QA has been extensively studied recently (Talmor & Berant, 2018;\nYang et al., 2018; Welbl et al., 2018) and remains an active research area in NLP (Qi et al., 2019; Nie\net al., 2019; Min et al., 2019; Zhao et al., 2020; Asai",
      " the answer is: tuberculosis.\n\n\nQ: Which film has the director died earlier, When The Mad Aunts Arrive or The Miracle Worker (1962 Film)?\nA: When The Mad Aunts Arrive was directed by Franz Josef Gottlieb. The Miracle Worker (1962 film) was directed by\n\nArthur Penn. Franz Josef Gottlieb died on 23 July 2006. Arthur Penn died on September 28, 2010. Thus, of the two, the\ndirector to die earlier is Franz Josef Gottlieb, who directed When The Mad Aunts Arrive. So the answer is: When The\nMad Aunts Arrive.\n\n\nQ: Which album was released earlier, What'S Inside or Cassandra'S Dream (Album)?\nA: What's Inside was released in the year 1995. Cassandra's Dream (album) was released in the year 2008. Thus, of the two,\n\nthe album to release earlier is What's Inside. So the answer is: What's Inside.\n\n\nQ: Are both mountains, Serre Mourene and Monte Galbiga, located in the same country?\nA: Serre Mourene is located in Spain. Monte Galbiga is located in Italy. Thus, the two countries are not located in the same\n\n\ncountry. So the answer is: no.\n\n\nQ: What is the date of birth of the director of film Best Friends (1982 Film)?\nA: The film Best Friends was directed by Norman Jewison. Norman Jewison was born on July 21, 1926. So the answer is:\n\nJuly 21, 1926.\n\n\nQ: Which film has the director born first, Two Weeks With Pay or Chhailla Babu?\nA: Two Weeks with Pay was directed by Maurice Campbell. Chhailla Babu was directed by Joy Mukherjee. Maurice\n\nCampbell was born on November 28, 1919. Joy Mukherjee was born on 24 February 1939. Thus, from the two directors,\n\nChhailla Babu was born first, who directed Two Weeks With Pay. So the answer is: Two Weeks With Pay.\n\n\nQ: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nA: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah has a\n\ngrandchild named Prithvipati Shah. So the answer is:",
      "Published as a conference paper at ICLR 2021\n\n## ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS\n### WITH MULTI-HOP DENSE RETRIEVAL\n\n\n**Wenhan Xiong** [1] _[\u2217]_ **Xiang Lorraine Li** [2] _[\u2217]_ **Srinivasan Iyer** _[\u2021]_ **Jingfei Du** _[\u2021]_\n\n\n**Patrick Lewis** _[\u2021\u2020]_ **William Wang** [1] **Yashar Mehdad** _[\u2021]_ **Wen-tau Yih** _[\u2021]_\n\n\n**Sebastian Riedel** _[\u2021\u2020]_ **Douwe Kiela** _[\u2021]_ **Barlas O\u02d8guz** _[\u2021]_\n\n\n_\u2021_ Facebook AI\n1University of California, Santa Barbara\n2University of Massachusetts Amherst\n\n_\u2020_ University College London\n_{_ xwhan, william _}_ @cs.ucsb.edu, xiangl@cs.umass.edu,\n_{_ sviyer, jingfeidu, plewis, mehdad, scottyih, sriedel, dkiela, barlaso _}_ @fb.com\n\n\nABSTRACT\n\n\nWe propose a simple and efficient multi-hop dense retrieval approach for answering\ncomplex open-domain questions, which achieves state-of-the-art performance on\ntwo multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previ\n     - us work, our method does not require access to any corpus-specific information,\nsuch as inter-document hyperlinks or human-annotated entity markers, and can\nbe applied to any unstructured text corpus. Our system also yields a much better\nefficiency-accuracy trade-off, matching the best published accuracy on HotpotQA\nwhile being 10 times faster at inference time. [1]\n\n\n1 INTRODUCTION\n\n\n_Open domain question answering_ is a challenging task where the answer to a given question needs to\nbe extracted from a large pool of documents. The prevailing approach (Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set",
      "I\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So\n\nthe answer is: 1894.\n\n\nQ: When was Neville A. Stanton's employer founded?\nA: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862.\n\nSo the answer is: 1862.\n\n\nQ: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to?\nA: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located\n\nin the Prince County. So the answer is: Prince County.\n\n\nQ: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America\u2212\n\nLite: How Imperial Academia Dismantled Our Culture's author?\nA: The author of America\u2212Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was\n\neducated at the Yale University. The city in Connecticut that has the highest number of Zagat\u2212rated restaurants is New\nHaven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale\nHerald.\n\n\nQ: What is the headquarters for the organization who sets the standards for ISO 21500?\nA: The standards for ISO 21500 were set by International Organization for Standardization. The International Organization\n\nfor Standardization has headquarters in Geneva. So the answer is: Geneva.\n\n\nQ: What did the publisher of Banjo\u2212Tooie rely primarily on for its support?\nA: The publisher of Banjo\u2212Tooie is Nintendo. Nintendo relied primarily for its support on first\u2212party games. So the answer is:\n\nfirst\u2212party games.\n\n\nQ: In which county was the birthplace of the Smoke in tha City performer?\nA: The performer of Smoke in tha City is MC Eiht. MC Eiht's birthplace is Compton. Compton is located in the county of Los\n\nAngeles County. So the answer is: Los Angeles County.\n\n\nQ: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?\n\n\nA: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central\n\nJakarta.\n\n\nQ:",
      "\nthe text at different levels so that it can effectively and efficiently answer questions at different levels.\n\n\n1\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: **Tree construction process:** RAPTOR recursively clusters chunks of text based on their\nvector embeddings and generates text summaries of those clusters, constructing a tree from the\nbottom up. Nodes clustered together are siblings; a parent node contains the text summary of that\ncluster.\n\n\nOur main contribution is the idea of using text summarization to allow retrieval augmentation of\ncontext at different scales, and to show its effectiveness in experiments on collections of long documents. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020),\nGPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current\nretrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with UnifiedQA, gives new state-of-the-art results on three QA tasks: free text response questions on books\nand movies (NarrativeQA, Ko\u02c7cisk`y et al. 2018), full-text NLP papers (QASPER, Dasigi et al. 2021),\nand multiple-choice questions based on medium-length passages (QuALITY, Pang et al. 2022). [1]\n\n\n2 RELATED WORK\n\n\n**Why Retrieval?** Recent advances in hardware and algorithms have indeed expanded the context lengths that models can handle, leading to questions about the need for retrieval systems (Dai\net al., 2019; Dao et al., 2022; Liu et al., 2023). However, as Liu et al. (2023) and Sun et al. (2021)\nhave noted, models tend to underutilize long-range context and see diminishing performance as context length increases, especially when pertinent information is embedded within a lengthy context.\nMoreover, practically, use of long contexts is expensive and slow. This suggests that selecting the\nmost relevant information for knowledge-intensive tasks is still crucial.\n\n\n**Retrieval Methods** Retrieval-augmented language models (RALMs) have seen improvements in\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\nhave transitioned from traditional term-based techniques like **TF-IDF** (Sp\u00a8arck Jones,"
    ],
    "response": "The main contribution of Facebook AI's research paper at ICLR 2021 is the development of a simple and efficient multi-hop dense retrieval approach aimed at answering complex open-domain questions. This method achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER, without requiring corpus-specific information like inter-document hyperlinks or human-annotated entity markers. It offers a significantly better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being ten times faster during inference (<document id=\"2009.12756_multihop_qi:chunk_0\">).",
    "reference": "The main contribution of Facebook AI's research paper at ICLR 2021 is proposing a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, their method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Their system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.",
    "answer_correctness": 0.9981244207,
    "context_precision": 0.8333333333,
    "context_recall": 1.0
  },
  {
    "user_input": "What are the performance comparisons between BERT-wwm reader and ELECTRA reader on HotpotQA dev set?",
    "retrieved_contexts": [
      "2019) explicitly encode the hyperlink graph structure within their\nanswer prediction models. In fact, this particular inductive bias provides a perhaps unreasonably\nstrong advantage in the specific context of HotpotQA, which by construction guarantees groundtruth passage sequences to follow hyperlinks. Despite not using such prior knowledge, our model\n\n- utperforms all previous systems by large margins, especially on supporting fact prediction, which\nbenefits more directly from better retrieval.\n\n\n\n**Reader** **Model** **Variants** Results for\nreader model variants are shown in Table 6. [7]\n\nTable 6: Reader comparison on HotpotQA dev set.\n\nFirst, we see that the BERT-wwm reader\nis 1-2% worse than the ELECTRA reader Model Top k EM F1\nwhen using enough passages. However, ELECTRA Top 50 61.7 74.3\nit still outperforms the results in (Asai Extractive ELECTRA Top 250 63.4 76.2\net al., 2020) which also uses BERT-wwm BERT-wwm Top 250 61.5 74.7\nfor answer prediction. While RAG and Multi-hop RAG Top 4*4 51.2 63.9\n\nGenerative\n\nFiD have shown strong improvements over FiD Top 50 61.7 73.1\nextractive models on single-hop datasets\nsuch as NaturalQuestions (Kwiatkowski\net al., 2019), they do not show an advantage in the multi-hop case. Despite having twice as many\nparameters as ELECTRA, FiD fails to outperform it using the same amount of context (top 50). In\ncontrast, on NaturalQuestions, FiD is 4 points better than a similar extractive reader when using the\ntop 100 passages in both. [8] We hypothesize that the improved performance on single-hop questions is\ndue to the ability of larger pretrained models to more effectively memorize single-hop knowledge\nabout real-world entities. [9] Compared to multi-hop questions that involve multiple relations and\nmissing entities, simple questions usually only ask about a certain property of an entity. It is likely\nthat such simple entity-centric information is explicitly mentioned by a single text piece in the\npretraining corpus, while the evidence for multihop questions is typically dispersed, making the\n\n\n7",
      " ask about a certain property of an entity. It is likely\nthat such simple entity-centric information is explicitly mentioned by a single text piece in the\npretraining corpus, while the evidence for multihop questions is typically dispersed, making the\n\n\n7For the compute-heavy generative models, we feed in as many passages as possible without running into\nmemory issues (Muli-hop RAG takes top 4 passages from hop1, and for each of those, takes another top 4 from\nhop2. They are not necessarily the same as the top 16 passages sequences.). As extractive models encode each\npassage sequence separately, we can use arbitrary number of input sequences. However, the performance mostly\nplateaus as we use over 200 input sequences.\n8We implemented NQ extractive readers with both RoBERTa-large and ELECTRA-large, and RoBERTa-large\nyielded a better answer EM of 47.3, which is much lower than the 51.4 answer EM achieved by FiD.\n9As shown by Roberts et al. (2020), a large pretrained seq2seq model can be finetuned to directly decode\nanswers with questions as the only inputs. However, we find that this retrieval-free approach performs poorly on\nmulti-hop questions. See Appendix C for the exact numbers.\n\n\n7\n\n\n\nTable 6: Reader comparison on HotpotQA dev set.\n\n\n\nModel Top k EM F1\n\n\n\nExtractive\n\n\n\nELECTRA Top 50 61.7 74.3\nELECTRA Top 250 63.4 76.2\nBERT-wwm Top 250 61.5 74.7\n\n\n\nMulti-hop RAG Top 4*4 51.2 63.9\nGenerative\nFiD Top 50 61.7 73.1\n\n\nPublished as a conference paper at ICLR 2021\n\n\nTable 7: Multi-Evidence FEVER Fact Verification Results. **Loose-Multi** represents the subset that\nrequires multiple evidence _sentences_ . **Strict-Multi** is a subset of **Loose-Multi** that require multiple\nevidence sentences from different _documents_ .\n\n\nLoose-Multi (1,960) Strict-Multi (1,059)\nMethod\nLA FEVER LA FEVER\n\n\nGEAR 66.4 38.0     -     \nGAT 66.1 38.2     -",
      "., 2019; Zhao et al., 2020), these two classes of models do not rely on hyperlinks and can be\napplied to any text.\n\n\n**Extractive** reader models learn to predict an answer span from the concatenation of the question\nand passage sequence ([ _q_, _p_ 1, ..., _pn_ ]). On top of the token representations produced by pretrained\nmodels, we add two prediction heads to predict the start and end position of the answer span. [6]\nTo predict the supporting sentences, we add another prediction head and predict a binary label at\neach sentence start. For simplicity, the same encoder is also responsible for reranking the top _k_\npassage sequences. The reranking detail has been discussed in \u00a73.1.2. Our best reader model is based\n\n- n ELECTRA (Clark et al., 2020), which has achieved the best single-model performance on the\nstandard SQuAD (Rajpurkar et al., 2018) benchmark. Additionally, we also report the performance\n\n- f BERT-large with whole word masking (BERT-wwm) to fairly compare with Asai et al. (2020).\n\n\n**Generative** models, such as RAG (Lewis et al., 2020b) and FiD (Izacard & Grave, 2020), are based\n\n- n pretrained seq2seq models. These methods finetune pretrained models with the concatenated\nquestions and retrieved documents as inputs, and answer tokens as outputs. This generative paradigm\nhas shown state-of-the-art performance on single-hop open-domain QA tasks. Specifically, FiD first\nuses the T5 encoder to process each retrieved passage sequence independently and then uses the\ndecoder to perform attention over the representations of all input tokens while generating answers.\n\n\n6To account for yes/no questions, we prepend _yes_ and _no_ tokens to the context.\n\n\n6\n\n\nPublished as a conference paper at ICLR 2021\n\n\nRAG is built on the smaller BART model. Instead of only tuning the seq2seq model, it also jointly\ntrain the question encoder of the dense retriever. We modified it to allow multi-hop retrieval.\n\n\nMore details about these two classes of reader models are described in Appendix B.2.\n\n\n3.2.1 RESULTS\n\nTable 5: HotpotQA-fullwiki test results.\n\n\nAnswer Support Joint\nMethods\nEM F1 EM",
      " the top 250 retrieved pas\nComp Bridge\n\nsage sequences (pairs of passage from hop1 and hop2). Model Overall (20%) (80%)\nSince generative models are generally heavier on the\n\nELECTRA 61.7 79.0 57.4\n\ncomputation side, we can only use fewer passages. Be\nFiD 61.7 75.3 58.3\n\nsides the observations alredy discussed in \u00a73.2.1, we\nhypothesize the worse performance of multi-hop RAG compared to FiD is partially due to the\nsmaller pretrained model used in RAG, i.e., BART is only half the size of T5-large. Also, as RAG\nback-propagate the gradients to the query encoder, it needs more memory footprint and can only\ntake in fewer retrieved contexts. Our RAG implementation largely follows the implementation of the\n\n- riginal paper and we did not use the PyTorch checkpoint (as used by FiD) to trade computation for\nmemory. We conjecture the multi-hop RAG performance will also improve if we augment the current\nimplementation with memory-saving tricks. However, given the same amount of context and read\nmodel size, the multi-hop RAG is still worse than the extractive ELECTRA reader, i.e., with only the\ntop 1 retrieved passage sequence, our ELECTRA reader gets 53.8 EM compared to the 51.2 answer\nEM achieved by multi-hop RAG when using more context.\n\n\nGiven the same number of retrieved passage sequences (top 50) as shown in table 10, FiD obtains\nsimilar performance to ELECTRA, despite that the generative model can generate arbitrary answers\nfor the given input. (We tried constrained decoding for the generative model. However, no significant\nperformance improvements were observed, indicating that the errors from the generative model are\nnot due to the free-form generation task.) Further question type analysis in HotpotQA showed that\nthe main difference comes from the comparison type of question, while for bridge question, FiD\nperforms slightly better than ELECTRA. This finding might indicate that for generation models,\nnumerical comparison is still a bigger issue compared to extractive models.\n\n\nB MODEL DETAILS\n\n\nB.1 BEST MODEL HYPERPARAMETERS\n\n\nTable 11: Hyperparameters of Retriever\n\nlearning rate 2e-5\n",
      " multi-hop retrieval.\n\n\nMore details about these two classes of reader models are described in Appendix B.2.\n\n\n3.2.1 RESULTS\n\nTable 5: HotpotQA-fullwiki test results.\n\n\nAnswer Support Joint\nMethods\nEM F1 EM F1 EM F1\n\n\nGoldEn Retriever (Qi et al., 2019) 37.9 48.6 30.7 64,2 18.9 39.1\nSemantic Retrieval (Nie et al., 2019) 46.5 58.8 39.9 71.5 26.6 49.2\nTransformer-XH (Zhao et al., 2020) 51.6 64.1 40.9 71.4 26.1 51.3\nHGN (Fang et al., 2019) 56.7 69.2 50.0 76.4 35.6 59.9\nDrKIT (Dhingra et al., 2020) 42.1 51.7 37.1 59.8 24.7 42.9\nGraph Recurrent Retriever (Asai et al., 2020) 60.0 73.0 49.1 76.4 35.4 61.2\n\n\nMDR (ELECTRA Reader) **62.3** **75.3** **57.5** **80.9** **41.8** **66.6**\n\n\n**Comparison with Existing Systems** Table 5 compares the HotpotQA test performance of our best\nELECTRA reader with recently published systems, using the numbers from the official leaderboard,\nwhich measure answer and supporting sentence exact match (EM)/F1 and joint EM/F1. Among\nthese methods, only GoldEn Retriever (Qi et al., 2019) does not exploit hyperlinks. In particular,\nGraph Recurrent Retriever trains a graph traversal model for chain retrieval; TransformerXH (Zhao\net al., 2020) and HGN (Fang et al., 2019) explicitly encode the hyperlink graph structure within their\nanswer prediction models. In fact, this particular inductive bias provides a perhaps unreasonably\nstrong advantage in the specific context of HotpotQA, which by construction guarantees groundtruth passage sequences"
    ],
    "response": "On the HotpotQA dev set, the performance comparison between the BERT-wwm reader and the ELECTRA reader is as follows:\n\n- **ELECTRA Reader**\n  - Top 250: EM 63.4, F1 76.2\n- **BERT-wwm Reader**\n  - Top 250: EM 61.5, F1 74.7\n\nThe ELECTRA reader outperforms the BERT-wwm reader by about 1.9% in EM and 1.5% in F1 when using 250 passages. Additionally, when using 50 passages, the ELECTRA reader's performance is also better, with EM 61.7 and F1 74.3, compared to the BERT-wwm reader. Overall, the ELECTRA reader shows a consistent advantage over the BERT-wwm reader on the HotpotQA dev set across different passage counts <document id=\"2009.12756_multihop_qi:chunk_6750\">.",
    "reference": "The BERT-wwm reader is 1-2% worse than the ELECTRA reader when using enough passages. However, ELECTRA still outperforms the results in (Asai et al., 2020) which also uses BERT-wwm for answer prediction.",
    "answer_correctness": 0.8621447419,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What is the purpose of HotpotQA in information retrieval?",
    "retrieved_contexts": [
      " only receive the question/claim as input without access to support\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\nan external environment to support reasoning.\n\n\n**Action Space** We design a simple Wikipedia web API with three types of actions to support\ninteractive information retrieval: (1) **search** [entity], which returns the first 5 sentences from\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\nWikipedia search engine, (2) **lookup** [string], which would return the next sentence in the page\ncontaining string, simulating Ctrl+F functionality on the browser. (3) **finish** [answer], which\nwould finish the current task with answer. We note that this action space mostly can only retrieve a\nsmall part of a passage based on exact passage name, which is significantly weaker than state-of-theart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\nand force models to retrieve via explicit reasoning in language.\n\n\n3.2 METHODS\n\n\n**ReAct Prompting** For HotpotQA and Fever, we randomly select 6 and 3 cases [2] from the training\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\nthought), where free-form thoughts are used for various purposes. Specifically, we use a combination\n\n- f thoughts that decompose questions (\u201cI need to search x, find y, then find z\u201d), extract information\nfrom Wikipedia observations (\u201cx was started in 1844\u201d, \u201cThe paragraph does not tell x\u201d), perform\ncommonsense (\u201cx is not y, so z must instead be...\u201d) or arithmetic reasoning (\u201c1844 < 1989\u201d), guide\n\n\n2We find more examples do not improve performance.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n**HotpotQA** **Fever**\n**Prompt Method** _[a]_\n(EM) (Acc)\n\n\nStandard 28.7 57.1\nCoT (Wei et al., 2022) 29.4 56.3\nCoT-SC (Wang et al., 2022a) 33.4 60.4\n\n\nAct 25.7 58.9\n\nReAct ",
      ". The answer relevance score, AR,\nfor question _q_ is then computed as:\n\n\n\nThis metric evaluates how closely the generated\nanswer aligns with the initial question or instruction.\n\n\n**Context relevance** The context _c_ ( _q_ ) is considered relevant to the extent that it exclusively contains information that is needed to answer the question. In particular, this metric aims to penalise the\n\n\n\nAR = [1]\n\n\n_n_\n\n\n\n_n_\n\n- sim( _q, qi_ ) (1)\n\n\n_i_ =1\n\n\ninclusion of redundant information. To estimate\n\ncontext relevance, given a question _q_ and its context _c_ ( _q_ ), the LLM extracts a subset of sentences,\n_Sext_, from _c_ ( _q_ ) that are crucial to answer _q_, using\nthe following prompt:\n\n\n_Please extract relevant sentences from_\n_the provided context that can potentially_\n_help answer the following question. If no_\n_relevant sentences are found, or if you_\n_believe the question cannot be answered_\n_from the given context, return the phrase_\n_\"Insufficient Information\". While extract-_\n_ing candidate sentences you\u2019re not al-_\n_lowed to make any changes to sentences_\n_from given context._\n\n\nThe context relevance score is then computed as:\n\n\nnumber of extracted sentences\nCR = (2)\ntotal number of sentences in _c_ ( _q_ )\n\n\n**4** **The WikiEval Dataset**\n\n\nTo evaluate the proposed framework, we ideally\nneed examples of question-context-answer triples\nwhich are annotated with human judgments. We\ncan then verify to what extent our metrics agree\nwith human assessments of faithfulness, answer\n\nrelevance and context relevance. Since we are not\n\naware of any publicly available datasets that could\nbe used for this purpose, we created a new dataset,\nwhich we refer to as _WikiEval_ [4] . To construct the\n\ndataset, we first selected 50 Wikipedia pages covering events that have happened since the start of\n2022 [5] . In selecting these pages, we prioritised\nthose with recent edits. For each of the 50 pages,\nwe then asked ChatGPT to suggest a question that\ncan be answered based on the introductory section\n\n- f the page, using the following prompt:\n\n\n_Your task is to formulate a question from_\n_given context satisfying the rules given_\n\n_below:_\n\n_1. The question should be fully answered_\n_from the given",
      " 48.4 _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nReAct _\u2212_ | _\u2212_ 35.1 | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nSelfAsk _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ 40.1 | _\u2212_ 15.2 | _\u2212_\n\nDecomP _\u2212_ | 50.0 _\u2212_ | _\u2212_ _\u2212_ | 59.3 _\u2212_ | _\u2212_\n\n\nIRCoT QA **45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8**\n\n\nTable 1: Comparison with other LLM-based ODQA\nsystems on EM and F1 scores. \u2018 _\u2212_ \u2019: score is unavailable. HpQA [Br] : Bridge questions subset of HotpotQA.\nMQ [2H] : MuSiQue 2-hop questions. IRCoT QA with\nGPT3 (ours) outperforms other systems by a large margin. Note: Comparisons aren\u2019t head-to-head as discussed in the text. App. \u00a7C reports updated SOTA numbers, including contemporaneous and newer works.\n\n\nAs shown in Table 1, IRCoT QA significantly\n\n- utperforms all of these recent systems by a large\nmargin, setting a new state of the art in terms of\nwhat\u2019s achievable via retrieval-augmented LLMs\n(without supervised training).\n\n\n**6** **Conclusions**\n\n\nChain-of-thought prompting has significantly improved LLMs\u2019 ability to perform multi-step reason\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nThe tap dancing\nsoloist in My\nFeet Keep\nDancing also\nstarred in what\n\n1935 MGM\n\nmusical?\n\n\nWho is the\n\nfather-in-law\n\n- f Queen Hyojeong?\n\n\nWhat is the\n\nname - f the\n\ncastle in the\ncity where the\nperformer - f\nA Collection\n\n1984\u20131989 was\n\nborn?\n\n\n\n_\u21d2_ The tap dancing\nsoloist in My Feet\nKeep Dancing is Fayard\nNicholas.\n_\u21d2_ Fayard Nicholas\nstarred in the 1935\n\nM",
      " points.\n\n\n\nWe speculate DecomP performs well on 2WikiMultihopQA because it has only a few easy-to-predict\ndecomposition patterns, which DecomP\u2019s question\ndecomposition can leverage. The lack of such patterns in HotpotQA and MuSiQue causes it to underperform compared to IRCoT. Lastly, it will be\nuseful to assess whether DSP, which is hardcoded\nfor 2-hop questions like that of HotpotQA, will\nwork well for a dataset with a varied number of\n\nhops like that of MuSiQue. We leave this further\ninvestigation to future work.\n\n\n**D** **Additional CoT Generation Examples**\n\n\nTable 5 provides illustrations, in addition to the\n\n- nes provided in Table 2, for how the CoT generations for NoR QA, OneR QA, and IRCoT QA\nmethods vary. This gives an insight into how IRCoT improves QA performance. Since NoR relies completely on parametric knowledge, it often\nmakes a factual error in the first sentence, which derails the full reasoning chain. Some of this factual\ninformation can be fixed by OneR, especially information closest to the question (i.e., can be retrieved\nusing the question). This is insufficient for fixing\n\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nWhat was the\n\nisland, - n\nwhich Marinelli\n\nGlacier is located, formerly\nknown as?\n\n\nWho is the\n\nchild - f the\ndirector of film\nMukhyamantri\n(1996 Film)?\n\n\nWhen did the\nGovernorship\nend - f the\ncity where the\nauthor of Sac\nerdotii Nostri\n\nPrimordia died?\n\n\n\n_\u21d2_ Marinelli Glacier is\n\nlocated on the island of\nTierra del Fuego.\n_\u21d2_ The island of Tierra\ndel Fuego was formerly\nknown as Isla de Xativa.\n\n\n_\u21d2_ The director - f\nMukhyamantri (1996\nfilm) is Anjan Choudhury.\n_\u21d2_ Anjan Choudhury\nhas a child named\nSandip Choudhury.\n\n\n_\u21d2_ Sacerdotii Nostri Primordia was written by\nPope John XXIII.\n_\u21d2_ Pope",
      "ing are the same as that of CoT prompting, except\nhave the answer after \"A:\" directly. Our prompts\nfor Flan-T5-* are slightly different from that of\nGPT3. For CoT Prompting, we prefix the question\n\n\n20We are not showing the paragraphs in the paper for brevity\nbut they can be obtained from the released code.\n\n\nline: \"Q: Answer the following question by reasoning step-by-step. <actual-question>\". For Direct\nPrompting, we prefix the question line: \"Q: Answer\nthe following question. <actual-question>\". We\ndid this to follow Flan-T5-*\u2019s training format and\nfound it to help its CoT generation.\n\n\nListing 1: Chain-of-Thought annotations for HotpotQA.\n\n\nQ: Jeremy Theobald and Christopher Nolan share what profession?\nA: Jeremy Theobald is an actor and producer. Christopher Nolan is a director, producer, and screenwriter. Therefore, they\n\nboth share the profession of being a producer. So the answer is: producer.\n\n\nQ: What film directed by Brian Patrick Butler was inspired by a film directed by F.W. Murnau?\nA: Brian Patrick Butler directed the film The Phantom Hour. The Phantom Hour was inspired by the films such as Nosferatu\n\nand The Cabinet of Dr. Caligari. Of these Nosferatu was directed by F.W. Murnau. So the answer is: The Phantom Hour.\n\n\nQ: How many episodes were in the South Korean television series in which Ryu Hye\u2212young played Bo\u2212ra?\nA: The South Korean television series in which Ryu Hye\u2212young played Bo\u2212ra is Reply 1988. The number of episodes Reply\n\n1988 has is 20. So the answer is: 20.\n\n\nQ: Vertical Limit stars which actor who also played astronaut Alan Shepard in \"The Right Stuff\"?\nA: The actor who played astronaut Alan Shepard in \"The Right Stuff\" is Scott Glenn. The movie Vertical Limit also starred\n\nScott Glenn. So the answer is: Scott Glenn.\n\n\nQ: What was the 2014 population of the city where Lake Wales Medical Center is located?\nA: Lake Wales Medical Center is located in the city of Polk County, Florida. The population of Polk County in 2014 was\n\n15,140. So the answer is: 15,140.\n\n\nQ: Who was born first? Jan"
    ],
    "response": "The purpose of HotpotQA in information retrieval is to improve the ability of models to answer multi-hop questions by facilitating the retrieval and reasoning process. It specifically allows models to demonstrate how they interact with knowledge sources, like Wikipedia, to gather information necessary for answering complex queries. The framework incorporates explicit reasoning steps in which a model retrieves relevant data, processes it, and derives answers based on that information. This encourages the models to engage with the material more deeply, simulating human-like reasoning rather than simply relying on internal knowledge or direct retrieval outputs <document id=\"2210.03629_react_yao:chunk_4050\">.",
    "reference": "HotpotQA is used for developing advanced models to improve information retrieval and understanding.",
    "answer_correctness": 0.4950880939,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What is the birthday of the author of 'She Walks These Hills'?",
    "retrieved_contexts": [
      " by the decomposed system. These errors\ncould be potentially avoided if the model has access to the full information in the original question or\nprevious hop results. The important clue for correctly retrieving the documents or avoiding errors is\nmarked in blue. Once decomposed, the marked information are not longer available in one of the\ndecomposed retrieval hop.\n\n\n**Multi-hop Question** : What is the birthday of the author of \u201dShe Walks These Hills\u201d?\n**Decomposed Questions** :\n1. Who is the author of She Walks These Hills?\n2. What is the birthday of Sharyn McCrumb?\n**Ground-truth SP Passages** :\nShe Walks These Hills: She Walks These Hills is a book written by Sharyn McCrumb and\npublished by Charles Scribner\u2019s Sons in 1994, which later went on to win the Anthony Award\nfor Best Novel in 1995.\nSharyn McCrumb: Sharyn McCrumb (born February 26, 1948) is an American writer whose\n~~books celebrate th~~ e history and folklore of Appalachia. McCrumb is the winner of numerous\nliterary awards...\n**Decomposed Error Case:**\n1. She Walks These Hills (\u2713)\n2. Tane McClure\u00b4 : Tan\u00b4e M. McClure (born June 8, 1958) is an American singer and actress.\n\n\n**Multi-hop Question:** When was the album with the song Unbelievable by American rapper\nThe Notorious B.I.G released?\n**Decomposed Questions:**\n1. What is the album with the song Unbelievable by American rapper The Notorious B.I.G?\n2. When was the album Ready to Die released?\n**Ground-truth SP Passages:**\nUnbelievable (The Notorious B.I.G. song): Unbelievable is a song by American rapper The\n~~Notorious B.I.G., recorded for his debut st~~ udio album Ready to Die...\nReady to Die: Ready to Die is the debut studio album by American rapper The Notorious B.I.G.;\n~~it was release~~ d on September 13, 1994, by Bad Boy Records and Arista Records...\n**Decomposed Error Case:**\n1. Unbelievable (The Notorious B.I.G. song) (\u2713)\n2. Ready to Die (The Stooges album",
      "I\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So\n\nthe answer is: 1894.\n\n\nQ: When was Neville A. Stanton's employer founded?\nA: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862.\n\nSo the answer is: 1862.\n\n\nQ: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to?\nA: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located\n\nin the Prince County. So the answer is: Prince County.\n\n\nQ: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America\u2212\n\nLite: How Imperial Academia Dismantled Our Culture's author?\nA: The author of America\u2212Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was\n\neducated at the Yale University. The city in Connecticut that has the highest number of Zagat\u2212rated restaurants is New\nHaven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale\nHerald.\n\n\nQ: What is the headquarters for the organization who sets the standards for ISO 21500?\nA: The standards for ISO 21500 were set by International Organization for Standardization. The International Organization\n\nfor Standardization has headquarters in Geneva. So the answer is: Geneva.\n\n\nQ: What did the publisher of Banjo\u2212Tooie rely primarily on for its support?\nA: The publisher of Banjo\u2212Tooie is Nintendo. Nintendo relied primarily for its support on first\u2212party games. So the answer is:\n\nfirst\u2212party games.\n\n\nQ: In which county was the birthplace of the Smoke in tha City performer?\nA: The performer of Smoke in tha City is MC Eiht. MC Eiht's birthplace is Compton. Compton is located in the county of Los\n\nAngeles County. So the answer is: Los Angeles County.\n\n\nQ: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?\n\n\nA: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central\n\nJakarta.\n\n\nQ:",
      "\n\n\nPreprint.\n\n\n**Input** What genre is A Question and Answer Guide to Astronomy? (PopQA)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] A Question and Answer Guide to Astronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ **ISSUP** =Fully\nSupported][ **ISUSE** =5]\n\n**Reference [1]** A Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is\nintended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene\nRoy, and published in English by Cambridge University Press in 2010. It was originally written in French. The\ncontent within the book is written using a question and answer format. It contains some 250 questions, which\nThe Science Teacher states each are answered with a concise and well-formulated essay that is informative and\nreadable. The Science Teacher review goes on to state that many of the answers given in the book are little gems\n\n - f science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a\nstudent, and that the answers are informative, well constructed, and thorough. The book covers information about\nthe planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in\nthe Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers,\nand the Moon illusion.\n\n**S&P** Supported and Plausible.\n**Correctness of** **ISREL and** **ISSUP :** Both **ISREL** and **ISSUP** are correct, as the Reference [1] provides\ndetailed information of the book including the genre, and the model output is fully supported by the reference.\n**Input** Who is the author of The Lie? (PopQA)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] The author of The Lie is Sam Harris.[\n**ISSUP** =Fully Supported][ **ISUSE** =5]\n\n**Reference [1]** Lying is a 2011 long-form essay book by American author and neuroscience expert Sam Harris.\nHarris argues that we can radically simplify our lives and improve society",
      " Nothing Artificial (0 .53oz) 6-Pack<br>Rating: N .A<br>.<br>[Description]<br>[Features]<br>[Reviews]<br>[BuyNow]<br>Action:think[Forsixteenpackofapplecinnamonfreezedriedbanana<br>chips, the item has options \u2019apple cinnamon\u2019 and \u20190 .53 ounce (pack of<br>16)\u2019 and seems good to buy.]<br>Observation: OK<br>.<br>Action:click[applecinnamon]<br>Observation: You have clicked apple cinnamon.<br>Action: click[0 .53 ounce (pack of 16)]<br>Observation: You have clicked 0 .53 ounce (pack of 16) .<br>Action:click[BuyNow]|\n|---|---|\n|**Score**: 0.125|**Score**: 1.0|\n\n\n31\n\n\nPublished as a conference paper at ICLR 2023\n\n\nE MORE ANALYSIS\n\n\nE.1 SUCCESS AND FAILURE MODES ANALYSIS\n\n\nWe provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.\nSearch results and non-representative steps are omitted to reduce space.\n\n\nSuccess: True positive\n\nReAct\n\nQuestion: Author David Chanoff has collaborated with a U.S. Navy admiral who served as\nthe ambassador to the United Kingdom under which President?\nThought 1: I need to search David Chanoff and find the U.S. Navy admiral he\n\ncollaborated with.\n\nAction 1: Search[David Chanoff]\n\nObservation 1: David Chanoff is a noted author...\n\nAction 2: Search[William J. Crowe]\n\nObservation 2: William J. Crowe is...\n\nThought 3: William J. Crowe served as the ambassador to the United Kingdom under\n\nPresident Bill Clinton. So the answer is Bill Clinton.\n\nAction 3: Finish[Bill Clinton]\nObservation 3: Episode finished\n\nCoT\n\nQuestion: What was the name of the team that represented an Arizona university in\nthe conference that covers a broad expanse of the western United States,\nincluding Arizona, California, and New Mexico?\nThought: Let\u2019s think step by step. The name of the team that represented an Arizona\nuniversity in the conference that covers a broad expanse of the western\nUnited States, including Arizona, California, and New Mexico is Arizona",
      " points.\n\n\n\nWe speculate DecomP performs well on 2WikiMultihopQA because it has only a few easy-to-predict\ndecomposition patterns, which DecomP\u2019s question\ndecomposition can leverage. The lack of such patterns in HotpotQA and MuSiQue causes it to underperform compared to IRCoT. Lastly, it will be\nuseful to assess whether DSP, which is hardcoded\nfor 2-hop questions like that of HotpotQA, will\nwork well for a dataset with a varied number of\n\nhops like that of MuSiQue. We leave this further\ninvestigation to future work.\n\n\n**D** **Additional CoT Generation Examples**\n\n\nTable 5 provides illustrations, in addition to the\n\n- nes provided in Table 2, for how the CoT generations for NoR QA, OneR QA, and IRCoT QA\nmethods vary. This gives an insight into how IRCoT improves QA performance. Since NoR relies completely on parametric knowledge, it often\nmakes a factual error in the first sentence, which derails the full reasoning chain. Some of this factual\ninformation can be fixed by OneR, especially information closest to the question (i.e., can be retrieved\nusing the question). This is insufficient for fixing\n\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nWhat was the\n\nisland, - n\nwhich Marinelli\n\nGlacier is located, formerly\nknown as?\n\n\nWho is the\n\nchild - f the\ndirector of film\nMukhyamantri\n(1996 Film)?\n\n\nWhen did the\nGovernorship\nend - f the\ncity where the\nauthor of Sac\nerdotii Nostri\n\nPrimordia died?\n\n\n\n_\u21d2_ Marinelli Glacier is\n\nlocated on the island of\nTierra del Fuego.\n_\u21d2_ The island of Tierra\ndel Fuego was formerly\nknown as Isla de Xativa.\n\n\n_\u21d2_ The director - f\nMukhyamantri (1996\nfilm) is Anjan Choudhury.\n_\u21d2_ Anjan Choudhury\nhas a child named\nSandip Choudhury.\n\n\n_\u21d2_ Sacerdotii Nostri Primordia was written by\nPope John XXIII.\n_\u21d2_ Pope"
    ],
    "response": "The author of \"She Walks These Hills\" is Sharyn McCrumb, who was born on February 26, 1948 <document id=\"2009.12756_multihop_qi:chunk_13950\">.",
    "reference": "Sharyn McCrumb was born on February 26, 1948.",
    "answer_correctness": 0.7234311135,
    "context_precision": 0.9999999999,
    "context_recall": 1.0
  },
  {
    "user_input": "What is the role of downstream models in the retrieval process described?",
    "retrieved_contexts": [
      " its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily\nbetter at using their input context (\u00a72.3).\nGiven that language models struggle to retrieve\nand use relevant information in the multi-document\n\nquestion answering task, to what extent can language models even _retrieve_ from their input contexts? We study this question with a synthetic keyvalue retrieval task, which is designed to be a minimal testbed for the basic ability to retrieve matching\ntokens from the input context. In this task, models\nare given a collection of JSON-formatted key-value\npairs and must return the value associated with a\nspecific key. Similar to the multi-document QA\ntask, the key-value retrieval task admits controlled\nchanges to the input context length (adding more\nkey-value pairs) and the position of relevant information. Although some models perform the\nsynthetic key-value retrieval task perfectly, other\nmodels struggle to simply retrieve matching tokens\nthat occur in the middle of their input context and\ncontinue to exhibit a U-shaped performance curve.\nTo better understand why language models struggle to robustly access and use information in their\ninput contexts, we study the role of model architecture (decoder-only vs. encoder-decoder), queryaware contextualization, and instruction fine-tuning\n(\u00a74). We find that:\n\n\n  - Encoder-decoder models are relatively robust\nto changes in the position of relevant information within their input context, but only when\nevaluated on sequences within its trainingtime sequence length. When evaluated on\nsequences longer than those seen during training, we observe a U-shaped performance\ncurve (\u00a74.1).\n\n\n  - Query-aware contextualization (placing the\nquery before _and_ after the documents or keyvalue pairs) enables near-perfect performance\n\n   - n the synthetic key-value task, but minimally\nchanges trends in multi-document QA (\u00a74.2).\n\n\n  - Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of\nrelevant information in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that",
      "downstream models, our system can match the best published result while being 10x faster.\n\n\n2 METHOD\n\n\n2.1 PROBLEM DEFINITION\n\n\nThe retrieval task considered in this work can be described as follows (see also Figure 1). Given a\nmulti-hop question _q_ and a large text corpus _C_, the retrieval module needs to retrieve a sequence of\npassages _Pseq_ : _{p_ 1 _, p_ 2 _, ..., pn}_ that provide _sufficient_ information for answering _q_ . Practically, the\nretriever returns the _k_ best-scoring sequence candidates, _{Pseq_ [1] _[,][ P]_ _seq_ [2] _[, ...,][ P]_ _seq_ _[k]_ _[}]_ [ (] _[k][ \u226a|C|]_ [), with the]\nhope that at least one of them has the desired qualities. _k_ should be small enough for downstream\nmodules to process in a reasonable time while maintaining adequate recall. In general, retrieval also\nneeds to be efficient enough to handle real-world corpora containing millions of documents.\n\n\n2.2 MULTI-HOP DENSE RETRIEVAL\n\n\n**Model** Based on the sequential nature of the multi-hop retrieval problem, our system solves it in an\niterative fashion. We model the probability of selecting a certain passage sequence as follows:\n\n\n\n_P_ ( _Pseq|q_ ) =\n\n\n\n_n_\n\n- _P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) _,_\n\n\n_t_ =1\n\n\n2\n\n\nPublished as a conference paper at ICLR 2021\n\n\nwhere for _t_ = 1, we only condition on the original question for retrieval. At each retrieval step, we\nconstruct a new query representation based on previous results and the retrieval is implemented as\nmaximum inner product search over the dense representations of the whole corpus:\n\n\nexp ( _\u27e8_ _**p**_ _t,_ _**q**_ _t\u27e9_ )\n_P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) =\n\n~~\ufffd~~ _p\u2208C_ [exp (] _[\u27e8]_ _**[p]**_ _[,]_ _**[ q]**_ _[t][\u27e9]_ [)] _[,]_",
      "GLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks.\n\n\nFigure 9 ( _Right_ ) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neither\nchange matters too much. The Mamba-MLP architecture is only slightly worse, and still better than all models except\nTransformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the fact\nthat many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao,\nFu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022).\n\n\n**H3 Architecture: Training Recipes.** Next we ablate differences between the Hyena and H3++ models, our weakest\nand strongest models outside of Transformer++ and Mamba, particularly to isolate the effect of training recipes.\n\n\n- **Hyena** : The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).\n\n\n**Hyena+** : The same architecture but with the improved training recipe described above.\n\n\n- **H3+** : The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel.\n\n\n- **H3++** : The same as H3+, but with a linear attention _head dimension_ - f 8. This increases computation inside the SSM\nrecurrence but does not increase parameters.\n\n\nOur general convention is that \u201cModel+\u201d represents the base model with the improved training recipe, and \u201cModel++\u201d also\nallows for architectural changes.\n\n\nFigure 9 ( _Right_ ) shows that\n\n\nA large improvement is achieved by the improved training recipe, which was used for many of the models in the main\nFigure 4 (RetNet, H3++, Transformer++, Mamba).\n\n\n- The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with findings throughout this paper.\n\n\nThe head dimension expansion improves performance, consistent with one of our main themes that expanded state\ndimension improves performance for SSMs (Section 3).\n\n\n**E.2.3** **Downstream Evaluation Details**\n\n\nThis pretraining procedure",
      " for instance, divides its retrieval process into distinct steps where LLMs\nproactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive\nstance allows LLMs to decide when to search for necessary\ninformation, akin to how an agent utilizes tools.\nWebGPT [110] integrates a reinforcement learning framework to train the GPT-3 model in autonomously using a\nsearch engine during text generation. It navigates this process\nusing special tokens that facilitate actions such as search\nengine queries, browsing results, and citing references, thereby\nexpanding GPT-3\u2019s capabilities through the use of external\nsearch engines. Flare automates timing retrieval by monitoring\nthe confidence of the generation process, as indicated by the\n\n\nprobability of generated terms [24]. When the probability falls\nbelow a certain threshold would activates the retrieval system\nto collect relevant information, thus optimizing the retrieval\ncycle. Self-RAG [25] introduces \u201creflection tokens\u201d that allow\nthe model to introspect its outputs. These tokens come in\ntwo varieties: \u201cretrieve\u201d and \u201ccritic\u201d. The model autonomously\ndecides when to activate retrieval, or alternatively, a predefined\nthreshold may trigger the process. During retrieval, the generator conducts a fragment-level beam search across multiple\nparagraphs to derive the most coherent sequence. Critic scores\nare used to update the subdivision scores, with the flexibility\nto adjust these weights during inference, tailoring the model\u2019s\nbehavior. Self-RAG\u2019s design obviates the need for additional\nclassifiers or reliance on Natural Language Inference (NLI)\nmodels, thus streamlining the decision-making process for\nwhen to engage retrieval mechanisms and improving the\nmodel\u2019s autonomous judgment capabilities in generating ac\ncurate responses.\n\n\nVI. TASK AND EVALUATION\n\n\nThe rapid advancement and growing adoption of RAG\nin the field of NLP have propelled the evaluation of RAG\nmodels to the forefront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop/multi-hop QA, multiplechoice, domain-specific QA as well as",
      " and human feedback\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\ndecision making process only requires language description of the reasoning procedure. [6]\n\n\nLLMS have also been increasingly employed in interactive and embodied environments for planning\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\nis then reranked by an affordance model grounded on the visual environments for final prediction.\nInner Monologue made further improvements by adding the eponymous \u201cinner monologue\", which is\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\nfirst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\nthat Inner Monologue does not truly comprise of inner thoughts \u2014 this is elaborated in Section 4. We\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti\net al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of\nLLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and\ndecision making. What is more, progress in LLMs has also inspired the development of versatile and\ngeneralist agents like Reed et al. (2022).\n\n\n6 CONCLUSION\n\n\nWe have proposed ReAct \u2013 a simple yet effective method for synergizing reasoning and acting in\nlarge language models. Through a diverse set of experiments on multi-hop question-answering, fact\nchecking, and interactive decision-making tasks, we show that ReAct leads to superior performance\nwith interpretable decision traces. Despite the simplicity of our method, complex tasks with large\naction spaces require more demonstrations to learn well, which unfortunately can easily go beyond\nthe input length limit of in-context learning. We explore the fine-tuning approach on HotpotQA\n\n\n6Human feedback can also be incorporated in a complementary manner but we leave it for future work.\n\n\n9\n\n\nPublished"
    ],
    "response": "The role of downstream models in the retrieval process is primarily to utilize the output from retrieval modules to perform specific tasks effectively, such as answering a multi-hop question. In the context described, the downstream models are responsible for processing the retrieved passages, which are selected based on their likelihood of containing relevant information for the question at hand. The retrieval module returns a sequence of candidate passages, and the downstream models must analyze these passages to extract answers, thereby leveraging the information retrieved.\n\nFor example, in the multi-hop retrieval scenario, the system iteratively constructs a new query representation from previously retrieved results and processes them to answer complex questions efficiently (<document id=\"2009.12756_multihop_qi:chunk_1350\">). This interaction highlights the importance of the effectiveness of the downstream models in ensuring that the information provided by the retrieval phase is adequately utilized to generate accurate responses. The choice of the number of passages retrieved (denoted by _k_) is also crucial as it needs to balance recall with processing feasibility for these downstream tasks (<document id=\"2009.12756_multihop_qi:chunk_1350\">).",
    "reference": "Downstream models are used to process the sequence candidates retrieved by the retriever and determine which one has the desired qualities.",
    "answer_correctness": 0.4390470507,
    "context_precision": 0.9166666666,
    "context_recall": 1.0
  },
  {
    "user_input": "What conference paper introduced WebQuestions?",
    "retrieved_contexts": [
      ".:\nToward multimodal image-to-image translation. In: Advances in Neural Information\nProcessing Systems (2017) 3\n91. Zontak, M., Irani, M.: Internal statistics of a single natural image. In: IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) (2011) 7\n\n\n**Appendix A** **Additional Image-to-Image Results**\n\n\nWe first show additional, randomly selected results on datasets used in our main\npaper. We then show results on additional datasets.\n\n\n**A.1** **Additional comparisons**\n\n\nIn Figure 10, we show additional, randomly selected results for Horse _\u2192_ Zebra\nand Cat _\u2192_ Dog. This is an extension of Figure 3 in the main paper. We compare\nto baseline methods CycleGAN [89], MUNIT [30], DRIT [41], Self-Distance and\nDistanceGAN [4], and GcGAN [18].\n\n\n**A.2** **Additional datasets**\n\n\nIn Figure 11 and Figure 12, we show additional datasets, compared against\nbaseline method CycleGAN [89]. Our method provides better or comparable\nresults, demonstrating its flexibility across a variety of datasets.\n\n_\u2022 Apple\u2192Orange_ contains 996 apple and 1,020 orange images from ImageNet\nand was introduced in CycleGAN [89].\n\n_\u2022 Yosemite Summer\u2192Winter_ contains 1,273 summer and 854 winter images of\nYosemite scraped using the FlickAPI was introduced in CycleGAN [89].\n\n_\u2022 GTA\u2192Cityscapes_ GTA contains 24,966 images [63] and Cityscapes [13] contains\n19,998 images of street scenes from German cities. The task was originally\nused in CyCADA [29].\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 21\n\n\nInput CUT FastCUT CycleGAN MUNIT DRIT DistanceGAN SelfDistGAN GcGAN\n\n\nFig. 10: **Randomly selected Horse** _\u2192_ **Zebra and Cat** _\u2192_ **Dog results** . This is an\nextension of Figure 3 in the main paper.\n\n\n22 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\nInput Ours(idt) CycleGAN Input Ours(idt) CycleGAN\n\n\nFig. 11: **Apple** _\u2192_ **Orange** and **Summer",
      "27B Suff 26.9% 27.5% 10.8% 23.3% 40.7% 64.1%\n\nInsuff 11.8% 6.9% 7.2% 10.1% 22.7% 37.9%\n\nClaude 3.5 Suff 67.9% 73.1% 48.9% 74.0% 46.3% 66.7%\n\nSonnet Insuff 26.5% 33.3% 19.9% 40.4% 29.0% 38.3%\n\n\nThe Contains Answer metric exhibits several characteristics when compared to LLMEval:\n\n\n1. Different formatting affects matching:\n\n\nQ: What date did the creator of Autumn Leaves die?\nGround Truth: 13 August 1896\nResponse: August 13, 1896.\nContains Answer: False\n\nLLMEval: Correct\n\n\n2. Semantic equivalents are not captured:\n\n\nQ: What former Los Angeles Lakers majority owner is the\nfather of Jeanie Marie Buss?\n\nGround Truth: Gerald Hatten Buss\n\nResponse: Jerry Buss.\nContains Answer: False\n\nLLMEval: Correct\n\n\n3. Partial matches can be marked as correct:\n\n\nQ: What is Amazon Prime Video\u2019s most watched premiere ever?\nGround Truth: The Rings of Power\nResponse: The series explores the forging of the Rings of Power,\nthe rise of Sauron...\n\nContains Answer: True\n\nLLMEval: Hallucinate\n\n\nThe LLM QA evaluator provides several practical advantages:\n\n\n20\n\n\nPublished as a conference paper at ICLR 2025\n\n\n    - Handles variations in model verbosity and formatting\n\n\n    - Distinguishes between correct, abstain, and incorrect responses\n\n\n    - Enables efficient evaluation across multiple datasets\n\n\nOur analysis shows two key findings that are consistent across both metrics: LLMs (i) exhibit\nhallucination even with sufficient context and (ii) struggle to abstain with insufficient context.\n\n\n21\n\n\nPublished as a conference paper at ICLR 2025\n\n\nC PROMPTS\n\n\nC.1 SUFFICIENT CONTEXT AUTORATER PROMPT\n\n\n\n\n\n22\n\n\nPublished as a conference paper at ICLR 2025\n\n\nC.2 FLAME PROMPT\n\n\nC.3 LLMEVAL PROMPT\n\n\nSince the questions in our datasets ask for free form answers, the LLM responses may not exactly\nmatch",
      "/testing splitting method as in previous\nwork (Lee et al., 2019). Below we briefly describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\n**Natural Questions (NQ)** (Kwiatkowski et al.,\n2019) was designed for end-to-end question answering. The questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identified by annotators.\n**TriviaQA** (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web.\n\n**WebQuestions (WQ)** (Berant et al., 2013) consists\n\n- f questions selected using Google Suggest API,\nwhere the answers are entities in Freebase.\n**CuratedTREC (TREC)** (Baudis and\u02c7 Sediv [\u02c7] y`,\n2015) sources questions from TREC QA tracks\n\n\n5However, Wang et al. (2019) also propose splitting documents into overlapping passages, which we do not find advantageous compared to the non-overlapping version.\n\n\n\n**Dataset** **Train** **Dev** **Test**\n\n\nNatural Questions 79,168 58,880 8,757 3,610\nTriviaQA 78,785 60,413 8,837 11,313\nWebQuestions 3,417 2,474 361 2,032\nCuratedTREC 1,353 1,125 133 694\nSQuAD 78,713 70,096 8,886 10,570\n\n\nTable 1: Number of questions in each QA dataset. The\ntwo columns of **Train** denote the original training examples in the dataset and the actual questions used for\ntraining DPR after filtering. See text for more details.\n\n\nas well as various Web sources and is intended for\n\n- pen-domain QA from unstructured corpora.\n**SQuAD v1.1** (Rajpurkar et al., 2016) is a popular benchmark dataset for reading comprehension.\nAnnotators were presented with a Wikipedia paragraph, and asked to write questions that could be\nanswered from the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison",
      "Published as a conference paper at ICLR 2025\n\n## - - VISRAG: VISION BASED RETRIEVAL AUGMENTED - GENERATION ON MULTI MODALITY DOCUMENTS\n\n\n**Shi Yu** [1] _[\u2217]_ **, Chaoyue Tang** [2] _[\u2217]_ **, Bokai Xu** [2] _[\u2217]_ **, Junbo Cui** [2] _[\u2217]_ **, Junhao Ran** [3] **, Yukun Yan** [1] _[\u2020]_ **,**\n**Zhenghao Liu** [4] **, Shuo Wang** [1] **, Xu Han** [1] **, Zhiyuan Liu** [1] _[\u2020]_ **, Maosong Sun** [1]\n\n1Department of Computer Science and Technology, Tsinghua University\n2ModelBest Inc. 3Rice University 4Northeastern University\nyus21@mails.tsinghua.edu.cn\n\n\nABSTRACT\n\n\nRetrieval-augmented generation (RAG) is an effective technique that enables large\nlanguage models (LLMs) to utilize external knowledge sources for generation.\nHowever, current RAG systems are solely based on text, rendering it impossible\nto utilize vision information like layout and images that play crucial roles in realworld multi-modality documents. In this paper, we introduce VisRAG, which\ntackles this issue by establishing a vision-language model (VLM)-based RAG\npipeline. In this pipeline, instead of first parsing the document to obtain text,\nthe document is directly embedded using a VLM as an image and then retrieved\nto enhance the generation of a VLM. Compared to traditional text-based RAG,\nVisRAG maximizes the retention and utilization of the data information in the\n\n     - riginal documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in\nVisRAG and explore a variety of generation methods. Experiments demonstrate\nthat VisRAG outperforms traditional RAG in both the retrieval and generation\nstages, achieving a 20\u201340% end-to-end performance gain over traditional textbased RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing\ntraining data and demonstrates strong generalization capability, positioning",
      "Published as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto finetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved significant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\n\n- f questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model\u2019s hidden states. However, their method requires an initial round\n\n- f TF-IDF/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\n"
    ],
    "response": "The conference paper that introduced WebQuestions is titled \"WebQuestions\" and it was authored by Berant et al. in 2013 <document id=\"2004.04906_dpr_karpukhin:chunk_4050\">.",
    "reference": "WebQuestions was published as a conference paper at ICLR 2021.",
    "answer_correctness": 0.2255116817,
    "context_precision": 0.2,
    "context_recall": 0.0
  },
  {
    "user_input": "What are structured state space models (SSMs) and how do they address the limitations of Transformers?",
    "retrieved_contexts": [
      ", including very strong modern Transformer training recipes based\n\n - n LLaMa (Touvron et al. 2023). Our Mamba language model has 5\u00d7 generation throughput compared to Transformers\n\n - f similar size, and Mamba-3B\u2019s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common\nsense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\n\n\n[Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.](https://github.com/state-spaces/mamba)\n\n### **2 State Space Models**\n\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related\nto RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n\n2\n\n\n**Selective State Space Model**\n\n_**with Hardware-aware State Expansion**_\n\n\n\ud835\udc34\n\n\n\n\ud835\udc65!\n\n\n\n\n\n\ud835\udc66!\n\n\n\n\n\n\n\n\n\n\n\n**Selection Mechanism**\n\n|\ud835\udc35 \ud835\udc36<br>! !<br>\u2206<br>!<br>Discretize<br>Project|Col2|\n|---|---|\n|||\n|||\n\n\n\nFigure 1: ( **Overview** .) Structured SSMs independently map each channel (e.g. _\ud835\udc37_ = 5) of an input _\ud835\udc65_ to output _\ud835\udc66_ through a higher\ndimensional latent state _\u210e_ (e.g. _\ud835\udc41_ = 4). Prior SSMs avoid materializing this large effective state ( _\ud835\udc37\ud835\udc41_, times batch size _\ud835\udc35_ and sequence\nlength _\ud835\udc3f_ ) through clever alternate computation paths requiring time-invariance: the (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ) parameters are constant across time. Our\nselection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize\nthe expanded states in more efficient levels of the GPU memory hierarchy.\n\n\n1-dimensional function or sequence _\ud835\udc65_ ( _\ud835\udc61_ ) \u2208 R \u21a6\u2192 _\ud835\udc66_ ( _\ud835\udc61_ ) \u2208 R through an implicit latent state _\u210e_ ( _\ud835\udc61_ )",
      "## Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\nAlbert Gu [\u2217][1] and Tri Dao [\u2217][2]\n\n\n1\nMachine Learning Department, Carnegie Mellon University\n\n\n2\nDepartment of Computer Science, Princeton University\nagu@cs.cmu.edu, tri@tridao.me\n\n\n**Abstract**\n\n\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the\nTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,\ngated convolution and recurrent models, and structured state space models (SSMs) have been developed to address\nTransformers\u2019 computational inefficiency on long sequences, but they have not performed as well as attention on important\nmodalities such as language. We identify that a key weakness of such models is their inability to perform content-based\nreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses\ntheir weakness with discrete modalities, allowing the model to _selectively_ propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though this change prevents the use of efficient\nconvolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a\nsimplified end-to-end neural network architecture without attention or even MLP blocks ( **Mamba** ). Mamba enjoys fast\ninference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves\n\n    - n real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art\nperformance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model\n\n    - utperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream\nevaluation.\n\n### **1 Introduction**\n\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged\nas an effective paradigm in modern machine learning. The backbone of these FMs are often _sequence models_, operating on\narbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and\ngenomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al",
      " is that the model\u2019s dynamics are constant\nthrough time. In other words (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ), and consequently ( _\ud835\udc68, \ud835\udc69_ ) as well, are fixed for all time-steps. This property is\n\n\n3\n\n\ncalled _linear time invariance (LTI)_, which is deeply connected to recurrence and convolutions. Informally, we think of\nLTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these\nclasses of models.\n\n\nThus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental efficiency constraints,\ndiscussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling\ncertain types of data, and our technical contributions involve removing the LTI constraint while overcoming the efficiency\nbottlenecks.\n\n\n**Structure and Dimensions.** Finally, we note that structured SSMs are so named because computing them efficiently\nalso requires imposing structure on the _\ud835\udc68_ matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022;\nGupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.\n\n\nIn this case, the _\ud835\udc68_ \u2208 R _[\ud835\udc41]_ [\u00d7] _[\ud835\udc41]_ _, \ud835\udc69_ \u2208 R _[\ud835\udc41]_ [\u00d7][1] _, \ud835\udc6a_ \u2208 R [1][\u00d7] _[\ud835\udc41]_ matrices can all be represented by _\ud835\udc41_ numbers. To operate over an input\nsequence _\ud835\udc65_ - f batch size _\ud835\udc35_ and length _\ud835\udc3f_ with _\ud835\udc37_ channels, the SSM is applied independently to each channel. Note that in\nthis case, the total hidden state has dimension _\ud835\udc37\ud835\udc41_ per input, and computing it over the sequence length requires _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ )\ntime and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3.\n\n\n**General State Space",
      " al. 2020). However, as previously\nmentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives\nused LTI (non-selective) models, most commonly in the form of global convolutions.\n\n\n**3.3.1** **Motivation of Prior Models**\n\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods.\n\n\nAt a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in\nSection 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to _maximize_\n_hidden state dimension without paying speed and memory costs_ .\n\n\n- Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding\nthe former (2) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and\nmaterializing the latent state _\u210e_ with shape (B _,_ L _,_ D _,_ N), which is much larger (by a factor of _\ud835\udc41_, the SSM state dimension)\nthan the input _\ud835\udc65_ and output _\ud835\udc66_  - f shape (B _,_ L _,_ D). Thus the more efficient convolution mode was introduced which could\nbypass the state computation and materializes a convolution kernel (3a) of size only (B _,_ L _,_ D).\n\n\nPrior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by\na factor of _\ud835\udc41_ (\u2248 10 \u2212 100), much larger than traditional RNNs, without efficiency penalties.\n\n\n6\n\n\n**3.3.2** **Overview of Selective Scan: Hardware-Aware State Expansion**\n\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to\nrevisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and\nrecomputation. We make two main observations:\n\n\n- The naive recurrent computation uses _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ ) FLOPs while the convolutional computation uses _\ufffd",
      "\ud835\udc68_ is manually set to 0,\ninstead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically\nto this setting and probe if the model has learned to automatically reset its state on episode boundaries.\n\n\n- Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of\nbeing an exponential moving average (EMA). They additionally make an interesting connection of the discretization step\n\n - f SSMs to an EMA _damping_ term. Contrary to findings in the original S4 papers, this was the first model to show that\nreal-valued SSMs are empirically effective in certain settings or when combined with different architectural components.\n\n\n- Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this\nperspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally\nand close to LTI.\n\n\n- SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox\n\n2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of\nS4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do\nfast autoregressive inference directly.\n\n\nNotably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually\nstrictly LTI (linear time invariant).\n\n\n**B.2** **SSM Architectures**\n\n\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating\n\n- ne of the previous SSMs as a black box layer.\n\n\n- GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated\nattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most\nimportantly, its projection"
    ],
    "response": "Structured state space models (SSMs) are a recent class of deep learning sequence models that expand upon the ideas of recurrent neural networks (RNNs) and convolutional neural networks (CNNs) while addressing their computational inefficiencies. These models utilize a framework inspired by continuous systems to map input sequences to outputs through latent states.\n\nThe key advancements of structured SSMs over traditional Transformers include:\n\n1. **Dynamic State Propagation**: Unlike previous SSMs that were constrained by linear time invariance (LTI) where model dynamics were constant across time, structured SSMs introduce input-dependent dynamics. This allows the model to selectively propagate or forget information based on the current token, enhancing its capability to perform content-based reasoning, particularly in discrete modalities like language <document id=\"2312.00752_rag_roadmap_asai:chunk_0\">.\n\n2. **Hardware-Efficiency and Scalability**: Structured SSMs have been designed with hardware-aware algorithms that optimize the computation process to efficiently utilize memory hierarchies, allowing for fast inference and the handling of longer sequences in a linearly scaling manner. The Mamba model, for instance, achieves a generation throughput that is 5 times that of Transformers, while matching or exceeding their performance on various tasks at a fraction of the computational cost <document id=\"2312.00752_rag_roadmap_asai:chunk_1800\">.\n\n3. **Elimination of LTI Constraints**: By moving beyond the LTI framework, structured SSMs can harness larger effective state dimensions without incurring the typical speed penalties associated with increasing complexity. This flexibility allows for more expressive models that can better capture temporal dynamics in a range of data types, including language and audio <document id=\"2312.00752_rag_roadmap_asai:chunk_3150\">.\n\nIn summary, structured SSMs address the limitations of Transformers by incorporating selective dynamics and improving computational efficiency, which broadens their applicability and effectiveness across various data modalities.",
    "reference": "Structured state space models (SSMs) are a type of architecture developed to improve information retrieval and understanding. They address the limitations of Transformers by allowing content-based reasoning, selectively propagating or forgetting information along the sequence length dimension depending on the current token.",
    "answer_correctness": 0.3660697469,
    "context_precision": 1.0,
    "context_recall": 0.5
  },
  {
    "user_input": "What is the ability of Mamba to solve the induction heads task?",
    "retrieved_contexts": [
      " \u201cPotter\u201d by copying from history.\n\n\n**Dataset.** We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is\ncomparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We additionally investigate\ngeneralization and extrapolation abilities by evaluating on a range of sequence lengths from 2 [6] = 64 up to 2 [20] = 1048576 at\ntest time.\n\n\n**Models.** Following established work on induction heads, we use 2 layer models, which allows attention to mechanistically\nsolve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads, with various positional\nencodings) and SSM variants. We use a model dimension _\ud835\udc37_ - f 64 for Mamba and 128 for the other models.\n\n\n**Results.** Table 2 shows that Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the task perfectly\nbecause of its ability to selectively remember the relevant token while ignoring everything else in between. **It generalizes**\n**perfectly to million-length sequences, or** 4000\u00d7 **longer than it saw during training**, while no other method goes\nbeyond 2\u00d7.\n\n\n10\n\n\nModel Arch. Layer Acc.\n\n\nS4 No gate S4 18.3\n\n  - No gate S6 **97.0**\n\n\nH3 H3 S4 57.0\n\nHyena H3 Hyena 30.1\n\n  - H3 S6 **99.7**\n\n\n  - Mamba S4 56.4\n\n  - Mamba Hyena 28.4\nMamba Mamba S6 **99.8**\n\n\nTable 1: ( **Selective Copying** .)\nAccuracy for combinations of architectures\nand inner sequence layers.\n\n\n\n\n\n\n\n\n\n|Induction Heads Extrapolation|Col2|Col3|nduction He|eads Extrapolation|n|\n|---|---|---|---|---|---|\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<",
      " details and training protocol are in Appendix E.1.\n\n\n**4.1.1** **Selective Copying**\n\n\nThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test\nthe memorization abilities of recurrent models. As discussed in Section 3.1, LTI SSMs (linear recurrences and global\nconvolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for example, by\nconstructing a convolution kernel of exactly the right length (Figure 2). This was explicitly validated in earlier work on\nglobal convolutions (Romero et al. 2021). The Selective Copying task prevents this shortcut by randomizing the spacing\nbetween tokens. Note that this task has been introduced before as the Denoising task (Jing et al. 2019).\n\n\nNote that many previous works argue that adding architecture gating (multiplicative interactions) can endow models with\n\u201cdata-dependence\u201d and solve related tasks (Dao, Fu, Saab, et al. 2023; Poli et al. 2023). However, we find this explanation\ninsufficient intuitively because such gating does not interact along the sequence axis, and cannot affect the spacing between\ntokens. In particular architecture gating is not an instance of a selection mechanism (Appendix A).\n\n\nTable 1 confirms that gated architectures such as H3 and Mamba only partially improve performance, while the selection mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more powerful\narchitectures.\n\n\n**4.1.2** **Induction Heads**\n\n\nInduction heads (Olsson et al. 2022) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021) that is\nsurprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative recall and copy:\nfor example, if the model has seen a bigram such as \u201cHarry Potter\u201d in the sequence, then the next time \u201cHarry\u201d appears in\nthe same sequence, the model should be able to predict \u201cPotter\u201d by copying from history.\n\n\n**Dataset.** We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is\ncomparable to prior work on this task",
      " Broadcast _\ud835\udc37_ (Linear1( _\ud835\udc65_ )), and _\ud835\udf0f_ \u0394 = softplus,\nwhere Linear _\ud835\udc51_ is a parameterized projection to dimension _\ud835\udc51_ . The choice of _\ud835\udc60_ \u0394 and _\ud835\udf0f_ \u0394 is due to a connection to RNN gating\nmechanisms explained in Section 3.5.\n\n\n5\n\n\nOutput\n\n\nInput\n\n\n\n**Copying** **Selective Copying**\n\n\nOutput\n\n\nInput\n\n\n\nSolution **Induction Heads**\n\n\nPerfectly solved by LTI (e.g. convolutional) models that do not need to look at the actual inputs\n\n\n\n\n\nFigure 2: ( _Left_ ) The standard version of the Copying task involves constant spacing between input and output elements and is easily\nsolved by time-invariant models such as linear recurrences and global convolutions. ( _Right Top_ ) The Selective Copying task has random\nspacing in between inputs and requires time-varying models that can _selectively_ remember or ignore inputs depending on their content.\n( _Right Bottom_ ) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key\nability for LLMs.\n\n\n\n**Algorithm 1** SSM (S4)\n\n\n**Input:** _\ud835\udc65_ : (B _,_ L _,_ D)\n**Output:** _\ud835\udc66_ : (B _,_ L _,_ D)\n\n1: _\ud835\udc68_ : (D _,_ N) \u2190 Parameter\n_\u22b2_ Represents structured _\ud835\udc41_ \u00d7 _\ud835\udc41_ matrix\n2: _\ud835\udc69_ : (D _,_ N) \u2190 Parameter\n3: _\ud835\udc6a_ : (D _,_ N) \u2190 Parameter\n4: \u0394 : (D) \u2190 _\ud835\udf0f_ \u0394 (Parameter)\n5: _\ud835\udc68, \ud835\udc69_ : (D _,_ N) \u2190 discretize(\u0394 _, \ud835\udc68, \ud835\udc69_ )\n6: _\ud835\udc66_ \u2190 SSM( _\ud835\udc68, \ud835\udc69, \ud835\udc6a_ )( _\ud835\udc65_ )\n_\u22b2_ Time-invariant: recurrence or convolution\n\n7: **return** _\ud835\udc66_\n\n\n\n**Algorithm 2** SSM + Selection (S6)\n\n\n**Input:** _\ufffd",
      "HA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|2<br>|3|4|5|\n\n\nTable 2: ( **Induction Heads** .) Models are trained on sequence length 2 [8] =\n256, and tested on increasing sequence lengths of 2 [6] = 64 up to 2 [20] =\n1048576. Full numbers in Table 11.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: ( **Scaling Laws** .) Models of size \u2248 125 _\ud835\udc40_ to \u2248 1 _._ 3 _\ud835\udc35_ parameters, trained on the Pile. Mamba scales better than all other\nattention-free models and is the first to match the performance of a very strong \u201cTransformer++\u201d recipe that has now become standard,\nparticularly as the sequence length grows.\n\n\nOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly\nbetter than the others; also note that all attention models were only tested up to sequence length 2 [14] = 16384 due to\nmemory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the findings in Poli et al. (2023).\n\n\n**4.2** **Language Modeling**\n\n\nWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on both\npretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to mirror GPT3\nspecifications. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown\net al. (2020). All training details are in Appendix E.2.\n\n\n**",
      " on A100 GPUs).\n\n\n**Architecture.** We simplify prior deep sequence model architectures by combining the design of prior SSM architectures\n(Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous\narchitecture design ( **Mamba** ) incorporating selective state spaces.\n\n\nSelective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them\nsuitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong\nperformance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory\nscales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only\nconstant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency\ntogether yield performance improvements on real data up to sequence length 1M.\n\n\nWe empirically validate Mamba\u2019s potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n\n**Synthetics.** On important synthetic tasks such as copying and induction heads that have been proposed as being key to\nlarge language models, Mamba not only solves them easily but can _extrapolate solutions indefinitely long_ ( _>_ 1M tokens).\n\n\n- **Audio and Genomics.** Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers\n\n - n modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing\nFID on a challenging speech generation dataset by more than half). In both settings, its _performance improves with longer_\n_context up to million-length sequences_ .\n\n\n- **Language Modeling.** Mamba is the first _linear-time sequence model that truly achieves Transformer-quality performance_,\nboth in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba\nexceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based\n\n - n LLaMa (Touvron et al. 2023). Our Mamba language model has 5\u00d7 generation throughput compared to Transformers\n\n - f similar size, and Mamba-"
    ],
    "response": "Mamba has demonstrated a perfect ability to solve the induction heads task. Its implementation utilizes a selective state space layer, which allows it to effectively remember the relevant tokens while disregarding irrelevant input. This capability enables Mamba to generalize perfectly to sequences that are significantly longer than those seen during training, with the model achieving functionality on sequences up to 1,048,576 tokens, far exceeding the 2x generalization seen in other methods (<document id=\"2312.00752_rag_roadmap_asai:chunk_10800\">).",
    "reference": "Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the task perfectly because of its ability to selectively remember the relevant token while ignoring everything else in between.",
    "answer_correctness": 0.9768932312,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What were the speed and memory benchmarks for the SSM scan operation in the context of the Mamba model?",
    "retrieved_contexts": [
      "Test    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC09 Model Ablations** ) Models with 6M parameters. In\nSaShiMi\u2019s U-Net backbone, there are 8 center blocks operating on\nsequence length 1000, sandwiched on each side by 8 outer blocks on\nsequence length 4000, sandwiched by 8 outer blocks on sequence\nlength 16000 (40 blocks total). The architecture of the 8 center\nblocks are ablated independently of the rest. Note that Transformers\n(MHA+MLP) were not tested in the more important outer blocks\nbecause of efficiency constraints.\n\n\nOuter Center NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\n\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\n\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\n\nMamba MHA+MLP **1.850** 1.37 5.63 58.23 0.62\n\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\n\nMamba Mamba 1.852 **0.94** **6.26** **88.54** **0.52**\n\n\n\nWe benchmark the speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of\n(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6.",
      "\ufffd, \ud835\udc41_ ) in SRAM.\n\n\n3. We perform a parallel associative scan, yielding intermediate states of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in SRAM.\n\n\n4. We multiply and sum with _\ud835\udc6a_, producing outputs of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37_ ) and write it to HBM.\n\n\nThis way, we reduce IOs by a factor of _\ud835\udc42_ ( _\ud835\udc41_ ) (the state dimension), which in practice speeds up the operation by 20-40\ntimes (Section 4.5).\n\n\nFor sequence length _\ud835\udc3f_ too long where we cannot fit the sequence in SRAM (which is much smaller than HBM), we split the\nsequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate scan states, we can\ncontinue the scan with the next chunk.\n\n\n**Memory.** We describe how we use the classical technique of _recomputation_ to reduce the total amount of memory\nrequired to train selective SSM layers.\n\n\nFrom the way we fuse the forward pass, we do not save the intermediate states of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) to avoid memory blowup.\nHowever, these intermediate states are necessary for the backward pass to compute gradients. We instead recompute those\nintermediate states in the backward pass. Since the inputs \u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ and output gradient read from HBM to SRAM are\n\n- f size _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc41_ + _\ud835\udc37\ud835\udc41_ ), and the input gradients are also of size _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc41_ + _\ud835\udc37\ud835\udc41_ ), recomputation avoids the cost of reading\n_\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc41\ud835\udc37_ ) elements from HBM. This means that recomputation of the SSM states in the backward pass speeds up the\ncomputation compared to storing them and reading them from HBM.\n\n\nBeyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize the\nmemory requirement of the entire selective SSM block (input projection, convolution",
      "The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of D = 64 and\npooling factor 4. The S4+MLP block has roughly 2 _\ud835\udc37_ [2] + 4 _\ud835\udc37_ [2] parameters (expansion factor 2 in the MLP). The Transformer\nblock has 4 _\ud835\udc37_ [2] + 2 _\ud835\udc37_ [2] parameters (expansion factor 1 in the MLP). The Mamba block has the usual \u2248 6 _\ud835\udc37_ [2] parameters. All\nmodels have roughly 6M total parameters.\n\n\n**E.5** **Efficiency Benchmark**\n\n\n**Scan Operation.** We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), against\nconvolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operations\n\n- utside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing the\nQKV projections in attention.\n\n\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing the\nparameters _\ud835\udc68, \ud835\udc69, \ud835\udc6a_ in HBM.\n\n\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the large\nparameters in HBM.\n\n\n35\n\n\nTable 15: ( **Memory benchmark** .) Mamba\u2019s memory footprint is comparable to the most optimized Transformer. Results for 125M\nmodels.\n\n\nBatch size Transformer (w/ FlashAttention-2) Mamba\n\n\n1 4.6GB 4.8GB\n\n2 5.2GB 5.8GB\n\n4 6.9GB 7.3GB\n\n8 11.5GB 12.3GB\n\n16 20.7GB 23.1GB\n\n32 34.5GB 38.2GB\n\n\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and the\nfilters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexity\nis _\ud835\udc42_ ( _\ud835\udc3f_ log( _\ufffd",
      " standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained) would have higher inference\nthroughput than a 5\u00d7 smaller Transformer-1.3B. Details in Appendix E.5, which additionally includes a benchmark of\nmemory consumption.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: ( **Efficiency Benchmarks** .) ( _Left_ ) Training: our efficient scan is 40\u00d7 faster than a standard implementation. ( _Right_ ) Inference:\nas a recurrent model, Mamba can achieve 5\u00d7 higher throughput than Transformers.\n\n\n**4.6** **Model Ablations**\n\n\nWe perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with\nsize \u2248 350M models at Chinchilla token counts (same setting as Figure 4).\n\n\n**4.6.1** **Architecture**\n\n\nTable 6 investigates the effects of the architecture (block) and its inner SSM layer (Figure 3). We find that\n\n\n- Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.\n\n\nReplacing the complex-valued S4 variant from previous work with a real-valued one does not affect performance much,\nsuggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware efficiency.\n\n\nReplacing any of these with a selective SSM (S6) significantly improves performance, validating the motivation of\nSection 3.\n\n\n15\n\n\nTable 6: ( **Ablations: Architecture and SSM layer** .) The Mamba block performs similarly to H3 while being simpler. In the inner layer,\nthere is little difference among different parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More\nspecifically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.\n\n\n\nModel Arch. SSM Layer Perplexity\n\n\nHyena H3 Hyena 10 _._ 24\nH3 H3 S4 (complex) 10 _._ 30\n\n      - H3 S4 (real) 10 _._",
      "-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5,\nshowing that it is up to 7\u00d7 times faster than attention at sequence length 32K, and is as memory-efficient as the best\nattention implementation (FlashAttention).\n\n\n**Speed.** On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by memorybandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This the case with our\nscan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to significant speedup compared to\na standard implementation.\n\n\nThe standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input _\ud835\udc68, \ud835\udc69_ - f size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in GPU\nHBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel associative scan implementation to\nwrite the scan output of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) to GPU HBM, then multiply that scan output with _\ud835\udc6a_ to produce an output of size\n( _\ud835\udc35, \ud835\udc3f, \ud835\udc37_ ). However, this requires the number of memory reads/writes on the order of _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ ). We can instead fuse the\ndiscretization step, the scan, and the multiplication with _\ud835\udc6a_ into one kernel:\n\n\n1. We read in _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37_ + _\ud835\udc37\ud835\udc41_ ) bytes of memory (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ) from slow HBM to fast SRAM.\n\n\n2. We discretize to produce _\ud835\udc68, \ud835\udc69_  - f size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in SRAM.\n\n\n3. We perform a parallel associative scan, yielding intermediate states of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in SRAM.\n\n\n4."
    ],
    "response": "The speed and memory benchmarks for the SSM scan operation in the context of the Mamba model indicate that:\n\n1. **Speed**: The efficient SSM scan implementation is reported to be faster than the best attention implementation (FlashAttention-2), achieving speeds of 20-40\u00d7 faster than a standard scan implementation in PyTorch. Additionally, Mamba achieves 4-5\u00d7 higher inference throughput compared to a Transformer of similar size due to the ability to use much higher batch sizes without needing a KV cache (Gu & Dao, 2023) <document id=\"2312.00752_rag_roadmap_asai:chunk_16200\">.\n\n2. **Memory**: The memory footprint for Mamba is comparable to that of the most optimized Transformer models. For instance, the memory consumption for a batch size of 1 is 4.8GB for Mamba compared to 4.6GB for a Transformer with FlashAttention-2. As the batch size increases, memory consumption for Mamba grows, but it remains competitive with Transformers (Gu & Dao, 2023) <document id=\"2312.00752_rag_roadmap_asai:chunk_31950\">.",
    "reference": "The speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16) was faster than the best attention implementation that we know of (FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation in PyTorch. Mamba achieved 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the KV cache it can use much higher batch sizes.",
    "answer_correctness": 0.696427246,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What is the role of FreshQA in evaluating time-sensitive information and how does it compare to HotPotQA and Musique?",
    "retrieved_contexts": [
      "|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|||||||||||||\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens||||||pot|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|\n\n\nFigure 2: We compare the % of instances that our autorater labels as sufficient across datasets,\neither with the first 10k, 6k, or 2k tokens of the provided sources. FreshQA has hand-curated URLs\nthat support the answers and exhibits high sufficient context. HotPotQA and Musique have lower\nsufficient context (and even lower with 2000 tokens). We use 6000 token contexts in the remainder.\n\n\nThis is not the focus of our work, as we wish to understand how models perform with or without\nsufficient context. Having a mix of both is inevitable in generic RAG systems.\n\n\n**Datasets.** We consider **FreshQA**, **Musique-Ans**, and **HotpotQA** as a representative spread of open\nbook QA datasets. FreshQA (Vu et al., 2023) evaluates time-sensitive information and has up-to-date\nURLs that should support an answer to the queries, which we use to construct the context (see\nAppendix A.3 for details on the retrieval). We use the \u2018True Premise\u2019 setting (452 instances), skipping\n\u2018False Premise\u2019 questions that",
      " contexts).** Assume the context contains multiple plausible answers to the\nquery. Then it is sufficient if and only if it also provides enough information to distinguish between\nqueries that would lead to each answer. For example, if the question is \u201cWhat country does Ali live\nin?\u201d and the context is \u201cAli lives in Paris\u201d then this instance does not have sufficient context because\nit is not clear if Ali lives in Paris, France or Paris, Texas, USA. If the context further contains \u201cThis\nweekend, Ali took the train from Paris to Marseille.\u201d Then this becomes sufficient because it is almost\ncertain that Ali lives in France as one cannot take a train from Texas to France.\n\n\n3.2 SUFFICIENT CONTEXT AUTORATER\n\n\nNext, we consider automating the task of labeling whether instances have sufficient context or not.\nWe investigate two questions: (1) Can today\u2019s models achieve high accuracy on a challenging,\nhuman-annotated dataset? (2) How does an entailment model compare to general-purpose LLMs? To\nanswer these questions, we evaluate methods on human-labeled data. Table 1 shows that Gemini 1.5\nPro can serve as an accurate autorater to label instances in terms of sufficient context. It achieves\n93% accuracy, outperforms other methods, and operates without needing a ground truth answer.\n\n\n**Sufficient Context Labeled Dataset.** Using the above definition, we construct gold labels for\neach (query, context) pair. We did not use ground truth answers or model responses. For\nthe instances, we sample a total of 115 instances (queries, contexts, and answers) from standard\nbenchmarks (PopQA, FreshQA, Natural Questions, EntityQuestions). We design the dataset to\nbe very challenging, including single- and multi-hop questions, as well as adding highly related\n\n\n4\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 1: **Sufficient Context AutoRater.** Evaluating model ability to classify sufficient context on a\ngold-labeled dataset of 115 (query, context; answer) instances. Gemini 1.5 Pro (1-shot)\nperforms the best, while FLAMe can be a cheaper alternative. TRUE-NLI and Contains GT need\nground truth (GT) answers, while others only use (query, context). Best in column in bold.\n\n\n**Metrics:** F1 Score Accuracy Precision Recall No GT",
      "Claude 3.5 Sonnet 9.6 53.8 36.5\n\nGemini 1.5 Flash 7.7 73.1 19.2\n\nGemma 27B 9.6 55.8 34.6\n\n\n19\n\n\nPublished as a conference paper at ICLR 2025\n\n\nB.3 COMPARISON OF QA EVALUATION METRICS\n\n\nWe compare two the LLM-based QA Evaluator (LLMEval) used in the paper with a deterministic\nlexical matching metric (Contains Answer). The Contains Answer metric labels responses based on\nwhether they contain the exact ground truth answer, while LLMEval uses an LLM to assess semantic\n\ncorrectness.\n\n\nTable 5 presents model performance across three datasets (FreshQA, Musique, HotpotQA), split by\n\n- ur sufficient context autorater. The results show Contains Answer is generally stricter than LLMEval,\nthough both metrics reveal similar patterns in model behavior.\n\n\nTable 5: **Comparison of evaluation metrics across models and datasets.** We show results for\nchecking whether the response contains one of the ground truth answer strings (\"Contains\"), where\nwe report the % of responses that contain an answer. We compare this to our LLMEval method that\nuses an LLM to evaluate if the response is correct, abstain, or hallucinated, and we report % correct.\n\n\nFreshQA Musique HotpotQA\n\n\nModel Context Contains LLMEval Contains LLMEval Contains LLMEval\n\n\nGemini 1.5 Pro Suff 80.3% 89.1% 60.1% 83.4% 47.6% 67.5%\n\nInsuff 31.4% 41.2% 33.6% 49.5% 34.2% 49.4%\n\nGPT-4 Suff 84.3% 89.1% 64.6% 83.4% 52.4% 71.9%\n\nInsuff 36.3% 44.1% 44.4% 61.4% 46.1% 59.5%\n\nGemma 27B Suff 26.9% 27.5% 10.8% 23.3% 40.7% 64.1%\n\nInsuff 11.8% 6.9% 7.2%",
      " set the rank to 4 and alpha to 8 for all experiments. The models were\nfine-tuned over 2 epochs with a batch size of 16 and a learning rate of 1 _\u00d7_ 10 _[\u2212]_ [5] . We note that the\ntraining was not smooth, and different checkpoints led to very different results. To be systematic,\nwe chose the best checkpoint in terms of Correct % after either 1 or 2 epochs (where for Musique it\nturned out to be after 1 epoch, and for HotPotQA we found that 2 epochs was better).\n\n\nA.3 DATASETS\n\n\nWe sample 500 examples from HotPotQA and Musique-Ans dev sets, following prior work. We use\nall \u2018True Premise\u2019 questions from FreshQA.\n\n\n**Retrieval for HotpotQA.** We adopt the FlashRAG framework (Jin et al., 2024) to implement our\nRetrieval-Augmented Generation (RAG) process. Our retrieval corpus is based on the wiki-18 dataset,\nutilizing \u2018intfloat/e5-base-v2\u2018 from Hugging Face\u2019s model hub as a Dense Retriever [3] . For each query,\nwe retrieved the top 5 documents, which are subsequently concatenated with the query and placed\nwithin a prompt template for inference.\n\n\nTo explore advanced retrieval techniques, we also evaluated the REPLUG (Shi et al., 2023b) method.\nREPLUG enhances the generation quality by prepending each retrieved document individually to the\ninput context and ensembling output probabilities across different passes. The REPLUG method is\nalso implemented based on the FlashRAG framework (Jin et al., 2024).\n\n\n1Available at huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n2We use dev set for HotPotQA since the training set had a much different distribution of sufficient context\nexamples. Namely, we found the train set to be over 88% sufficient context, while the dev set was only 44%.\n3huggingface.co/intfloat/e5-base-v2\n\n\n15\n\n\nPublished as a conference paper at ICLR 2025\n\n\n**Retrieval for FreshQA** We use the urls provided in the FreshQA dataset as retrieval for the\ncontext. We scraped each url and discarded extra HTML content such as headers, footers,",
      " retrieval of specific low-level facts.\n\n\n5: Collect the generated questions to produce _K \u2217_ _N \u2217_ _M_ test questions for the dataset.\n6: **end procedure**\n\n\nFor our evaluation, we set _K_ = _M_ = _N_ = 5 for a total of 125 test questions per dataset. Table 1\nshows example questions for each of the two evaluation datasets.\n\n\n**3.3** **Criteria for Evaluating Global Sensemaking**\n\n\nGiven the lack of gold standard answers to our activity-based sensemaking questions, we adopt\nthe head-to-head comparison approach using an LLM evaluator that judges relative performance\naccording to specific criteria. We designed three target criteria capturing qualities that are desirable\nfor global sensemaking activities.\n\n\nAppendix F shows the prompts for our head-to-head measures computed using an LLM evaluator,\nsummarized as:\n\n\n    - _Comprehensiveness_ . How much detail does the answer provide to cover all aspects and\ndetails of the question?\n\n\n    - _Diversity_ . How varied and rich is the answer in providing different perspectives and insights\n\n     - n the question?\n\n\n    - _Empowerment_ . How well does the answer help the reader understand and make informed\njudgments about the topic?\n\n\nTable 1: Examples of potential users, tasks, and questions generated by the LLM based on short\ndescriptions of the target datasets. Questions target global understanding rather than specific details.\n\n\n**Dataset** **Example activity framing and generation of global sensemaking** **questions**\n\nPodcast _User_ : A tech journalist looking for insights and trends in the tech industry\ntranscripts _Task_ : Understanding how tech leaders view the role of policy and regulation\n_Questions_ :\n1. Which episodes deal primarily with tech policy and government regulation?\n2. How do guests perceive the impact of privacy laws on technology development?\n3. Do any guests discuss the balance between innovation and ethical considerations?\n4. What are the suggested changes to current policies mentioned by the guests?\n5. Are collaborations between tech companies and governments discussed and how?\n\nNews _User_ : Educator incorporating current affairs into curricula\narticles _Task_ : Teaching about health and wellness\n_Questions_ :\n1. What current topics in health can be integrated into health education curricula?\n2. How do news articles address the concepts of preventive medicine and wellness?\n3. Are there examples of health articles that"
    ],
    "response": "FreshQA plays a crucial role in evaluating time-sensitive information by providing up-to-date URLs that support answers to queries, which contributes to high context sufficiency. It has a high percentage of instances labeled as sufficient (77.4%) across its dataset, indicating that it effectively provides relevant context for responding to questions <document id=\"2411.06037_sufficient_context_joren:chunk_7200\">.\n\nIn contrast, HotPotQA and Musique have lower sufficiency percentages, at 46.2% and 44.6%, respectively <document id=\"2411.06037_sufficient_context_joren:chunk_7200\">. This demonstrates that FreshQA is more adept at delivering the necessary context for evaluating time-sensitive queries, making it a standout option compared to HotPotQA and Musique in terms of sufficiency of context for answering questions.",
    "reference": "FreshQA evaluates time-sensitive information and has up-to-date URLs that should support an answer to the queries. It exhibits high sufficient context compared to HotPotQA and Musique, which have lower sufficient context even with 6000 tokens.",
    "answer_correctness": 0.9861227164,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What open-domain QA models use dense retrieval techniques and how do they compare to traditional IR methods?",
    "retrieved_contexts": [
      "Published as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto finetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved significant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\n\n- f questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model\u2019s hidden states. However, their method requires an initial round\n\n- f TF-IDF/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\n",
      "Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set. Until recently, retrieval models were dependent\n\n- n traditional term-based information retrieval (IR) methods, which fail to capture the semantics of\nthe question beyond lexical matching and remain a major performance bottleneck for the task. Recent\nwork on dense retrieval methods instead uses pretrained encoders to cast the question and documents\ninto dense representations in a vector space and relies on fast maximum inner-product search (MIPS)\nto complete the retrieval. These approaches (Lee et al., 2019; Guu et al., 2020; Karpukhin et al.,\n2020) have demonstrated significant retrieval improvements over traditional IR baselines.\n\n\nHowever, such methods remain limited to _simple_ questions, where the answer to the question\nis explicit in a single piece of text evidence. In contrast, _complex_ questions typically involve\naggregating information from multiple documents, requiring logical reasoning or sequential (multihop) processing in order to infer the answer (see Figure 1 for an example). Since the process for\nanswering such questions might be sequential in nature, single-shot approaches to retrieval are\ninsufficient. Instead, iterative methods are needed to recursively retrieve new information at each\nstep, conditioned on the information already at hand. Beyond further expanding the scope of existing\ntextual open-domain QA systems, answering more complex questions usually involves _multi-hop_\n_reasoning_, which poses unique challenges for existing neural-based AI systems. With its practical\n\n\n_\u2217_ Equal Contribution\n1\n[https://github.com/facebookresearch/multihop_dense_retrieval.](https://github.com/facebookresearch/multihop_dense_retrieval)\n\n\n1\n\n\nPublished as a conference paper at ICLR 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: An overview of the multi-hop dense retrieval approach.\n\n\nand research values, multi-hop QA has been extensively studied recently (Talmor & Berant, 2018;\nYang et al., 2018; Welbl et al., 2018) and remains an active research area in NLP (Qi et al., 2019; Nie\net al., 2019; Min et al., 2019; Zhao et al., 2020; Asai",
      ", our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar of text retrieval using learned dense representations, which has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using sparse bag-of-words\nrepresentations (Zhan et al., 2020; Xiong et al.,\n2020; Hofst\u00a8atter et al., 2020; Lin et al., 2020).\nOur experiments largely verify the claims of\nKarpukhin et al. (2020) regarding the effectiveness of their proposed techniques. However, we\narrived at two important additional findings, one\n\n- f which is inconsistent with the original work, the\n\n- ther of which presents an enhancement:\n\n\n1. Focusing on retrieval, we found that the effectiveness of the sparse retrieval (BM25)\nbaseline is higher than values reported by\nKarpukhin et al. (2020). Whereas they reported\nthat dense\u2013sparse hybrid results do not meaningfully improve over dense retrieval alone,\nwe arrived at the opposite conclusion, where\nhybrid techniques yield statistically significant\ngains. We are able to achieve on average\na three-point improvement in top-20 accuracy\n\n  - ver the best DPR results across five standard\nQA test collections.\n\n\n2. Focusing on end-to-end QA effectiveness, we\nexplored different techniques for evidence com\n\n[1Artifact Review and Badging](https://www.acm.org/publications/policies/artifact-review-and-badging-current)\n\n\nbination to extract the final answer span.\nWhereas the original DPR paper only used\nscores from the reader to identify the final\nanswer span, we investigated combining retriever scores and further experimented with\nthe answer span selection technique described\nby Mao et al. (2020). In our best condition,\nwe were able to achieve statistically significant\nimprovements of around three points on exact\nmatch scores over the original DPR implementation, using the same exact models.\n\n\nThe main contribution of this work is the replication of DPR, where our experimental results\nadd a number of important refinements to the\n\n- riginal work. Code associated with our retrieval experiments is packaged in our Pyserini IR\ntoolkit [2] (Lin et al., 2021) and code associated with\n\n-",
      " MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\nexpansion techniques (Rocchio, 1971; Voorhees, 1994; Ruthven & Lalmas, 2003) that are widely used\nin traditional IR systems. In particular, our system is similar in spirit to pseudo-relevance feedback\ntechniques (Croft & Harper, 1979; Cao et al., 2008; Lv & Zhai, 2010), where no additional user\ninteraction is required at the query reformulation stage. Existing studies mainly focus on alleviating\nthe uncertainty of the user query (Collins-Thompson & Callan, 2007) by adding relevant terms from\nthe first round of retrieval, where the retrieval target remains the same throughout the iterative process.\nIn contrast, the query reformulation in our approach aims to follow the multi-hop reasoning chain and\neffectively retrieves different targets at each step. Furthermore, instead of explicitly selecting terms\nto expand the query, we simply concatenate the whole passage and rely on the pretrained encoder to\nchoose useful information from the last retrieved passage.\n\n\n**Other Multi-hop QA Work** Apart from HotpotQA, other multi-hop QA datasets (Welbl et al.,\n2018; Talmor & Berant, 2018; Zhang et al., 2018) are mostly built from knowledge bases (KBs).\nCompared to questions in HotpotQA, questions in these datasets are rather synthetic and less diverse.\nAs multi-hop relations in KBs could be mentioned together in a single text piece, these datasets are\nnot designed for an open-domain setting which necessitates multi-hop retrieval. Existing methods on\nthese datasets either retrieve passages from a small passage pool pruned based on the the specific\ndataset (Sun et al., 2019; Dhingra et al., 2020), or focus on a non-retrieval setting where a compact\ndocuments set is already given (De Cao et al., 2018; Zhong et al., 2019; Tu et al., 2019; Beltagy et al.,\n2020). Compared to these research, our work aims at building an efficient multi-hop retrieval model\nthat easily scales to large real-world corpora that",
      " shown effective in passage\n\n- r dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates\nthe feasibility of full dense retrieval in IR tasks.\nInstead of employing the dual-encoder framework,\nthey introduced a late-interaction operator on top\n\n- f the BERT encoders.\n\n\nDense retrieval for open-domain QA has been\nexplored by Das et al. (2019), who propose to retrieve relevant passages iteratively using reformulated question vectors. As an alternative approach\nthat skips passage retrieval, Seo et al. (2019) propose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input questions efficiently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al. (2019) jointly train\nthe question encoder and reader. Their approach\n\n- utperforms the BM25 plus reader paradigm on\nmultiple open-domain QA datasets in QA accuracy,\nand is further extended by REALM (Guu et al.,\n2020), which includes tuning the passage encoder\nasynchronously by re-indexing the passages during training. The pretraining objective has also\nrecently been improved by Xiong et al. (2020b).\nIn contrast, our model provides a simple and yet\n\n\n\neffective solution that shows stronger empirical performance, without relying on additional pretraining\n\n- r complex joint training schemes.\nDPR has also been used as an important module in very recent work. For instance, extending\nthe idea of leveraging hard negatives, Xiong et al.\n(2020a) use the retrieval model trained in the previous iteration to discover new negatives and construct a different set of examples in each training\niteration. Starting from our trained DPR model,\nthey show that the retrieval performance can be\nfurther improved. Recent work (Izacard and Grave,\n2020; Lewis et al., 2020b) have also shown that\nDPR can be combined with generation models\nsuch as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2019), achieving good performance on\n\n- pen-domain QA and other knowledge-intensive\ntasks.\n\n\n**8** **Conclusion**\n\n\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the"
    ],
    "response": "Open-domain QA models using dense retrieval techniques include Dense Passage Retrieval (DPR) and methods developed by Lee et al. (2019), Guu et al. (2020), and Karpukhin et al. (2020). These models leverage pretrained encoders to create dense representations of questions and documents, significantly improving retrieval effectiveness compared to traditional sparse term-based information retrieval methods like BM25, which primarily rely on lexical matching.\n\nDense retrieval methods offer better semantic matching capabilities, making them more effective for capturing the nuances of questions beyond simple term overlaps. They have shown substantial improvements in accuracy on datasets such as NaturalQuestions and WebQuestions compared to traditional IR methods (<document id=\"2009.12756_multihop_qi:chunk_8550\">).\n\nAdditionally, while dense retrieval excels in handling simple questions, it also extends to complex, multi-hop reasoning tasks, where multiple pieces of evidence are required to answer a question. The iterative nature of some dense methods, as opposed to single-shot retrieval strategies, allows them to better handle these complex tasks (<document id=\"2009.12756_multihop_qi:chunk_450\">).\n\nIn summary, dense retrieval techniques are reshaping open-domain QA by improving retrieval accuracy and handling complex reasoning, marking a significant advancement over traditional sparse retrieval methods (<document id=\"2104.05740_hybrid_retrieval_ma:chunk_450\">; <document id=\"2004.04906_dpr_karpukhin:chunk_9900\">).",
    "reference": "Open-domain QA models that use dense retrieval techniques typically employ dense passage retrieval methods, which better capture semantic matching beyond simple n-gram overlaps. These methods either conduct large-scale pretraining with self-supervised tasks close to the underlying question-passage matching or directly finetune pretrained masked language models using human-labeled question-passage pairs. On single-hop information-seeking QA datasets like NaturalQuestions or WebQuestions, these dense methods have achieved significant improvements over traditional IR methods. Prior to these model-based methods, Das et al. (2019) used RNN encoders for generating dense representations of questions and passages, involving an iterative retrieval process and reformulating the query representation based on reader model's hidden states. However, their method required an initial round of TF-IDF/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Unlike Feldman & El-Yaniv (2019)'s bi-attentive reformulation component applied on token-level representations, our approach uses a straightforward query reformulation strategy by concatenating the original query and previous retrieval as inputs to the query encoder. Together with stronger pretrained encoders and more effective training methods, MDR doubles the accuracy of their system.",
    "answer_correctness": 0.4870025255,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What is the relationship between Selective State Space Models and linear attention approximations like SSMs?",
    "retrieved_contexts": [
      "Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be\nviewed as a degenerate linear SSM.\n\n\n- H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM\nsandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a\nshift-SSM, before the main SSM layer.\n\n\n- Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global\nconvolution (Romero et al. 2021).\n\n\n- RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative\nparallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.\n\n\n- RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \u201cWKV\u201d mechanism involves LTI recurrences and\ncan be viewed as the ratio of two SSMs.\n\n\nOther closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight\nin particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which\nwe view as the most closely related methods to our core selective SSM.\n\n\n4\n\n\n### **3 Selective State Space Models**\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate\nthis mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting\na technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits\nthe memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or\neven MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.",
      "## Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\nAlbert Gu [\u2217][1] and Tri Dao [\u2217][2]\n\n\n1\nMachine Learning Department, Carnegie Mellon University\n\n\n2\nDepartment of Computer Science, Princeton University\nagu@cs.cmu.edu, tri@tridao.me\n\n\n**Abstract**\n\n\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the\nTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,\ngated convolution and recurrent models, and structured state space models (SSMs) have been developed to address\nTransformers\u2019 computational inefficiency on long sequences, but they have not performed as well as attention on important\nmodalities such as language. We identify that a key weakness of such models is their inability to perform content-based\nreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses\ntheir weakness with discrete modalities, allowing the model to _selectively_ propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though this change prevents the use of efficient\nconvolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a\nsimplified end-to-end neural network architecture without attention or even MLP blocks ( **Mamba** ). Mamba enjoys fast\ninference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves\n\n    - n real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art\nperformance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model\n\n    - utperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream\nevaluation.\n\n### **1 Introduction**\n\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged\nas an effective paradigm in modern machine learning. The backbone of these FMs are often _sequence models_, operating on\narbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and\ngenomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al",
      " network architecture incorporating SSMs. It is motivated by the gated\nattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most\nimportantly, its projection _contracts_ the model dimension to reduce the state size of the SSM, while ours _expands_ the\nmodel dimension in order to increase the state size, based on the motivation in Section 3.1.\n\n\n- Mega (Ma et al. 2023) combined the EMA simplification of S4 described above into a hybrid architecture using an efficient\nattention approximation.\n\n\n- H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is\nthe first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later\narchitectures.\n\n\n- Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the\ninput. While sharing the \u201cselection\u201d name, we consider this an architectural modification that is closer to architectural\ngating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective\n\n\n25\n\n\nCopying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones\n(indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0).\n\n\n- RetNet (Y. Sun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to a\nspecial case where the state dimension is _\ud835\udc41_ = 1. Although not framed as such, its recurrence can be viewed as a special\ncase of a linear SSM.\n\n\nIts primary source of improvement is using a linear attention with large _head dimension_, which can be viewed as another\nmethod to perform input-dependent state expansion. Using a larger head dimension in the context of linear attention\nvariants was first done by H3, but not extensively used since this requires a proportional amount of extra computation.\nRetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention\ninstead of convolutions, made feasible by their",
      "\u0394 _\ud835\udc68_ ) =\n1 + exp(Linear( _\ud835\udc65\ud835\udc61_ )) [=] _[ \ud835\udf0e]_ [(\u2212][Linear][(] _[\ud835\udc65][\ud835\udc61]_ [))]\n\n\n= 1 \u2212 _\ud835\udf0e_ (Linear( _\ud835\udc65\ud835\udc61_ ))\n\n\n_\ud835\udc69\ud835\udc61_ = (\u0394 _\ud835\udc68_ ) [\u2212][1] (exp(\u0394 _\ud835\udc68_ ) \u2212 _\ud835\udc70_ ) \u00b7 \u0394 _\ud835\udc69_ = \u2212(exp(\u0394 _\ud835\udc68_ ) \u2212 _\ud835\udc70_ ) = 1 \u2212 _\ud835\udc68_\n\n\n= _\ud835\udf0e_ (Linear( _\ud835\udc65\ud835\udc61_ )) _._\n\n\n27\n\n\nThus the final discrete recurrence (2a) is\n\n\n_\ud835\udc54\ud835\udc61_ = _\ud835\udf0e_ (Linear( _\ud835\udc65\ud835\udc61_ ))\n\n\n_\u210e\ud835\udc61_ = (1 \u2212 _\ud835\udc54\ud835\udc61_ ) _\u210e\ud835\udc61_                          - 1 + _\ud835\udc54\ud835\udc61\ud835\udc65\ud835\udc61_\n\n\nas desired. \n### **D Hardware-aware Algorithm For Selective SSMs**\n\n\nWithout input-dependent selectivity, SSMs can be efficiently implemented as a convolution (Dao, Fu, Saab, et al. 2023; Gu,\nGoel, and R\u00e9 2022), which leverages the fast Fourier transform (FFT) as primitive. With selectivity, SSMs are no-longer\nequivalent to convolution, but we leverage the parallel associative scan. While SSM scans are theoretically efficient\n( _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ ) FLOPs, scaling linear in _\ud835\udc3f_ ), training foundation models with selective SSMs requires them to be efficient on\nmodern hardware (GPUs) as well. We describe how we use _kernel fusion_ and _recomputation_ to make SSM scan fast and\nmemory-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5,\nshowing that it is up to 7\u00d7 times faster than attention at sequence length 32K, and is as memory-efficient as the",
      " done by H3, but not extensively used since this requires a proportional amount of extra computation.\nRetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention\ninstead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.\n\n\n- RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-free\nTransformer (S. Zhai et al. 2021)), another variant of linear attention. Its main \u201cWKV\u201d mechanism involves LTI recurrences\nand can be seen as the ratio of two SSMs.\n\n\nWe also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformer\u2019s\nMHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLP\nblocks.\n\n\n**B.3** **Relationship to RNNs**\n\n\nRNNs and SSMs are broadly related, as they both involve the concepts of _recurrence_ - n a latent _state_ .\n\n\nSeveral older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016),\nand simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities.\nBecause of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs,\nand are thus more powerful in a sense than the family of LTI structured SSMs above. The main differences are:\n\n\n  - They do not use state expansion ( _\ud835\udc41_ = 1) or selective _\ud835\udc69, \ud835\udc6a_ parameters, both of which are important for performance\n(Section 4.6).\n\n\n   They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +\ndiscretization (Theorem 1). The connections to principled SSM theory provides better parameterizations and\ninitializations (Section 3.6).\n\n\nAdditionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991;\nHochreiter"
    ],
    "response": "Selective State Space Models (SSMs) and linear attention approximations, such as those found in architectures like H3 and RetNet, are related through their approach to computational efficiency and managing attention-like mechanisms.\n\nSelective SSMs enhance the traditional SSM framework by allowing model parameters to depend on the input, enabling the model to selectively propagate or forget information based on the current token. This contrasts with traditional attention mechanisms which rely on content-based reasoning. Specific architectures, such as H3, merge features of linear attention and SSMs, generalizing prior linear attention approximations into a more versatile recurrence methodology (<document id=\"2312.00752_rag_roadmap_asai:chunk_28800\">).\n\nAdditionally, RetNet, another notable architecture, utilizes a specific case of SSMs and integrates concepts from linear attention, demonstrating that various frameworks can leverage the underlying principles of SSMs while adopting linear attention mechanisms for improved performance on tasks like language modeling (<document id=\"2312.00752_rag_roadmap_asai:chunk_31050\">).\n\nIn summary, the relationship revolves around the fundamental goal of optimizing sequence modeling through efficient computations and selective information processing.",
    "reference": "Selective State Space Models (SSMs) are related to linear attention approximations such as SSMs through their ability to handle time-varying data efficiently. They can be viewed as an architecture with an SSM sandwiched by two gated connections, which allows for parallelizable computation paths and the use of simpler mechanisms like multi-head attention instead of convolutions.",
    "answer_correctness": 0.5676641623,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What were the speed and memory benchmarks for the Mamba model in comparison to other models?",
    "retrieved_contexts": [
      "Test    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC09 Model Ablations** ) Models with 6M parameters. In\nSaShiMi\u2019s U-Net backbone, there are 8 center blocks operating on\nsequence length 1000, sandwiched on each side by 8 outer blocks on\nsequence length 4000, sandwiched by 8 outer blocks on sequence\nlength 16000 (40 blocks total). The architecture of the 8 center\nblocks are ablated independently of the rest. Note that Transformers\n(MHA+MLP) were not tested in the more important outer blocks\nbecause of efficiency constraints.\n\n\nOuter Center NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\n\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\n\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\n\nMamba MHA+MLP **1.850** 1.37 5.63 58.23 0.62\n\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\n\nMamba Mamba 1.852 **0.94** **6.26** **88.54** **0.52**\n\n\n\nWe benchmark the speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of\n(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6.",
      " small Mamba model outperforms the state-of-the-art (and much larger) GAN-**\n**and diffusion- based models.** A larger model parameter-matched to the baselines further improves on fidelity metrics\ndramatically.\n\n\nTable 5 takes the small Mamba model and investigates combinations of different architectures for the outer stages and\ncenter stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba _>_ S4+MLP _>_\nMHA+MLP in the center blocks.\n\n\n14\n\n\nTable 4: ( **SC09** ) Automated metrics for unconditional generation on\na challenging dataset of fixed-length speech clips. ( _Top to Bottom_ )\nAutoregressive baselines, non-autoregressive baselines, Mamba, and\ndataset metrics.\n\n\nModel Params NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nSampleRNN 35.0M 2.042 8.96 1.71 3.02 1.76\nWaveNet 4.2M 1.925 5.08 2.27 5.80 1.47\n\nSaShiMi 5.8M 1.873 1.99 5.13 42.57 0.74\n\n\nWaveGAN 19.1M   - 2.03 4.90 36.10 0.80\n\nDiffWave 24.1M   - 1.92 5.26 51.21 0.68\n\n+ SaShiMi 23.0M     - 1.42 5.94 69.17 0.59\n\n\n**Mamba** 6.1M **1.852** 0.94 6.26 88.54 0.52\n\n**Mamba** 24.3M 1.860 **0.67** **7.33** **144.9** **0.36**\n\n\nTrain    -    - 0 _._ 00 8 _._ 56 292 _._ 5 0 _._ 16\n\nTest    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC",
      "HA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|2<br>|3|4|5|\n\n\nTable 2: ( **Induction Heads** .) Models are trained on sequence length 2 [8] =\n256, and tested on increasing sequence lengths of 2 [6] = 64 up to 2 [20] =\n1048576. Full numbers in Table 11.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: ( **Scaling Laws** .) Models of size \u2248 125 _\ud835\udc40_ to \u2248 1 _._ 3 _\ud835\udc35_ parameters, trained on the Pile. Mamba scales better than all other\nattention-free models and is the first to match the performance of a very strong \u201cTransformer++\u201d recipe that has now become standard,\nparticularly as the sequence length grows.\n\n\nOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly\nbetter than the others; also note that all attention models were only tested up to sequence length 2 [14] = 16384 due to\nmemory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the findings in Poli et al. (2023).\n\n\n**4.2** **Language Modeling**\n\n\nWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on both\npretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to mirror GPT3\nspecifications. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown\net al. (2020). All training details are in Appendix E.2.\n\n\n**",
      " a total of 10 _\ud835\udc35_ tokens.\n\n\n**Results.** Figure 5 ( _Left_ ) shows that Mamba\u2019s pretraining perplexity improves smoothly with model size, and that Mamba\nscales better than both HyenaDNA and Transformer++. For example, at the largest model size of \u2248 40 _\ud835\udc40_ parameters, the\ncurve shows that **Mamba can match the Transformer++ and HyenaDNA models with roughly** 3\u00d7 **to** 4\u00d7 **fewer**\n**parameters** .\n\n\n**4.3.2** **Scaling: Context Length**\n\n\nIn the next DNA experiment, we investigate the scaling properties of models with respect to sequence length. We only\ncompare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at longer sequence\nlengths. We pretrain models on sequence lengths 2 [10] = 1024, 2 [12] = 4096, 2 [14] = 16384, 2 [16] = 65536, 2 [18] = 262144,\n2 [20] = 1048576. We fix a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models were trained for 20 _\ud835\udc3e_\ngradient steps for a total of \u2248 330 _\ud835\udc35_ tokens. The longer sequence lengths used sequence length warmup similar to (Nguyen,\nPoli, et al. 2023).\n\n\n**Results.** Figure 5 ( _Right_ ) shows that **Mamba is able to make use of longer context even up to extremely long**\n**sequences of length 1M**, and its pretraining perplexity improves as the context increases. On the other hand, the\nHyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on properties of the\nselection mechanism. In particular, LTI models cannot selectively ignore information; from a convolutional perspective, a\nvery long convolution kernel is aggregating all information across a long sequence which may be very noisy. Note that\nwhile HyenaDNA claims to improve with longer context, their results do not control for computation time.\n\n\n**4.3.3** **Synthetic Species Classification**\n\n\nWe evaluate models on a downstream task of classifying between 5 different species by",
      "Avsec et al. 2021). We investigate Mamba as\na FM backbone for pretraining and fine-tuning in the same setting as recent works on long-sequence models for DNA\n(Nguyen, Poli, et al. 2023). In particular, we focus on two explorations of scaling laws across model size and sequence\nlength (Figure 5), and a difficult downstream synthetic classification task requiring long context (Figure 6).\n\n\nFor pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training and\nmodel details (see also Appendix E.2). For the dataset, we largely follow the setup of HyenaDNA (Nguyen, Poli, et al. 2023),\nwhich uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5 billion tokens (DNA base\npairs) in the training split.\n\n\n12\n\n\nFigure 5: ( **DNA Scaling Laws** .) Pretraining on the HG38 (human genome) dataset. ( _Left_ ) Fixing short context length 2 [10] = 1024 and\nincreasing size from \u2248 200 _\ud835\udc3e_ to \u2248 40 _\ud835\udc40_ parameters, Mamba scales better than baselines. ( _Right_ ) Fixing model size and increasing sequence\nlengths while keeping tokens/batch and total training tokens fixed. Unlike baselines, the selection mechanism of Mamba facilitates\nbetter performance with increasing context length.\n\n\n**4.3.1** **Scaling: Model Size**\n\n\nIn this experiment, we investigate the scaling properties of genomics foundation models with various model backbones\n(Figure 5 _Left_ ).\n\n\n**Training.** To advantage the baselines, we train on a short sequence length of 1024; as shown in Section 4.3.2, we expect\nresults to favor Mamba even more at longer sequence lengths. We fix a global batch size of 1024, for a total of 2 [20] \u2248 1 _\ud835\udc40_\ntokens per batch. Models were trained for 10 _\ud835\udc3e_ gradient steps for a total of 10 _\ud835\udc35_ tokens.\n\n\n**Results.** Figure 5 ( _Left_ ) shows that Mamba\u2019s pretraining perplexity improves smoothly with model size, and that Mamba\nscales better than both Hy"
    ],
    "response": "The Mamba model demonstrated significant performance advantages in speed and memory benchmarks compared to other models:\n\n1. **Inference Throughput**: Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, allowing it to utilize much higher batch sizes due to the absence of a KV cache.\n\n2. **SSM Scan Efficiency**: The SSM scan operation of Mamba is faster than the best-known attention-based implementation (FlashAttention-2) beyond a sequence length of 2K, and it is up to 20-40\u00d7 faster than standard implementations in PyTorch.\n\n3. **Model Performance**: In terms of generated metrics, Mamba outperformed several architectures at various sizes. For instance, a smaller Mamba model with 6.1M parameters achieved a negative log likelihood (NLL) of 1.852 and a FID of 0.94, outperforming larger models like SampleRNN (35.0M parameters) and WaveNet (4.2M parameters) in multiple metrics <document id=\"2312.00752_rag_roadmap_asai:chunk_16200\">.\n\nIn summary, Mamba not only provides higher throughput compared to Transformers but also demonstrates better performance metrics on a range of established models, highlighting its efficiency in both speed and resource usage <document id=\"2312.00752_rag_roadmap_asai:chunk_15750\">.",
    "reference": "The Mamba model achieved 4-5\u00d7 higher inference throughput than a Transformer of similar size, as it can use much higher batch sizes without the KV cache. It was faster than the best attention implementation that we know of (FlashAttention-2) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation in PyTorch.",
    "answer_correctness": 0.6103427236,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What is RAFT and how does it improve the performance of pre-trained Large Language Models on domain-specific RAG tasks?",
    "retrieved_contexts": [
      " with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does not know, it can still refer to the context.\n\n\n4.1 Results\n\n\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\n\n- utperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes/no question, we don\u2019t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.\n\n\nOverall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its\nanswering style not aligning with the ground truth. By applying domain-specific tuning,\nwe significantly enhance its performance. This process enables the model to learn and adopt\nthe appropriate style of answering. However, introducing RAG to a domain-specifically\nfine-tuned (DSF) model doesn\u2019t invariably lead to better outcomes. This might indicate that\nthe model lacks training in context processing and extracting useful information from it. By\nincorporating our method, RAFT, we train the model not only to match its answering style\nwith that required but also to improve its document processing capabilities. Consequently,\n\n- ur approach outperforms all others.\n\n\n4.2 Effect of CoT\n\n\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\nin enhancing the model\u2019s performance. As indicated in Table 2, simply providing the answer\nto a question",
      " LLM without the correct corresponding context at times can be\nbeneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we maintain this format by providing the golden document with four distractors.\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of\ntraining data without the golden documents in the context proves to be advantageous.\n\n\n**5** **RAFT Generalizes to Top-K RAG**\n\n\nWe now study another important problem: How does the number of distractor documents\nin RAFT affect the model\u2019s performance when augmented with top-k RAG results during\nevaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text\n(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is\nparticularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to\nensure high recall. Such a scenario necessitates the model to have the ability to discern and\ndisregard irrelevant content, focusing solely on pertinent information.\n\n\n5.1 Making Model Robust to top-K RAG\n\n\nTo tackle the challenge of enhancing large language models\u2019 (LLMs) ability to sift through\nirrelevant text within the retrieval pipeline, our analysis revealed that training solely with\ngolden (highly relevant) documents can inadvertently diminish the model\u2019s ability to discern and disregard irrelevant information. To address this, our algorithm, RAFT, adopts\na strategy that integrates golden documents with a mix of irrelevant ones. This method\n- logy prompts us to investigate the ideal fraction of distractor (irrelevant) documents to\nincorporate throughout the training process and to assess how well this training approach\nadapts to different volumes of documents encountered by the Retrieval-Augmented Generation (RAG) during the test phase. Our aim is to refine the balance between relevant\nand irrelevant information to strenghten the model\u2019s efficiency in identifying and utilizing\npertinent content. Notice that Sec 4.4 looked what what P% of training data should include\ndistractors, while in this section, we study test-time scenarios.\n\n\n**Training with Distractor Documents** To enhance the robustness of LLMs against irrelevant\ntext in retrieved documents, we adopted a fin",
      "Preprint, Under Review\n\n## **RAFT: Adapting Language Model to Domain Specific RAG**\n\n\n\n**Tianjun Zhang** _[\u2217]_\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n{tianjunz}@berkeley.edu\n\n```\n\n\n**Shishir G. Patil, Naman Jain, Sheng Shen**\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\n\n```\n\n\n**Matei Zaharia, Ion Stoica, Joseph E. Gonzalez**\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n {matei,istoica,jegonzal}@berkeley.edu\n\n```\n\n**Abstract**\n\n\nPretraining Large Language Models (LLMs) on large corpora of textual\ndata is now a standard paradigm. When using these LLMs for many\ndownstream applications, it is common to additionally incorporate new information into the pretrained model either through RAG-based-prompting,\n\n    - r finetuning. However, the best methodology to incorporate information\nremains an open question. In this paper, we present Retrieval Augmented\nFine Tuning (RAFT), a training recipe which improves the model\u2019s ability\nto answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don\u2019t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT\u2019s chain-of-thought-style response helps improve\nthe model\u2019s ability to reason. In domain specific RAG, RAFT consistently\nimproves the model\u2019s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n\n\n**1** **Introduction**\n\n\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized\ndomains",
      " in\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\nIn such scenarios, typically, the LLM is paired with retriever which retrieves \u2018k\u2019 documents\n(or specific segments of the document) which are appended to the users\u2019 prompt. It is\n\n- nly through these documents retrieved that the LLM gains access to \u201cdomain-specific\ninformation\u201d. As a result, we argue that the LLM\u2019s performance in these settings, where it\nis trained as a general-purpose LLM is largely dependent on the quality of the retriever and\nhow accurately the retriever can identify the most relevant piece of information.\n\n\n**Domain-Specific Open-Book Exam** In this paper, we focus on the narrower but increasingly popular domain than the general open book exam, which we call the domain-specific\n\n- pen-book exam. Here, we know apriori the domain in which the LLM will be tested. The\nLLM can respond to the users\u2019 prompt using use any and all information from this specific\ndomain, which it has been fine-tuned on. Examples of domain specific examples include\nenterprise documents, code repositories belonging to an organization, etc. In all these\nscenarios, the LLM will be used to respond to the questions, whose answers can be found\nwithin a collection of documents. The retrieval technique itself has little to no-impact on the\nmechanism (though it may impact the accuracy). This paper studies the domain-specific\n\n- pen-book setting and how to adapt a pretrained LLM to this specific domain, including\nhow to make it more robust to a varying number of retrieved documents and distractors.\n\n\n**3** **RAFT**\n\n\nIn this section, we present RAFT, a novel way of training LLMs for domain-specific openbook exams. We first introduce the classical technique of supervised fine-tuning, followed\nwith the key takeaways from our experiments. Then, we introduce RAFT, a modified\nversion of general instruction tuning. Lastly, we provide an overview of the experiments to\nexpect in the later sections.\n\n\n**Supervised Finetuning**\n\n\nConsider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The\nformulation consists of the Dataset ( _D_ ) from which a set of Question ( _Q_ ) and corresponding\nanswer ( _A_ ) pairs are derived or already available. In the",
      " question-answer pairs while referencing the documents in a simulated\nimperfect retrieval setting \u2014 thereby effectively preparing for the open-book exam setting.\n\n\nanswering questions. However, RAG based in-context learning methods fail to leverage\nthe learning opportunity afforded by the fixed domain setting and early access to the test\ndocuments. Alternatively, supervised fine-tuning offers the opportunity to learn more\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\ndocuments at test time (don\u2019t incorporate RAG) or fail to account for the imperfections in\nretrieval process during training.\n\n\nWe can draw an analogy to an open-book exam. Existing in-context retrieval methods are\nequivalent to taking an open-book exam without studying. Alternatively, existing finetuning based approaches implement \u201cstudying\" by either directly \u201cmemorizing\" Xiong\net al. (2023) the input documents or answering practice questions Wang et al. (2022) without\nreferencing the documents. While these approaches leverage in-domain learning they fail to\nprepare for the open-book nature of the test setting.\n\n\nIn this paper, we study how to combine instruction fine-tuning (IFT) with retrieval augmented generation (RAG). We propose a novel adaptation strategy \u2013 Retrieval-Augmented\nFine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both\nincorporate domain knowledge while also improving in-domain RAG performance. RAFT\naims to not only enable models to learn domain-specific knowledge through fine-tuning,\nbut also to ensure robustness against distracting retrieved information. This is achieved\nby training the models to understand the dynamics between the question (prompt), the\ndomain-specific documents retrieved, and the right answer. Going back to our analogy to\nthe open book exam, our approach is analogous to studying for an open-book exam by\nrecognizing relevant, and irrelevant retrieved documents.\n\n\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate\nanswer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic\n(2023), and in the presence of distractor documents ( _Dk_ ). We explain the methodology in\nSection 3 and analyze the sensitivity to the number of distractor documents ("
    ],
    "response": "RAFT, or Retrieval-Augmented Fine Tuning, is a training methodology designed to enhance the performance of pre-trained Large Language Models (LLMs) on domain-specific Retrieval-Augmented Generation (RAG) tasks. Here are the key aspects of RAFT and its impact on LLMs:\n\n1. **Domain-Specific Training**: RAFT fine-tunes LLMs to better perform in specific domains by training them to effectively process domain-specific documents during the question answering process. This allows the model to access and utilize domain-specific information more proficiently, leading to improved performance in tasks such as HotpotQA and PubMed QA (<document id=\"2403.10131_raft_zhang:chunk_0\">).\n\n2. **Robustness to Distractors**: RAFT incorporates distractor documents during training\u2014documents that are not relevant to the question at hand. This approach helps the model learn to identify and disregard irrelevant information while focusing on the pertinent content needed to generate accurate answers. By carefully balancing the training between relevant and irrelevant information, RAFT enhances the model's ability to sift through distracting content during the retrieval process (<document id=\"2403.10131_raft_zhang:chunk_6750\">).\n\n3. **Integration of Chain-of-Thought (CoT)**: The methodology also employs Chain-of-Thought reasoning, which aids the model in articulating responses that demonstrate logical coherence. This helps improve the model\u2019s reasoning capabilities, making it more effective in answering questions based on the context provided by the retrieved documents (<document id=\"2403.10131_raft_zhang:chunk_900\">).\n\n4. **Empirical Performance Improvements**: RAFT has consistently shown significant performance gains over existing models in various datasets, with improvements as high as 76.35% on certain evaluations compared to previous state-of-the-art methods. This suggests that RAFT not only enhances the model's knowledge integration but also bolsters its ability to handle complexity in domain-specific queries (<document id=\"2403.10131_raft_zhang:chunk_4500\">).\n\nIn summary, RAFT enhances pre-trained LLMs by integrating domain-specific fine-tuning with robust strategies for context processing, leading to improved performance in domain-specific RAG tasks.",
    "reference": "RAFT (Retrieval Augmented Fine Tuning) is a training recipe that improves the model's ability to answer questions in 'open-book' in-domain settings. In training RAFT, given a question and a set of retrieved documents, the model is trained to ignore distractor documents and cite verbatim the right sequence from relevant documents to help answer the question. This method consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets when used in domain-specific RAG tasks.",
    "answer_correctness": 0.8210708497,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "How does the Flare framework utilize recursive retrieval to enhance information retrieval?",
    "retrieved_contexts": [
      " for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n\n\n_B. Recursive Retrieval_\n\n\nRecursive retrieval is often used in information retrieval and\n\nNLP to improve the depth and relevance of search results.\nThe process involves iteratively refining search queries based\n\n- n the results obtained from previous searches. Recursive\nRetrieval aims to enhance the search experience by gradually converging on the most pertinent information through a\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\nthe retrieval process and refines the CoT with the obtained\nretrieval results. ToC [57] creates a clarification tree that\nsystematically optimizes the ambiguous parts in the Query. It\ncan be particularly useful in complex search scenarios where\nthe user\u2019s needs are not entirely clear from the outset or where\nthe information sought is highly specialized or nuanced. The\nrecursive nature of the process allows for continuous learning\nand adaptation to the user\u2019s requirements, often resulting in\nimproved satisfaction with the search outcomes.\nTo address specific data scenarios, recursive retrieval and\nmulti-hop retrieval techniques are utilized together. Recursive\n\n\n\nretrieval involves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\n\n\n_C. Adaptive Retrieval_\n\n\nAdaptive retrieval methods, exemplified by Flare [24] and\nSelf-RAG [25], refine the RAG framework by enabling LLMs\nto actively determine the optimal moments and content for\nretrieval, thus enhancing the efficiency and relevance of the\ninformation sourced.\n\nThese methods are part of a broader trend wherein\nLLMs employ active judgment in their operations, as seen\nin model agents like AutoGPT, Toolformer, and GraphToolformer [107]\u2013[109]. Graph-Toolformer, for instance, divides its retrieval process into distinct steps where LLMs\nproactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive\nstance allows LLMs to decide when to search",
      "-time\nretrieval augmented LMs, which is the most relevant area to this paper.\n\n\n**Iterative and adaptive retrieval** Iterative retrieval and refinement has been studied in both\ntext and code generation tasks (Peng et al., 2023;\nZhang et al., 2023; Zemlyanskiy et al., 2022; Yu\net al., 2023). FLARE differs from these methods in\nthe granularity of generation and retrieval strategies.\nAdaptive retrieval has been studied in single-time\nretrieval scenarios based on either question popularity or generation probabilities (Mallen et al.,\n2022; Li et al., 2023), while we focus on long-form\ngeneration requiring active information access.\n\n\n**Browser-enhanced LMs** WebGPT (Nakano\net al., 2021) and WebCPM (Qin et al., 2023) train\nLMs to interact with browser to enhance factuality\nusing reinforcement learning or supervised training where multiple queries can be triggered before\ngeneration. FLARE is built on text-based retrievers\nbut can be combined with a browser to potentially\nimprove retrieval quality.\n\n\n\n**8** **Conclusion**\n\n\nTo aid long-form generation with retrieval augmentation, we propose an active retrieval augmented generation framework that decides when\nand what to retrieve during generation. We implement this framework with forward-looking active\nretrieval that iteratively uses the upcoming sentence\nto retrieve relevant information if it contains low\nconfidence tokens and regenerates the next sentence. Experimental results on 4 tasks/datasets\ndemonstrate the effectiveness of our methods. Fu\nture directions include better strategies for active\nretrieval and developing efficient LM architectures\nfor active information integration.\n\n\n**9** **Limitations**\n\n\nWe also conduct experiments on Wizard of\nWikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,\n\n2019), and found that FLARE did not provide significant gains. Wizard of Wikipedia is a knowledgeintensive dialogue generation dataset where the out\n_\u223c_\nput is relatively short ( 20 tokens on average) so\nretrieving multiple disparate pieces of information\nmight not be necessary. ELI5 (Fan et al., 2019)\nis a long-form QA dataset requiring in-depth answers to open-ended questions. Due to issues mentioned in Krishna et al. (2021) such as difficulties\n\n-",
      " effective method is to distill the more\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\nbe coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT [27], aligns the\nscoring functions between Retriever and Generator using KL\ndivergence.\n\n\nV. AUGMENTATION PROCESS IN RAG\n\n\nIn the domain of RAG, the standard practice often involves\na singular (once) retrieval step followed by generation, which\ncan lead to inefficiencies and sometimes is typically insufficient for complex problems demanding multi-step reasoning,\nas it provides a limited scope of information [105]. Many\nstudies have optimized the retrieval process in response to this\nissue, and we have summarised them in Figure 5.\n\n\n_A. Iterative Retrieval_\n\n\nIterative retrieval is a process where the knowledge base\nis repeatedly searched based on the initial query and the text\ngenerated so far, providing a more comprehensive knowledge\n\n\n11\n\n\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\n\n\n\nbase for LLMs. This approach has been shown to enhance\nthe robustness of subsequent answer generation by offering\nadditional contextual references through multiple retrieval\niterations. However, it may be affected by semantic discontinuity and the accumulation of irrelevant information. ITERRETGEN [14] employs a synergistic approach that leverages \u201cretrieval-enhanced generation\u201d alongside \u201cgenerationenhanced retrieval\u201d for tasks that necessitate the reproduction\n\n- f specific information. The model harnesses the content\nrequired to address the input task as a contextual basis for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n\n\n_B. Recursive Retrieval_\n\n\nRecursive retrieval is often used in information retrieval and\n\nNLP to improve the depth and relevance of search results",
      " **RE** trieval augmented generation\n( **FLARE** ), as illustrated in Figure 1. FLARE iteratively generates _a temporary next sentence_, use\nit as the query to retrieve relevant documents _if it_\n_contains low-probability tokens_ and regenerate the\nnext sentence until reaches the end.\n\n\nFLARE is applicable to any existing LMs at\ninference time without additional training. Con\n\nsidering the impressive performance achieved by\nGPT-3.5 (Ouyang et al., 2022) on a variety of\ntasks, we examine the effectiveness of our meth\n- ds on text-davinci-003. We evaluate FLARE\n\n- n 4 diverse tasks/datasets involving generating\nlong outputs, including multihop QA (2WikiMultihopQA), commonsense reasoning (StrategyQA),\nlong-form QA (ASQA), and open-domain summarization (WikiAsp) (Ho et al., 2020; Geva et al.,\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\nOver all tasks, FLARE achieves superior or competitive performance compared to single-time and\nmulti-time retrieval baselines, demonstrating the\neffectiveness and generalizability of our method.\n\n\n**2** **Retrieval Augmented Generation**\n\n\nWe formally define single-time retrieval augmented\ngeneration and propose the framework of active\nretrieval augmented generation.\n\n\n**2.1** **Notations and Definitions**\n\n\nGiven a user input _**x**_ and a document corpus _D_ =\n_{_ _**d**_ _i}_ _[|D|]_ _i_ =1 [(such as all Wikipedia articles), the goal of]\nretrieval augmented LMs is to generate the answer\n_**y**_ = [ _**s**_ 1 _,_ _**s**_ 2 _, ...,_ _**s**_ _m_ ] = [ _w_ 1 _, w_ 2 _, ..., wn_ ] containing\n_m_ sentences or _n_ tokens leveraging information\nretrieved from the corpus.\nIn retrieval augmented LM, the LM typically\npairs with a retriever that can retrieve a list of\ndocuments _D_ _**q**_ = ret( _**q**_ ) for a query _**q**_ ; the LM\nconditions on both the user input",
      "**_ _<_ 1 = _\u2205_ ), and the user input is used as the\ninitial query ( _**q**_ 1 = _**x**_ ). Given retrieved documents\n_D_ _**q**_ _t_, LMs continually generate the answer until the\nnext retrieval is triggered or reaches the end:\n\n\n_**y**_ _t_ = LM([ _D_ _**q**_ _t,_ _**x**_ _,_ _**y**_ _<t_ ]) _,_\n\n\nwhere _**y**_ _t_ represents the generated tokens at the current step _t_, and the input to LMs is the concatenation of the retrieved documents _D_ _**q**_ _t_, the user input\n_**x**_, and the previous generation _**y**_ _<t_ . We discard\npreviously retrieved documents _\u222at\u2032<tD_ _**q**_ _t\u2032_ and only\nuse the retrieved documents from the current step\nto condition the next generation to prevent reaching\nthe input length limit of LMs.\n\n\n**3** **FLARE: Forward-Looking Active**\n**REtrieval Augmented Generation**\n\n\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should reflect\nthe intents of future generations. We propose two\nforward-looking active retrieval augmented generation (FLARE) methods to implement the active\nretrieval augmented generation framework. The\nfirst method prompts the LM to generate retrieval\nqueries when necessary while generating the answer using retrieval-encouraging instructions, denoted as FLAREinstruct. The second method directly\nuses the LM\u2019s generation as search queries, denoted\nas FLAREdirect, which iteratively generates the next\nsentence to gain insight into the future topic, and\nif uncertain tokens are present, retrieves relevant\ndocuments to regenerate the next sentence.\n\n**3.1** **FLARE with Retrieval Instructions**\n\nInspired by Toolformer (Schick et al., 2023), a\nstraightforward way of expressing information\nneeds for retrieval is to generate \u201c[Search(query)]\u201d\nwhen additional information is needed (Schick\net al., 2023), e.g., \u201cThe colors on the flag of\nGhana have"
    ],
    "response": "The Flare framework enhances information retrieval through recursive retrieval by employing an iterative process that refines search queries based on previously obtained results. This method allows for a feedback loop where the framework improves the relevance and depth of search results over time. Specifically, Flare generates a temporary next sentence and uses it as a query to retrieve relevant documents. If this sentence contains low-probability tokens\u2014indicative of uncertainty\u2014it retrieves additional information to refine the generated content. This iterative approach helps in continuously adapting to the user's requirements and contributes to more satisfying search outcomes <document:2312.10997_rag_survey_gao:chunk_10800> <document:2305.06983_active_rag_jiang:chunk_1800>.\n\nFurthermore, Flare integrates recursive retrieval with the capability to summarize lengthy documents before conducting secondary searches, enhancing the relevance and accuracy of the retrieved information <document:2312.10997_rag_survey_gao:chunk_10800>. This structured index process enables it to effectively handle complex queries that often require multi-step reasoning, thereby ensuring a more comprehensive retrieval experience <document:2312.10997_rag_survey_gao:chunk_10350>.",
    "reference": "The Flare framework utilizes recursive retrieval by iteratively refining search queries based on previous results, aiming to enhance the depth and relevance of search outcomes through a feedback loop. This process involves summarizing sections of a document or lengthy PDF before performing a retrieval based on this summary, followed by a secondary refinement within the document.",
    "answer_correctness": 0.826046714,
    "context_precision": 0.8041666666,
    "context_recall": 1.0
  },
  {
    "user_input": "What is an example of a multi-hop reasoning task in MMLU high-school Physics?",
    "retrieved_contexts": [
      " 4 trials, the model scoring agrees with human ratings 97%, 98%, 99% and 99%\n\n- f the time.\n\n\nD PROMPTS AND FEW SHOT EXAMPLES\n\n\nD.1 STEM\n\n\nFor MMLU high-school Physics and Chemistry, we first prompt the model to generate the first\nprinciples behind the question. Using the generated first principles, we further prompt the model to\ngenerate the final answer through few-shot demonstrations The prompt generating first principles is\nshown in Table 7 for MMLU high-school Physics and Chemistry.\n\n\n15\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nMMLU Physics/Chemistry Final Answer Prompt\n\n\nYou are an expert at Physics/Chemistry. You are given a\nPhysics/Chemistry problem and a set of principles involved in\nsolving the problem. Solve the problem step by step by following the\nprinciples. Here are a few examples:\n\n\nQuestion: _<_ Question Example1 _>_\nPrinciples: _<_ Principles Example1 _>_\nAnswer: _<_ Answer Example1 _>_\n\n...\nQuestion: _<_ Question Example5 _>_\nPrinciples: _<_ Principles Example5 _>_\nAnswer: _<_ Answer Example5 _>_\nQuestion: _<_ Question _>_\nPrinciples: _<_ Principles _>_\nAnswer:\n\n\nTable 8: Prompt of querying the model for final answer with first principles behind the question in\nMMLU high-school Physics and Chemistry.\n\n\nAfter extracting the first principles of solving a particular question, we formulate the prompt in\nTable 8 to query the model for the final answer.\n\n\nTables 9-10 show one demonstration exemplar of Question-Principles-Answer triplets for MMLU\nhigh-school Physics and Chemistry, respectively. For GSM8K, given the simplicity of the principles,\nwe directly combine the principles and the solution in the demonstration exemplar. Table 11 shows\nthe exemplar we used in the paper.\n\n\nD.2 KNOWLEDGE QA\n\n\nWe use the following prompting in Table 12 to demonstrate to the LLM on asking a step-back question\nfor TimeQA and SituatedQA including up to 5 exemplar demonstrations of pairs of Original Question\nand Step-back Question.\n\n\nTable 13 shows 5 exemplars from the Train split of TimeQA and SituatedQA as demonstrations of\nasking step-back questions.\n\n\nThe step-back",
      ".\n\n\nTable 24: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Math Error** . As highlighted, the math error leads to directly a wrong answer with a factor of 2 off.\n\n\n28\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nQuestion Two spheres of net charge +5e and -6e briefly come into contact. Afterward,\nwhich of the following is a possible combination of net charges for the two\nspheres?\n0) +3e and -4e\n1) +1.5e and -2.5e\n2) Both of the above\n3) None of the above\n\nPrinciples Law of conservation of charge: the total charge in a closed system remains\n\nconstant.\nCoulomb\u2019s Law: the mutual repulsion of like charges demands that the charges\nbe as far apart as possible.\n\nAnswer Using the Principles of Conservation of charge, we can solve the problem as\nfollowing:\n\n\nStep 1: Apply Conservation of charge to find out the possible combination of\nnet charges for the two spheres.\nThe total charge of the two spheres before they come into contact is +5e - 6e =\n\n          - 1e. After they come into contact, the total charge of the two spheres must still\nbe -1e.\n\n\nStep 2: Find out the possible combination of net charges for the two spheres.\nThe possible combination of net charges for the two spheres are:\n(1) +3e and -4e\n(2) +1.5e and -2.5e\n\n\nTherefore, the correct answer is 2) Both of the above.\n\n\nTable 25: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Principle Error** . In this case, Coulomb\u2019s Law is not needed for solving this problem. Instead, the\nCharge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule\n\n- ut option (2) of fractional charges.\n\n\n29\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nQuestion The circumference of a helium-filled balloon is measured for three different\nconditions: at room temperature, after being in a warm oven for 30 minutes,\nand after being in a freezer for 30 minutes. A student plotting the circumference\ncubed _C",
      " as well as emergent abilities (Wei et al., 2022a) such as multi-step reasoning (Wei et al.,\n2022b; Zhou et al., 2022) and instruction following (Mishra et al., 2022b; Wei et al., 2021).\n\n\nFigure 1: Strong Performance of STEP-BACK PROMPTING: our proposed Abstraction-and-Reasoning\nscheme leads to a substantial improvement in a wide range of challenging tasks in STEM, Knowledge\nQA and Multi-Hop Reasoning requiring complex (often multi-hop) reasoning.\n\n\n_\u2217_ Equal Contribution\n\n\n1\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nDespite the great advancements, complex multi-step reasoning remains challenging for even the state\n- f-the-art LLMs. Lightman et al. (2023) show that process-supervision with step-by-step verification\nis a promising remedy to improve the correctness of intermediate reasoning steps. Techniques such\nas Chain-of-Thought (Wei et al., 2022b) were introduced to produce a coherent series of intermediate\nreasoning steps to increase the success rate of following the right decoding path. Inspired by the\nfact that when faced with challenging tasks humans often step back and do abstractions to arrive at\nhigh-level principles to guide the process, we propose STEP-BACK PROMPTING to ground reasoning\n\n- n abstractions to reduce the chance of making errors in the intermediate reasoning steps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of STEP-BACK PROMPTING with two steps of Abstraction and Reasoning\nguided by concepts and principles. _Top_ : an example of MMLU high-school physics (Hendrycks et al.,\n2020) where the first principle of Ideal Gas Law is retrieved via abstraction. _Bottom_ : an example\nfrom TimeQA (Chen et al., 2021) where the high-level concept of education history is a result of the\nabstraction. _Left_ : PaLM-2L (Anil et al., 2023) fails to answer the original question. Chain-of-Thought\nprompting (Wei et al., 2022b; Kojima et al., 2022) ran into errors during intermediate reasoning\nsteps (highlighted as red). _Right_ : PaLM-2L (Anil et al., 2023) successfully answers the question via",
      " by the friction\nbetween the object and the air molecules. The greater the surface area of the\n\n           - bject, the greater the air resistance. The tennis ball has a large surface area, so\nit experiences a lot of air resistance. This air resistance slows down the tennis\nball and causes the difference between the predicted velocity and the velocity\nmeasured by the photogate.\nTherefore, the correct answer is 0) The ball changes its shape while falling.\n\n\nTable 27: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Reasoning Error** . In this case, the model was able to correctly identify air resistance is the cause of\nthe velocity difference, but made the error at the final reasoning step of deriving the right answer.\n\n\n31\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nMMLU-Physics **Original Question** : What happens to the pressure, P, of an ideal gas if\nthe temperature is increased by a factor of 2 and the volume is increased\nby a factor of 8 ?\n0) P decreases by a factor of 16.\n1) P decreases by a factor of 4.\n2) P decreases by a factor of 2.\n3) P increases by a factor of 4.\n**Original Answer [Incorrect]** : The correct answer is 0. The ideal gas\nlaw is PV = nRT. If the temperature is increased by a factor of 2 and the\nvolume is increased by a factor of 8, then the pressure will decrease by a\nfactor of 16.\n**Stepback Question** : What are the physics principles behind this\nquestion?\n**Principles** :\nIdeal gas law: PV = nRT, where P is the pressure, V is the volume, n is\nthe number of moles, R is the gas constant, and T is the temperature.\n**Final Answer** : Using the Principles of Ideal gas law, we can solve the\nproblem as following:\nStep 1: Apply Ideal gas law to find out the pressure of an ideal gas. The\nideal gas law states that PV = nRT, where P is the pressure, V is the\nvolume, n is the number of moles, R is the gas constant, and T is the\ntemperature.\n\n\nStep 2: Apply Ideal gas law to find out the pressure of",
      " PaLM-2L model is queried with zero-shot\nCoT prompting (Kojima et al., 2022): \u201c _Let\u2019s think step by step_ \u201d is appended to the question.\nFor 1-shot, One demonstration example of a question and answer pair is provided in the\nprompt, where the answer is in the style of CoT (Wei et al., 2022b).\n\n    - **PaLM-2L + TDB** : Zero-shot prompting with \u201c _Take a deep breath and work on this problem_\n_step-by-step._ \u201d (Yang et al., 2023) prepended to the question.\n\n    - **PaLM-2L + RAG** : For Sections 5 and 6, we use retrieval-augmented generation (RAG)\nwhere the retrieved passage is used as context by the LLM.\n\n    - **GPT-4 and Llama2-70B** : we run GPT-4 and Llama2-70B on MMLU tasks for all methods.\nIn addition, we also run GPT-4 on all baselines for all tasks.\n\n\nWe do not use RAG for STEM tasks, because of the inherent reasoning nature of the tasks contrary to\nthe other fact-seeking datasets. All inferences are done using greedy decoding.\n\n\n4 STEM\n\n\nWe evaluate STEP-BACK PROMPTING on STEM tasks (Hendrycks et al., 2020) to gauge the efficacy\n\n- f our method on reasoning in highly specialized domains. We explain below our experimental setup,\nresult, and analysis of applying STEP-BACK PROMPTING on the MMLU high-school Physics and\nChemistry, and GSM8K benchmarks.\n\n\n4.1 STEP-BACK PROMPTING\n\n\nQuestions in the MMLU benchmarks require deeper reasoning. Furthermore, they also require\nunderstanding and application of formulae which are often physics and chemistry principles and\nconcepts. In this case, we first demonstrate to the model abstraction skills in the form of concepts\nand first principles such as _Newton\u2019s first law of motion_, _Doppler effect_, and _Gibbs free energy_ etc.\n\n\n4\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nTable 1: Strong performance of STEP-BACK PROMPTING on MMLU tasks across three model\nfamilies. CoT: zero-shot Chain of Thought prompting (Kojima et"
    ],
    "response": "An example of a multi-hop reasoning task in MMLU high-school Physics is:\n\n**Question:** Two spheres of net charge +5e and -6e briefly come into contact. Afterward, which of the following is a possible combination of net charges for the two spheres?\n- 0) +3e and -4e\n- 1) +1.5e and -2.5e\n- 2) Both of the above\n- 3) None of the above\n\n**Principles:**\n1. Law of conservation of charge: the total charge in a closed system remains constant.\n2. Coulomb\u2019s Law: the mutual repulsion of like charges demands that the charges be as far apart as possible.\n\n**Answer:**\nUsing the principles of conservation of charge, we can solve the problem step by step:\n1. The total charge of the two spheres before they come into contact is +5e - 6e = -1e. After they come into contact, the total charge must still be -1e.\n2. The possible combinations of net charges for the two spheres can be +3e and -4e, or +1.5e and -2.5e.\n\nTherefore, the correct answer is 2) Both of the above. \n\nThis task requires the application of laws (conservation of charge and Coulomb's Law), demonstrating multi-hop reasoning as it involves both understanding the principles and applying them to deduce possible outcomes <document id=\"2310.06117_step_back_zheng:chunk_19800\">.",
    "reference": "An example of a multi-hop reasoning task in MMLU high-school Physics is the original question about the pressure of an ideal gas when temperature and volume are changed. The correct answer requires applying the Ideal gas law twice, first to find the initial pressure and then to find the final pressure after the changes.",
    "answer_correctness": 0.220796168,
    "context_precision": 0.8041666666,
    "context_recall": 1.0
  },
  {
    "user_input": "What are retrieval units in the context of open-domain QA?",
    "retrieved_contexts": [
      " are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\n\n- pen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve comparable or better results on multiple QA datasets in the\n\n- pen-retrieval setting, compared to several, much\ncomplicated systems.\n\n\n**2** **Background**\n\n\nThe problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \u201c _Who first voiced Meg on Family_\n_Guy?_ \u201d or \u201c _Where was the 8th Dalai Lama born?_ \u201d, a\nsystem is required to answer it using a large corpus\n\n- f diversified topics. More specifically, we assume\n\n\n\nthe extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more passages in the corpus. Assume that our collection\ncontains _D_ documents, _d_ 1 _, d_ 2 _, \u00b7 \u00b7 \u00b7, dD_ . We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units [3] and get _M_\ntotal passages in our corpus _C_ = _{p_ 1 _, p_ 2 _, . . ., pM_ _}_,\nwhere each passage _pi_ can be viewed as a sequence\n\n- f tokens _w_ [(] _[i]_ [)]\n1 _[, w]_ 2 [(] _[i]_ [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _|_ [(] _p_ _[i]_ [)] _i|_ [. Given a question] _[ q]_ [,]\n\nthe task is to find a span _ws_ [(] _[i]_ [)] _[, w]_ _s_ [(] _[i]_ +1 [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _e_ [(] _[i]_ [)] from\n\n- ne of the passages _pi_ that can",
      "## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [",
      " time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine",
      "T:\n\n\n1. improves both retrieval and few-shot QA performance on several multi-step open-domain\nQA datasets, in both IID and OOD settings;\n2. reduces factual errors in generated CoTs; and\n3. improves performance with both large-scale\n(175B models) as well as smaller-scale models (Flan-T5-*, _\u2264_ 11B) without any training.\n\n\n**2** **Related Work**\n\n\n**Prompting for Open-Domain QA.** LLMs can\nlearn various tasks by simply using a few examples as prompts (Brown et al., 2020). They\u2019ve\nalso been shown to answer complex questions\nby producing step-by-step reasoning (chain-ofthoughts, or CoT) when prompted with a few or\nzero demonstrations (Wei et al., 2022; Kojima et al.,\n2022). Prompting has been applied to open-domain\nQA (Lazaridou et al., 2022; Sun et al., 2022; Yu\net al., 2023) but its value in improving retrieval and\nQA for multi-step open-domain questions remains\nrelatively underexplored.\n\n\n3We explain later (in the Metric section and Footnote 7)\nthe appropriateness of this metric in our setting as opposed to\nmore mainstream information recall metrics.\n\n\nRecently three approaches have been proposed\nfor multi-step open-domain QA. SelfAsk (Press\net al., 2022) prompts LLMs to decompose a question into subquestions and answers subquestions by\na call to Google Search API. DecomP (Khot et al.,\n2023) is a general framework that decomposes a\ntask and delegates sub-tasks to appropriate submodels. They also decompose questions but delegate retrieval to a BM25-based retriever. Both of\nthese approaches are not developed for CoT reasoning, do not focus on the retrieval problem, and require a single-hop QA model to answer the decomposed questions. Recently proposed ReAct (Yao\net al., 2022) system frames the problem as generating a sequence of reasoning and action steps. These\nsteps are much more complex, rely on much larger\nmodels (PaLM-540B), and require fine-tuning to\n\n- utperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be\neffective for smaller models without any training.\nWhile a direct comparison",
      " its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily\nbetter at using their input context (\u00a72.3).\nGiven that language models struggle to retrieve\nand use relevant information in the multi-document\n\nquestion answering task, to what extent can language models even _retrieve_ from their input contexts? We study this question with a synthetic keyvalue retrieval task, which is designed to be a minimal testbed for the basic ability to retrieve matching\ntokens from the input context. In this task, models\nare given a collection of JSON-formatted key-value\npairs and must return the value associated with a\nspecific key. Similar to the multi-document QA\ntask, the key-value retrieval task admits controlled\nchanges to the input context length (adding more\nkey-value pairs) and the position of relevant information. Although some models perform the\nsynthetic key-value retrieval task perfectly, other\nmodels struggle to simply retrieve matching tokens\nthat occur in the middle of their input context and\ncontinue to exhibit a U-shaped performance curve.\nTo better understand why language models struggle to robustly access and use information in their\ninput contexts, we study the role of model architecture (decoder-only vs. encoder-decoder), queryaware contextualization, and instruction fine-tuning\n(\u00a74). We find that:\n\n\n  - Encoder-decoder models are relatively robust\nto changes in the position of relevant information within their input context, but only when\nevaluated on sequences within its trainingtime sequence length. When evaluated on\nsequences longer than those seen during training, we observe a U-shaped performance\ncurve (\u00a74.1).\n\n\n  - Query-aware contextualization (placing the\nquery before _and_ after the documents or keyvalue pairs) enables near-perfect performance\n\n   - n the synthetic key-value task, but minimally\nchanges trends in multi-document QA (\u00a74.2).\n\n\n  - Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of\nrelevant information in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that"
    ],
    "response": "In the context of open-domain question answering (QA), retrieval units refer to text passages of equal lengths that are created by splitting documents in a corpus. These passages serve as the basic units for retrieval during the question-answering process. When a question is posed, the system searches for a specific span within one of these passages that contains the answer to the question. Essentially, each passage can be viewed as a sequence of tokens, and the task involves identifying the answer span from among these passages <document id=\"2004.04906_dpr_karpukhin:chunk_1350\" />.",
    "reference": "Retrieval units in the context of open-domain QA refer to text passages of equal lengths used as basic units for information retrieval. These passages are split from documents and form a corpus, which is then used to answer factoid questions by finding relevant spans within these passages.",
    "answer_correctness": 0.9912740758,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "How do different contextual inputs affect the embeddings generated for documents in the TREC-Covid dataset?",
    "retrieved_contexts": [
      ". Section 10.5 displays a few qualitative examples. We generate embeddings for randomly\nsampled documents from the TREC-Covid dataset and visualize their embeddings with PCA, where\nunique document inputs with different contextual embeddings are visualized in the same color. By\nchanging only the conditioning we reshape the embedding space and our model produces different\nembedding for the same text. Note that although the embeddings are clearly moving in response to\nchanging the contextual inputs, they still remain closer to each other than to different documents.\n\n\n19\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|||||Rand<br>TSP|om||\n||||||||\n||||||||\n||||||||\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Pre-training with TSP vs. random batching across cluster sizes.\n\n\n\nFigure 15: Each color indicates a single document input _d_ . Different points represent different values _\u03d5_ ( _d_ ; _D_ ) for different contexts.\n\n\n\nFigure 16: Performance of CDE model as the\nnumber of contextual examples increases.\n\n\n\nWe also consider how additional context is improving our model. Because the model includes an\n\n- ptional null token, we can supply any number of contextual inputs. We plot our model\u2019s performance\nacross context sizes in Figure 10.5. We see that our model is able to utilize partial context window\nsizes, and even perform reasonably with no context (i.e. all null token inputs) but offers the best\nperformance given a full context window size.\n\n\n10.6 CLUSTER TEXT EXAMPLES\n\n\nWe include random examples from a cluster gathered from our supervised dataset, shown in Table 4.\nThis particular cluster appears to be a combination of documents about county populations in the\nUntied States (in Kentucky, Iowa, Pennsylvania, etc.) and documents about criminal trials (mentioning\nhearings, depositions, and courts).\n\n\n10.7 TASK PREFIXES\n\n\nPrefixes are hand-written for each dataset in both meta-training sets. We follow the same prefix\nselection procedure as Nussbaum et al. (2024), inspired by Reimers et al. (2023):\n\n\n   - search ~~q~~ uery\n\n   - search ~~d~~   - cument\n\n   - classification\n\n\n20\n\n\nquery document\n\n\npopulation of breckenridge mi breckenridge, michigan. breckenridge is",
      " with small cluster sizes clearly\nimproves performance, and larger batch sizes do not.\n\n\nFigure 10: Correlation between batch difficulty and perforamnce after supervised training.\n\n\nThis might explain part of why performance stagnates with higher batch sizes; increasing the batch\nsize decreases the total number of learning examples seen by our contextual model.\n\n\n**Supervised training: difficulty correlations.** In Section 10.5 we plot the correlation between\nbatch difficulty and downstream performance across cluster sizes (and within batch sizes) in the\nsupervised setting. In this case we also see the best performance through the most difficult clusters.\n\n\n18\n\n\nFigure 11: Performance of all supervised models, across numbers of hard negatives.\n\n\n\nFigure 12: Model performance vs. cluster size\nwith and without filtering. When false negative\nfiltering is enabled, we see more improvements\nin performance from clustering at small cluster\nsizes.\n\n\n\nFigure 13: Model performance vs. batch size\nwith and without filtering. With and without\nfiltering, the optimal batch size ranges between\n10 [2] and 10 [4] ; performance starts to decrease as\nbatch size grows too large.\n\n\n\n**Supervised training: full results.** We plot the full results of all supervised training experiments in\nSection 10.5. Our experiments in this setting (using the mined negatives from the Nomic supervised\nmeta-datasets) generally show _decreasing_ performance with additional hard negatives.\n\n\n**TSP Packing.** We compare randomly packing clusters into batches vs. a greedy traveling salesmanstyle solution, similar to (Shi et al., 2024). In our scenario, we first cluster datapoints, then find the\ncentroid embedding of each cluster. We begin packing by randomly selecting a cluster, and then\nchoose the next cluster by finding the cluster with the closest centroid to the current one. Results\nare shown in Figure 14. Although these results appear slightly noisy, we see an improvement from\nTSP-style packing especially at smaller cluster sizes (where packing has an outsized impact). We\ntherefore opt to use this packing procedure for our main model.\n\n\n**Impact of context size** We consider contextual embeddings might move in space as their conditioning varies. Section 10.5 displays a few qualitative examples. We generate embeddings for randomly\nsampled documents from the TREC-Covid dataset and visualize their embeddings with PCA, where\nunique document inputs with different contextual embeddings are visualized in the same",
      " that negative-sharing across GPUs may not be necessary in most settings. If\npossible, the most sanity-preserving way to perform contrastive training could be to\n\n\n10.4 REMOVING POSITIONALITY WITH ROTARY EMBEDDINGS\n\n\nOne detail of our model architecture is that it does not track positionality between dataset input\ntokens. Although disabling positionality would be trivial an a BERT-like encoder model that uses\nlearned positional embeddings, we use a version of BERT with _rotary_ positional embeddings which\ninject positional information at each layer of the transformer. To circumvent this step, we modify the\nmodel internals to set dataset input tokens to zero for the self-attention step only, and add a residual\nconnection propagating the dataset input tokens past the attention phase.\n\n\n10.5 ADDITIONAL RESULTS\n\n\nSection 10.5 show sweeps over batch and cluster sizes under our small experimental settings when\nperforming unsupervised pretraining with contextual architecture. We see similar trends to those\n\n- bserved with the biencoder architecture, however we note that performance is higher across the\nboard and our transductive model is able to perform well even at higher cluster sizes and low batch\nsizes.\n\n\nOne confounding factor in these experiments is that since the number of contextual documents is\nfixed, the number of different contextual inputs seen during training decreases with higher batch size.\n\n\n17\n\n\n|core (|Col2|Col3|@ )<br>,|Col5|ere|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n|core ( @|Col2|Col3|), u|n ere|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\nFigure 9: Contextual performance with filtering (left) and without (right) across batch and cluster\nsizes during unsupervised contrastive pre-training. Here, clustering with small cluster sizes clearly\nimproves performance, and larger batch sizes do not.\n\n\nFigure 10: Correlation between batch difficulty and perforamnce after supervised training.\n\n\nThis might explain part of why performance stagnates with higher batch sizes; increasing",
      "ive pre-training. With filtering, small cluster sizes clearly improve\nperformance, and larger batch sizes do not.\n\n\n7 ANALYSIS\n\n\n**How hard are our clusters?** To analysis the relationship between cluster size in our clustering\nalgorithm and the overall average difficulty of in-batch negatives, we measure the average difficulty\n\n- f 1000 batches across a variety of batch and cluster sizes and plot the data in Figure 6. We observe\n\n\n9\n\n\nFigure 4: Impact of filtering during training\nacross various batch and cluster sizes. Each dot\nis a biencoder pretrained with a different batch\nand cluster size.\n\n\nFigure 6: Average difficulty of in-batch negatives\nas measured by a surrogate model as cluster size\nand batch size change.\n\n\n\nFigure 5: Performance on MTEB across epochs\n\n- f supervised training on the Nomic and BGE\nsupervised meta-datasets.\n\n\nFigure 7: Impact of context by testing our\nmodel with different Stackexchange forum\ninput types. Y-axis indicates the input domain, X-axis indicates the test domain. Dark\nsquares come within one point NDCG@10.\n\n\n\nthat larger batches bring easier non-negative examples, and decreasing cluster size clearly increases\nthe average hardness of negative examples in a given cluster.\n\n\n**Which contextual documents help?** To confirm that the CDE model is utilizing contextual\ninformation from _D_ we consider how different contextual documents help for a given docuent _d_ .\nFigure 7 measures results on CQADupstack, a collection of Stack Exchange forum posts. We\nrandomly sample inputs to from _D_ from a domain (x-axis) and use them as input to the downstream\ntask _d_ marked along the y-axis. We mark a square as red if its score comes within 1 point of NDCG of\nthe top score for its domain. Generally utilizing in-domain works best, but there are some crossover\ninteractions.\n\n\n8 CONCLUSION\n\n\nWe propose two improvements to traditional biencoder models for generating embeddings. The\nfirst improvement involves an algorithm for reordering training datapoints to make batches harder\n\n\n10\n\n\nand improves vanilla training with minimal changes. Our second improvement involves a new\ncorpus-aware architecture for retrieval and allows us to train a state-of-the-art text embedding model.\n\n\n9 ACKNOWLEDGEMENTS\n\n\nThanks to Orion Weller, Vin Sachidananda, and Zach Nussbaum for valuable feedback on this\nresearch",
      " available\ncontext. To improve our model\u2019s generalization, we use _sequence dropout_, where we randomly replace\ncontext embeddings _M_ 1( _d_ _[\u2217]_ ) with some null token _v\u2205_ according to some a uniform probability _p_ .\n\n\nAt test time, if no corpus information is available, our model can now function as a non-contextual\nbiencoder simply by replacing all sequence token inputs with _v\u2205_ .\n\n\n**Position-agnostic embedding.** Since documents of _D_ are unordered, we remove all positionality\nfrom the neural encodings. When parameterizing _\u03b8_ with a traditional transformer, this can be\nachieved by omitting positional embeddings at the positions corresponding to _D_ . In practice, we use\ntransformers implementations dependent on FlashAttention with rotary positional embeddings at\neach self-attention layer. Full details of how we disable positionality are available in Section 10.4.\n\n\n**Two-stage gradient caching.** To improve training we employ a gradient-caching technique analogous to a two-stage version of GradCache (Gao et al., 2021). This technique allows us to fit larger\nbatches, longer sequences with more contextual samples without running out of memory. Essentially,\nwe compute first-stage and second-stage representations independently without gradients. We then\nuse these frozen representations to compute the loss, and gradients with respect to the second-stage\nrepresentations. We then re-run the second stage with gradients enabled and use the output gradients\nto backpropagate through the second-stage model, and obtain gradients for the first-stage representations. We repeat this process for the first-stage representations. This allows us to tradeoff computation\n(running each transformer forward pass twice) for memory.\n\n\n1Context reuse is only feasible because documents within the same batch typically share a large amount of\ncontext anyway, since they are clustered.\n\n\n6\n\n\n5 EXPERIMENTAL SETUP\n\n\nWe consider a range of retrieval experiments across different scales. To run experiments across a suitable number of settings, we devise a small setting: six-layer transformer, maximum sequence length\n\n- f 64, and maximum number of 64 additional contextual tokens. In this scenario, we evaluate on a\ntruncated version of the BEIR benchmark (Thakur et al., 2021). Given the low cost of each experiment,\nwe are able to pre-train and fine-tune both biencoder and contextual models"
    ],
    "response": "Different contextual inputs significantly affect the embeddings generated for documents in the TREC-Covid dataset by reshaping the embedding space. Qualitative experiments reveal that when the conditioning of the input changes, the resultant embeddings for the same text adjust accordingly, although they tend to stay closer to each other than to embeddings of different documents. This indicates that while contextual variations do impact the embeddings, the relatedness between similar documents is preserved within the embedding space (<document id=\"2410.02525_contextual_embeddings_nussbaum:chunk_16650\">).\n\nFurthermore, the model's performance improves with the number of contextual examples provided. Performance is best when a full context window size is used, demonstrating that additional context enhances the model's ability to generate meaningful embeddings, even though it can still function reasonably with partial or null contextual inputs (<document id=\"2410.02525_contextual_embeddings_nussbaum:chunk_16200\">).",
    "reference": "Different contextual inputs reshape the embedding space, causing the model to produce different embeddings for the same text. Although the embeddings move in response to changing contextual inputs, they remain closer to each other than to different documents.",
    "answer_correctness": 0.4448740766,
    "context_precision": 1.0,
    "context_recall": 1.0
  },
  {
    "user_input": "What method did Wu et al. propose to enhance the diversity and coverage of instruction-following knowledge extracted from teacher LLMs?",
    "retrieved_contexts": [
      " expansion method to a more powerful teacher LLM, textdavinci-003, to distill 52K high-quality data. To improve\nthe diversity and coverage during expansion, Wu et al.\n(2023c) and (Sun et al., 2024b) prompt the teacher LLM to\ngenerate instructions corresponding to some specific topics.\nXu et al. (2023a) propose an Evol-Instruct method to expand the instructions from two dimensions: difficulty (e.g.\nrewriting the question to be more complex) and diversity\n(e.g. generating more long-tailed instructions). This EvolInstruct method is domain-agnostic and has been used to\nexpand the distillation of coding (Luo et al., 2023a) and\nmath (Luo et al., 2023b). Additionally, expansion methods\ncan significantly augment NLP task datasets with similar\nsamples, thereby enhancing task performance. For instance,\nAugGPT (Dai et al., 2023a) leverages a teacher LLM to\nrephrase each sentence in the training samples into multiple conceptually similar, but semantically varied, samples\nto improve classification performance. Similarly, TDG (He\n\n\n\net al., 2023b) proposes the Targeted Data Generation (TDG)\nframework, which automatically identifies challenging subgroups within data and generates new samples for these\nsubgroups using LLMs through in-context learning.\nIn summary, the expansion method leverages the incontext learning strengths of LLMs to produce more varied and extensive datasets with both inputs and outputs.\nHowever, the quality and diversity of the generated data\nare heavily reliant on the teacher LLMs and the initial seed\ndemonstrations. This dependence can lead to a dataset with\ninherent bias from LLMs (Yu et al., 2023a; Wei et al., 2023)\nand a homogeneity issue where the generations may be\nprone to similarity ultimately, limiting the diversity this\nmethod seeks to achieve (Ding et al., 2023b). Moreover, the\nexpansion process may inadvertently amplify any biases\npresent in the seed data.\n\n\n_3.1.3_ _Data Curation_\n\nThe pursuit of high-quality and scalable data generation in\nknowledge distillation from LLMs has led to the emergence\n\n- f the Data Curation approach. This method arises in response to the limitations observed",
      "\n\n\n\nInput Set\n\n\n\n\n\n\n\n\n\n\n|et|Col2|Col3|\n|---|---|---|\n|t|||\n|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|xtract|||\n|xtract|||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 5: An illustration of different knowledge elicitation methods from teacher LLMs. _Labeling_ : The teacher generates\nthe output from the input; _Expansion_ : The teacher generates samples similar to the given demonstrations through incontext learning; _Data Curation_ : The teacher synthesizes data according to meta-information, such as a topic or an entity;\n_Feature_ : Feed the data into the teacher and extract its internal knowledge, such as logits and features; _Feedback_ : The teacher\nprovides feedback on the student\u2019s generations, such as preferences, corrections, expansions of challenging samples, etc;\n_Self-Knowledge_ : The student first generates outputs, which is then filtered for high quality or evaluated by the student itself.\n\n\n\nIn this formulation, _x_ and _y_ represent the new input\n- utput pairs generated by the teacher LLM. The input _x_\nis generated based on a set of input-output demonstrations\n_c_ . The output _y_ is then generated in response to the new\ninput _x_ under the guidance of an instruction _I_ . Note that\nthe demonstrations could be predefined or dynamically\nupdated by adding the newly generated samples.\nExpansion techniques have been widely utilized to\nextract extensive instruction-following knowledge from\nteacher LLMs. Wang et al. (2022a) first introduces an iterative bootstrapping method, Self-Instruct, to utilize LLMs\nto generate a wide array of instructions based on several demonstrations sampled from 175 manually-written instructions. The newly generated instructions are then added\nback to the initial pool, benefiting subsequent expansion\niterations. Subsequently, Taori et al. (2023) applies this expansion method to a more powerful teacher LLM, textdavinci-003, to distill 52K high-quality data. To improve\nthe diversity and coverage during expansion, Wu et al.\n(2023c) and (Sun et",
      " . This method involves\ngathering 3.5K common topics from Wikipedia to serve as\nguidance during the generation process.\n\n\n_**Complex Instructions.**_ Some works promote students to\nsolve more complex instructions (Xu et al., 2023a; Luo et al.,\n2023b,a; Guo et al., 2023c). According to Xu et al. (2023a), instruction datasets derived from human-written seeds often\nexhibit low to moderate complexity. To enhance the complex instruction-following capabilities of smaller models,\nWizardLM (Xu et al., 2023a) introduces _Evol-Instruct_ . This\nmethod gradually transforms instructions into more complex forms through a multi-step evolution process, focusing\n\n- n both increasing difficulty levels and expanding the diversity of topics. They conducted four rounds of evolution\nusing the OpenAI ChatGPT API, resulting in a dataset of\n250k complex instructions. Subsequently, they trained the\nLLaMA 7B model, referred to as WizardLM, on this dataset.\nIn the high-difficulty section of test instructions, WizardLM\neven outperformed ChatGPT, achieving a win rate 7.9%\nhigher than ChatGPT. Zhao et al. (2023e) further conduct\npreliminary studies revealing the effectiveness of increasing\ninstruction complexity. Instruction Fusion (Guo et al., 2023c)\nfurther uses teacher LLMs to increase the complexity by\nfusing two distinct evolved instructions. Furthermore, this\nconcept of \u201cevolving\u201d instructions has been extended to\ndistill specific skills such as coding (Luo et al., 2023a) and\nmathematics (Luo et al., 2023b).\n\n\n_**Human Instructions.**_ In contrast to works that rely on generating instructions from ChatGPT, which may lack diversity\nand have gaps with real human instructions, Vicuna (Chiang\net al., 2023) and Koala (Geng et al., 2023) showcase impressive performance by using human conversations and natural instructions from community-contributed conversations.\nThese conversations, found in platforms like ShareGPT, provide a forum for users to share their interactions with ChatGPT. It\u2019s important to note, however, that models trained\n\n- n such natural conversations might mimic the style but\nmay not fully capture the reasoning process of the original\nteacher (G",
      "-tuning of LLMs\nto acquire the ability to follow instructions or transfer\nknowledge for forthcoming tasks, skills, or domains, leveraging advancements in LLM technology. Nevertheless, investigations have revealed that the continual fine-tuning of\nLLMs on particular datasets (skills, domains) can lead to\na phenomenon known as catastrophic forgetting, wherein\npreviously acquired knowledge and problem-solving abilities for earlier tasks are compromised (Chen et al., 2023f;\nKotha et al., 2023; Koloski et al., 2023; Wu et al., 2024;\nLuo et al., 2023f). Earlier studies in machine learning and\ndeep learning have investigated various techniques to help\nmitigate forgetting during the fine-tuning or continue learning process, such as rehearsal, which entails periodically\nrevisiting and training on past data (Kirkpatrick et al., 2017;\nRostami et al., 2019; Rolnick et al., 2019), as well as regularization methods like elastic weight consolidation (Lee\net al., 2017), or dynamic architecture methods (Mallya et al.,\n2018; Wang et al., 2022c; Hu et al., 2023c; Chen et al., 2023f).\nTo address the challenges of catastrophic forgetting and to\nenhance the diversity of generated instructions in knowledge distillation for LLMs, Jiang et al. (2023b) randomly\nsample an instruction from the easy instructions and also\nprompt the generator to generate a new instruction that\nbelongs to the same domain as the sampled one. In a similar\nvein, Li et al. (2023m) study the problem of instructiontuning in multi-modal LLMs knowledge distillation and\nintroduce a competitive distillation framework. The model\ntries to produce new instructions that differ in content but\nare similar in difficulty to the original pictures in the multimodal augmentation phase, so as to alleviate catastrophic\nforgetting of the model and enhance the diversity of the\ninstruction tuning pool. Chen et al. (2023f) propose the\nLifelong-MoE (Mixture-of Experts) architecture based on\ngeneral language models, which dynamically adds model\ncapacity via adding experts with regularized pretraining.\nAdditionally, the model also introduces implicit regularization via distillation of the knowledge from old experts and\ngatings to effectively preserve old knowledge",
      " Learning, RO:\nRanking Optimization.\n\n\n\nformats with templates, such as prefacing machine translation data with _\u201dTranslate this sentence to Spanish:\u201d_ . However,\nthese approaches have limitations. Manual data creation is\nlabor-intensive, while template-based transformation lacks\ndiversity in instructions and may not align well with natural\nhuman input. LLMs like GPT-4 offer an efficient alternative\nfor creating diverse and controlled SFT data by their capabilities of in-context learning and instruction following. Most\n\n\n\nrelevant works use OpenAI\u2019s GPT series models to generate\nprompt-response data pairs and then train the student LLMs\nby supervised fine-tuning (Wang et al., 2022a; Taori et al.,\n2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a;\nMukherjee et al., 2023; Mitra et al., 2023; Luo et al., 2023b;\nPeng et al., 2023a).\n\n\n_**Basic Instructions.**_ Self-Instruct (Wang et al., 2022a) leverages the in-context learning capability of GPT-3 to expand\n\n\na seed pool of 175 tasks to 52K task-agnostic instructions,\nensuring a broad spectrum of general instructions. Additionally, a filtering and post-processing stage is introduced\nto eliminate redundant or similar instructions. Notably,\nthrough training with this enriched dataset, GPT-3 acquires\nthe ability to follow instructions, enabling it to perform\ncomparably to InstructGPT in zero-shot instruction tasks\nand when provided with expert-written instructions for\nnovel tasks. Based on the self-instruct method, Taori et al.\n(2023) train an Alpaca model using the Llama 7B model\n\n- n 52K instruction-following demonstrations, generated in\na similar style as self-instruct but utilizing the more robust\ntext-davinci-003 model. To enhance the diversity of instructional data, Wu et al. (2023c) introduce a technique known\nas _Topic-Guided Instruction Generation_ . This method involves\ngathering 3.5K common topics from Wikipedia to serve as\nguidance during the generation process.\n\n\n_**Complex Instructions.**_ Some works promote students to\nsolve more complex instructions (Xu et al.,"
    ],
    "response": "Wu et al. proposed a technique called **Topic-Guided Instruction Generation** to enhance the diversity and coverage of instruction-following knowledge extracted from teacher LLMs. This method involves gathering 3.5K common topics from Wikipedia to guide the generation process, thereby improving the variety of instructions produced <document id=\"2402.13116_long_context_rag_xu:chunk_20700\">.",
    "reference": "Wu et al. proposed a method to enhance the diversity and coverage of instruction-following knowledge extracted from teacher LLMs by dynamically updating the initial pool of demonstrations with newly generated samples during expansion iterations.",
    "answer_correctness": 0.5296776405,
    "context_precision": 0.8875,
    "context_recall": 1.0
  },
  {
    "user_input": "What specific research approach did Xu et al. propose to improve the performance of LLMs in handling factual errors?",
    "retrieved_contexts": [
      "Introduction**\n\n\nDespite the excellent capabilities of large language\nmodels (LLMs) (Zhao et al., 2023b), they still face\nsignificant challenges in complex reasoning tasks\n( _e.g.,_ multi-hop question answering), which often\ngo beyond simple, single-step problem-solving,\ndemanding a deeper level of cognitive reasoning\nacross multiple facts, sources, or contexts (Huang\net al., 2024; Suzgun et al., 2023). Great efforts have\nbeen made to improve the reasoning effectiveness\n\n- f LLMs by conducing step-by-step reasoning, exemplified by chain-of-thought (CoT) (Wei et al.,\n2022). However, as the number of reasoning steps\ngrows, LLMs are often prone to introduce logical\n\n\n_\u2217_ Equal contributions.\n\n_\u2020_ Corresponding author.\n\n\n\nerrors, factual hallucinations, or inconsistent statements (Wei et al., 2022; Lyu et al., 2023).\n\nIn fact, step-by-step reasoning in the autoregressive generation paradigm can be described\nas akin to \u201cSystem 1\u201d, a mode of thinking which is\nfast, instinctive but less accurate (Kahneman, 2011).\nConversely, solving complex reasoning problems\nrequires more in-depth, deliberative, and logical\nthinking, known as the \u201cSystem 2\u201d mode, which requires conscious effort to conduct massive strategic\ndecision-making (Kahneman, 2011). To enhance\nthe \u201cSystem 2\u201d reasoning capabilities of LLMs,\nprior studies have proposed to conduct deliberative\ngeneration by leveraging basic tree search algorithms ( _e.g.,_ Monte Carlo Tree Search (Silver et al.,\n2017)). However, LLMs in these studies mainly\ndepend on their _internal knowledge_ to search over\nintermediate reasoning steps, limited to handling\nproblems with relatively simple reasoning process.\nTo leverage external knowledge in model reasoning,\nextensive research has sought to augment LLMs\nwith external information sources ( _a.k.a._ retrievalaugmented generation, RAG) (Lewis et al., 2020b;\nYao et al., 2022), while existing efforts mainly consider sequential reasoning structure, which cannot\nnaturally support more complex reasoning structure\nlike MCTS. Thus, we raise the following research\nquestion: _Can RAG enhance the deliberative rea-_\n_soning capabilities of",
      "<br>89<br>9<br>**8**<br>**7**<br>**57.14**|\n\n\nTable 7: The result of counterfactual robustness. ACC is the\naccuracy (%) of LLMs without external documents. ACCdoc\nis the accuracy (%) of LLMs with counterfactual documents.\nED and ED _[\u2217]_ are error detection rates evaluated by exact\nmatching and ChatGPT, respectively. CR is the error correction rate.\n\n\nthought approach to break down complex problems (Zhou\net al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). However, these methods slow down the inference speed and cannot provide timely responses.\n\n\n**Results on Counterfactual Robustness testbed**\n\nIn order to ensure that LLMs possess relevant knowledge,\nwe assess their performance by directly asking them questions. However, we found that most LLMs struggle to answer them correctly. To ensure a more reasonable evaluation, we only consider LLMs that have an accuracy rate of\n\n- ver 70% as this threshold is relatively high and encompasses more LLMs. The results are shown in Table 7. We\npresent the following metrics: accuracy without any documents, accuracy with counterfactual documents, error detection rates, and error correction rates. We can see that It\nis hard for LLMs to identify and correct factual errors in the\ndocuments. This suggests that the model can be easily misled by documents containing incorrect facts.\nIt is important to note that retrieval-augmented generation\nis not designed to automatically address factual errors within\na given context, as this contradicts the underlying assumption that the model lacks knowledge and relies on retrieved\ndocuments for additional information. However, this issue is\ncrucial in practical applications due to the abundance of fake\nnews on the internet. Existing LLMs do not have a safeguard\nto handle inaccurate responses caused by misinformation. In\nfact, they heavily depend on the information they retrieve.\nEven when LLMs contain the internal knowledge about the\nquestions, they often trust false information that is retrieved.\nThis presents significant a challenge for the future development of RAG in LLMs.\n\n\n**Conclusion**\n\nIn this paper, we evaluated four abilities of retrievalaugmented generation in LLMs: noise robustness, negative rejection, information integration, and counterfactual\nrobustness",
      " The \u201c _{_ DOCS _}_ \u201d and \u201c _{_ QUERY _}_ \u201d will be replaced by\nthe external documents and the question.\n\n\nquires utilizing information from various documents. Different from the first three abilities, the data of counterfactual\nrobustness is constructed solely based on the internal knowledge of the model. Based on the aforementioned generated\nquestions mentioned above, we adopt ChatGPT to automatically generate its known knowledge. Specifically, we use\nprompts to allow the model to generate both questions and\nanswers that are already known. For example, based on the\nquestion \u201cWho was awarded the 2022 Nobel Prize for Physiology and Medicine?\u201d, the model will generate the known\nquestion \u201cWho was awarded the 2021 Nobel Prize in Literature?\u201d and answer \u201c _Abdulrazak Gurnah_ \u201d. We then manually verified the generated answers, and retrieve relevant\ndocuments as described above. In order to make documents\ncontain factual errors, we manually modify the answers and\nreplace the corresponding parts in the document.\nFinally, we collect totally 600 base questions in RGB,\nand 200 additional questions for the information integration\nability and 200 additional questions for counterfactual robustness ability. Half of the instances are in English, and the\n\n- ther half are in Chinese.\n\n\n**Evaluation metrics**\n\nThe core of this benchmark is to evaluate whether LLMs can\nutilize the provided external documents to acquire knowledge and generate reasonable answers. We evaluate the responses of LLMs in order to measure above-mentioned four\nabilities of them.\n**Accuracy** is used to measure noise robustness and information integration. We employ an exact matching approach\nwhere if the generated text contains an exact match to the\nanswer, it is considered as a correct answer.\n**Rejection rate** is used to measure negative rejection.\nWhen only noisy documents are provided, LLMs should\n\n- utput the specific content \u2013 \u201cI can not answer the question\nbecause of the insufficient information in documents.\u201d (We\nuse instructions to inform the model.). If the model generates this content, it indicates a successful rejection.\n**Error detection rate** measures whether the model can\n\ndetect the factual errors in the documents for counterfactual\nrobustness. When the provided documents contain factual\nerrors, the model should output the specific content \u2013 \u201cThere\nare factual errors in the provided documents.\u201d (We use in\n\n\n**English** **Chinese**\n\n\n\n**",
      " expansion method to a more powerful teacher LLM, textdavinci-003, to distill 52K high-quality data. To improve\nthe diversity and coverage during expansion, Wu et al.\n(2023c) and (Sun et al., 2024b) prompt the teacher LLM to\ngenerate instructions corresponding to some specific topics.\nXu et al. (2023a) propose an Evol-Instruct method to expand the instructions from two dimensions: difficulty (e.g.\nrewriting the question to be more complex) and diversity\n(e.g. generating more long-tailed instructions). This EvolInstruct method is domain-agnostic and has been used to\nexpand the distillation of coding (Luo et al., 2023a) and\nmath (Luo et al., 2023b). Additionally, expansion methods\ncan significantly augment NLP task datasets with similar\nsamples, thereby enhancing task performance. For instance,\nAugGPT (Dai et al., 2023a) leverages a teacher LLM to\nrephrase each sentence in the training samples into multiple conceptually similar, but semantically varied, samples\nto improve classification performance. Similarly, TDG (He\n\n\n\net al., 2023b) proposes the Targeted Data Generation (TDG)\nframework, which automatically identifies challenging subgroups within data and generates new samples for these\nsubgroups using LLMs through in-context learning.\nIn summary, the expansion method leverages the incontext learning strengths of LLMs to produce more varied and extensive datasets with both inputs and outputs.\nHowever, the quality and diversity of the generated data\nare heavily reliant on the teacher LLMs and the initial seed\ndemonstrations. This dependence can lead to a dataset with\ninherent bias from LLMs (Yu et al., 2023a; Wei et al., 2023)\nand a homogeneity issue where the generations may be\nprone to similarity ultimately, limiting the diversity this\nmethod seeks to achieve (Ding et al., 2023b). Moreover, the\nexpansion process may inadvertently amplify any biases\npresent in the seed data.\n\n\n_3.1.3_ _Data Curation_\n\nThe pursuit of high-quality and scalable data generation in\nknowledge distillation from LLMs has led to the emergence\n\n- f the Data Curation approach. This method arises in response to the limitations observed",
      " When indexing, we\nutilize information from the corpus to produce document and query embeddings that are specific to a\nparticular domain.\n\n\nExperiments compare these two extensions to standard approaches for training document embeddings.\nOur results show that contextual contrastive training improves standard text embedding model training,\nand can be run without other approaches such as additional hard negatives. With the contextual\nencoder architecture, we see additional improvements over a baseline model in all settings tested,\nwith larger improvements in highly specific domains such as small datasets of financial and medical\ndocuments. When trained at industry-scale, our model achieves state-of-the-art results for small\n( _<_ 250M parameter) models on the MTEB benchmark.\n\n\n2 RELATED WORK\n\n\n**Text retrieval.** Our work is related to the general field of text retrieval; we propose specific\nimprovements to the training of \u201cbiencoder\u201d text embedding models such as DPR (Karpukhin et al.,\n2020), GTR (Ni et al., 2021), Contriever (Izacard et al., 2022), LaPraDoR (Xu et al., 2022), Instructor\n(Su et al., 2023), Nomic-Embed (Nussbaum et al., 2024), E5 (Wang et al., 2024), and GTE (Li et al.,\n2023). We focus on the problem of adapting these text retrieval models to new corpora at test time;\nsome prior work has noted this problem (Dai et al., 2022; Sciavolino, 2021) and proposed solutions\nsuch as unsupervised span-sampling and training on test corpora (Gao & Callan, 2021) and distillation\n\n- n the test corpus from a reranker (Sung et al., 2023). Late interaction methods (Khattab & Zaharia,\n2020; Santhanam et al., 2022) also offer one way to improve out-of-domain retrieval performance,\nbut increase the runtime and complexity of search. We propose a better sampling scheme that can be\nused to train any biencoder or late interaction model as well as a _training-free_ method for test-time\nadaptation.\n\n\n2\n\n\n**Contrastive learning.** Much research has focused on the effect of hard negatives on the performance\n\n- f contrastive learning methods Chen et al. (2020); Qu et al"
    ],
    "response": "Xu et al. proposed an Evol-Instruct method aimed at improving the performance of large language models (LLMs) in handling factual errors. This method involves expanding instructions from two dimensions: difficulty and diversity. They generate more complex questions while also producing a variety of instructions, which enhances the breadth and depth of the training data. This approach is designed to be domain-agnostic and has been applied across various NLP tasks to improve performance by augmenting datasets with higher quality and more diverse examples. However, it is noted that the quality of generated data can depend heavily on the teacher LLMs used and may introduce biases present in initial seed data <document id=\"2402.13116_long_context_rag_xu:chunk_8550\" />.",
    "reference": "Xu et al. proposed a thought approach to break down complex problems, which involves using methods like Zhou et al. (2023a), Xu et al. (2023b), and Drozdov et al. (2023). These methods aim to enhance the performance of LLMs by providing relevant knowledge and addressing issues such as slow inference speed and delayed responses.",
    "answer_correctness": 0.3539637118,
    "context_precision": 0.5,
    "context_recall": 1.0
  },
  {
    "user_input": "What datasets did the researchers use to evaluate the performance of their models for Musique?",
    "retrieved_contexts": [
      " time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine",
      " typed as above. We sample new entities uniform randomly from the 5% least common entities\nin NewsCrawl ( _I_ _[RandArg][\u2193]_ ), and the 5% most common ( _I_ _[RandArg][\u2191]_ ). We insert the sampled entities\nwhile preserving the rest of each statement.\n\n\n**4** **Querying Models with Prompts**\n\n\n**4.1** **Models**\n\n\n**LLaMA** is a recent LLM model family which\nrivals or surpasses GPT-3 performance while being\n\n- pen to scientific study. A range of model sizes\n\n\nare provided, and we test the largest **LLaMA-65B**\nmodel. LLaMA is not fine-tuned. In preliminary\nexperiments on the Levy/Holt dataset, we found\ntwo popular fine-tuned LLaMA variants, Alpaca\n(Taori et al., 2023) and Vicuna (Chiang et al., 2023),\nperform similarly to LLaMA base models and underperform LLaMA-65B, so we leave them out of\nfurther experiments.\n\n\n**GPT-3 Series** models are closed to deep scientific review (Brown et al., 2020), though they are a\nwidely-used comparison for their performance, and\nhave been reasonably well-studied. We evaluate on\n**text-davinci-003 (GPT-3.5)**, as it is the largest, and\nhas undergone instruction- and RLHF-finetuning,\nenabling interesting comparisons.\n\n\n**PaLM** is larger than GPT-3, which often claims\nstate-of-the-art on evaluation datasets. We use the\n\nlargest **PaLM-540B** base model, which is also only\npretrained, so it serves as a further comparison\npoint to LLaMA.\n\n\nLater GPT models (like text-davinci-003 in our\nexperiments) have been pre-trained and fine-tuned,\nwhile base LLaMA and PaLM have only undergone pre-training, so their contrast indicates what\nstage of training is responsible for the phenomena\nwe study. Our aim is not to judge which LLM\nis superior, but to show the common sources of\nhallucination they share.\nWe also omit models superseded in performance\nby LLaMA (e.g. OPT, GPT-J, etc",
      "studio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine of philosophy of language\n\n\nwhich baseball team won the world\n\nseries in 2015 baseball?\n\n\n\nStrategyQA Could the members of The Police what can the members of The Police\nperform lawful arrests? do?\n\n\nStrategyQA Would a Monoamine Oxidase candy What are the effects of Monoamine\nbar cheer up a depressed friend? Oxidase?\n\n\nStrategyQA Would a dog respond to bell before Would a dog respond to bell before\nGrey seal? Grey seal?\n\n\nStrategyQA Is shrimp scampi definitely free of what is shrimp scampi made of?\nplastic?\n\n\nStrategyQA Do the anchors on Rede Globo What languages do the anchors on\nspeak Chinese? Rede Globo speak?\n\n\nTable 15: Few-shot demonstration exemplars for asking step-back questions in MuSiQue and StrategyQA.\n\n\n20\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nBaseline few-shot Prompt\n\n\nYou are an expert of world knowledge and physics. Your task is to solve\nthe following question. Here are a few examples:\n\n\nQuestion: _<_ Question Example _>_\nAnswer: _<_ Answer Example _>_\nQuestion: _<_ Question _>_\nAnswer:\n\n\nTable 16: Prompt of querying the baseline model for final answer with few-shot demonstration\nexemplars.\n\n\nFor few-shot CoT prompting, we use the same template as the Baseline prompting in Sec. D.4 by\nreplacing the few-shot examples using CoT responses, as shown in Tables 18, 19, 20, 21, and 22.\n\n\nD.6 TAKE A DEEP BREATH (TDB)\n\n\nWe study the zero-shot prompting found in Yang et al. (2023): we take _Take a deep breath and work_\n\n_on this problem step-by-step_, and prepend it to the question.\n\n\nE EXAMPLES OF ERROR ANALYSIS AND WINS OF STEP-BACK PROMPTING\n\n\nE.1 MMLU ERROR ANALYSIS\n\n\nIn Tables 23-27, we show one example for each of the 5 error categories we identified through error\nanalysis on STEP-BACK PROMPTING.\n\n\nE.2 EXAMPLE WINS FROM STEP-BACK PROMPTING\n\n\nTables 28- ",
      " points.\n\n\n\nWe speculate DecomP performs well on 2WikiMultihopQA because it has only a few easy-to-predict\ndecomposition patterns, which DecomP\u2019s question\ndecomposition can leverage. The lack of such patterns in HotpotQA and MuSiQue causes it to underperform compared to IRCoT. Lastly, it will be\nuseful to assess whether DSP, which is hardcoded\nfor 2-hop questions like that of HotpotQA, will\nwork well for a dataset with a varied number of\n\nhops like that of MuSiQue. We leave this further\ninvestigation to future work.\n\n\n**D** **Additional CoT Generation Examples**\n\n\nTable 5 provides illustrations, in addition to the\n\n- nes provided in Table 2, for how the CoT generations for NoR QA, OneR QA, and IRCoT QA\nmethods vary. This gives an insight into how IRCoT improves QA performance. Since NoR relies completely on parametric knowledge, it often\nmakes a factual error in the first sentence, which derails the full reasoning chain. Some of this factual\ninformation can be fixed by OneR, especially information closest to the question (i.e., can be retrieved\nusing the question). This is insufficient for fixing\n\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nWhat was the\n\nisland, - n\nwhich Marinelli\n\nGlacier is located, formerly\nknown as?\n\n\nWho is the\n\nchild - f the\ndirector of film\nMukhyamantri\n(1996 Film)?\n\n\nWhen did the\nGovernorship\nend - f the\ncity where the\nauthor of Sac\nerdotii Nostri\n\nPrimordia died?\n\n\n\n_\u21d2_ Marinelli Glacier is\n\nlocated on the island of\nTierra del Fuego.\n_\u21d2_ The island of Tierra\ndel Fuego was formerly\nknown as Isla de Xativa.\n\n\n_\u21d2_ The director - f\nMukhyamantri (1996\nfilm) is Anjan Choudhury.\n_\u21d2_ Anjan Choudhury\nhas a child named\nSandip Choudhury.\n\n\n_\u21d2_ Sacerdotii Nostri Primordia was written by\nPope John XXIII.\n_\u21d2_ Pope",
      "\n19 electrons.\n\n**Stepback Question** : What are the chemistry principles behind this question?\n**Principles** :\nAtomic number: The atomic number of an element is the number of protons in\nthe nucleus of an atom of that element.\n**Final Answer** : The atomic number of potassium is 19, so all potassium atoms\nhave 19 protons. The mass number of potassium-40 is 40, so it has 40 - 19 =\n21 neutrons. When potassium is part of a compound, it loses one electron to\nbecome a potassium ion. Therefore, potassium-40 has 19 - 1 = 18 electrons\nwhen it is part of K2SO4.\nThe correct answer is 0) 21 neutrons, 19 protons, 18 electrons.\n\n\nTable 31: Illustration of wins of STEP-BACK PROMPTING on the MMLU-Chemistry dataset.\n\n\n35\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nTimeQA **Original Question** : Carlos Ascues played for which team from 2013 to\n2014?\n**Original Answer [Incorrect]** : Carlos Ascues played for Benfica from\n2013 to 2014.\n**Stepback Question** : Which teams did Carlos Ascues play for?\n**Final Answer [Correct]** : From 2013 to 2014, Carlos Ascues played for\nthe Greek team Panetolikos.\n\n\nTimeQA **Original Question** : Seth Nana Twumasi played for which team between\nJan 2011 and Feb 2011?\n**Original Answer [Incorrect]** : Seth Nana Twumasi played for the FC\nHonka team between Jan 2011 and Feb 2011.\n**Stepback Question** : Which teams did Seth Nana Twumasi play for?\n**Final Answer [Correct]** : Northampton. He was on a loan transfer from\nPeterborough.\n\n\nTimeQA **Original Question** : What position did Carmel Hanna take in May 2002?\n**Original Answer [Incorrect]** : According to the article, Carmel Hanna\nwas appointed as Minister of State for Children and the Family in May\n2002. She held this position until 2005.\n**Stepback Question** : What positions did Carmel Hanna hold?\n**Final Answer [Correct]** : From the context, we can see that Carmel\n"
    ],
    "response": "The researchers used the **MuSiQue** dataset to evaluate the performance of their models. This is indicated in examples where they pose questions and showcase retrieval-based queries using the MuSiQue dataset for various inquiries related to philosophy, sports, and historical facts <document 2310.06117_step_back_zheng:chunk_14850>.",
    "reference": "The researchers used FreshQA and HotpotQA datasets to evaluate the performance of their models for Musique.",
    "answer_correctness": 0.2187208742,
    "context_precision": 0.0,
    "context_recall": 0.0
  },
  {
    "user_input": "What is the qrel matrix used in Yang et al.'s work for instantiating attributes?",
    "retrieved_contexts": [
      " for simplicity in instantiating and to mirror previous\nwork (i.e. NQ, HotpotQA, etc. [Kwiatkowski et al., 2019, Yang et al., 2018]).\n\n\nOur last step is to choose a qrel matrix to instantiate these attributes. Although we could not prove\nthe hardest qrel matrix definitively with theory (as the sign rank is notoriously hard to prove), we\nspeculate based on intuition that our theoretical results imply that the more interconnected the qrel\nmatrix is (e.g. dense with all combinations) the harder it would be for models to represent. [8] Following\nthis, we use the qrel matrix with the highest number of documents for which all combinations would\nbe just above 1000 queries for a top- _\ud835\udc58_ - f 2 (46 docs, since [\ufffd][46] 2 - is 1035, the smallest above 1k).\n\n\nWe then assign random natural language attributes to the queries, adding these attributes to their\nrespective relevant documents (c.f. Figure 1). We give each document a random first and last name\nfrom open-source lists of names. Finally, we randomly sample new attributes for each document until\nall documents have the same number of attributes. As this setup has many more documents than\nthose that are relevant to any query (46 relevant documents, 49.95k non-relevant to any query) we\nalso create a \u201csmall\u201d version with only the 46 documents that are relevant to one of the 1000 queries.\n\n\n8See Appendix 10 for specific metrics that show the difference between LIMIT and other IR datasets.\n\n\n8\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n\n\n\n\n\n\n\n\n|Reca|ll@2|\n|---|---|\n|||\n|||\n|||\n\n\n|Recal|l@10|\n|---|---|\n|||\n|||\n|||\n\n\n|Recal|l@100|\n|---|---|\n|||\n|||\n\n\n\n\n\n\n\n\n\n\n\nFigure 3 | Scores on the LIMIT task. Despite the simplicity of the task we see that SOTA models\nstruggle. We also see that the dimensionality of the model is a limiting factor and that as the\ndimension increases, so does performance. Even multi-vector models struggle. Lexical models like\nBM25 do very well due to their higher dimensionality. Stars indicate models trained with MRL.\n\n\n",
      " right operators if the documents are\nsufficiently expressive (i.e. non-trivial). Thus, that existing datasets choose to only instantiate some\n\n- f these combinations is mainly for practical reasons and not because of a lack of existence.\n\n\nIn contrast to these previous works, we seek to build a dataset that evaluates all combinations of\ntop- _\ud835\udc58_ sets for a small number of documents. Rather than using difficult query operators like QUEST,\nBrowseComp, etc. (which are already difficult for reasons outside of the qrel matrix) we choose very\nsimple query and documents to highlight the difficulty of representing all top- _\ud835\udc58_ sets themselves.\n\n\n**5.2. The LIMIT Dataset**\n\n\n**Dataset Construction** In order to have a natural language version of this dataset, we need some\nway to map combinations of documents into something that could be retrieved with a query. One\nsimple way to do this is to create a synthetic version with latent variables for queries and documents\nand then instantiate it with natural language. For this mapping, we choose to use attributes that\nsomeone could like (i.e. Jon likes Hawaiian pizza, sports cars, etc. ) as they are plentiful and don\u2019t\npresent issues w.r.t. other items: one can like Hawaiian pizza but dislike pepperoni, all preferences\nare valid. We then enforce two constraints for realism: (1) users shouldn\u2019t have too many attributes,\nthus keeping the documents short (less than 50 per user) and (2) each query should only ask for one\nitem to keep the task simple (i.e. \u201cwho likes X\u201d). We gather a list of attributes a person could like\nthrough prompting Gemini 2.5 Pro. We then clean it to a final 1850 items by iteratively asking it to\nremove duplicates/hypernyms, while also checking the top failures with BM25 to ensure no overlap.\n\n\nWe choose to use 50k documents in order to have a hard but relatively small corpus and 1000\nqueries to maintain statistical significance while still being fast to evaluate. For each query, we choose\nto use two relevant documents (i.e. _\ud835\udc58_ =2), both for simplicity in instantiating and to mirror previous\nwork (i.e. NQ, HotpotQA, etc. [Kwiatkowski et al., 2019, Yang et al., 2018]).\n\n\nOur last step is to",
      "I\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So\n\nthe answer is: 1894.\n\n\nQ: When was Neville A. Stanton's employer founded?\nA: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862.\n\nSo the answer is: 1862.\n\n\nQ: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to?\nA: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located\n\nin the Prince County. So the answer is: Prince County.\n\n\nQ: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America\u2212\n\nLite: How Imperial Academia Dismantled Our Culture's author?\nA: The author of America\u2212Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was\n\neducated at the Yale University. The city in Connecticut that has the highest number of Zagat\u2212rated restaurants is New\nHaven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale\nHerald.\n\n\nQ: What is the headquarters for the organization who sets the standards for ISO 21500?\nA: The standards for ISO 21500 were set by International Organization for Standardization. The International Organization\n\nfor Standardization has headquarters in Geneva. So the answer is: Geneva.\n\n\nQ: What did the publisher of Banjo\u2212Tooie rely primarily on for its support?\nA: The publisher of Banjo\u2212Tooie is Nintendo. Nintendo relied primarily for its support on first\u2212party games. So the answer is:\n\nfirst\u2212party games.\n\n\nQ: In which county was the birthplace of the Smoke in tha City performer?\nA: The performer of Smoke in tha City is MC Eiht. MC Eiht's birthplace is Compton. Compton is located in the county of Los\n\nAngeles County. So the answer is: Los Angeles County.\n\n\nQ: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?\n\n\nA: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central\n\nJakarta.\n\n\nQ:",
      " We note that this is the\nbest case: a real embedding model cannot directly optimize the query and document vectors\nto match the test qrel matrix (and is constrained\nby factors such as \u201cmodeling natural language\u201d).\nHowever, these numbers already show that for\nweb-scale search, even the largest embedding\ndimensions with ideal test-set optimization are\nnot enough to model all combinations.\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n|<br>|<br>|~~ritical Po~~<br>Regression|~~ nts~~<br> (Degree 3)||\n|<br>|<br>||||\n||||||\n||||||\n||||||\n\n\nFigure 2 | The critical-n value where the\ndimensionality is too small to successfully\nrepresent all the top-2 combinations. We plot the\ntrend line as a polynomial function.\n\n\n##### **5. Empirical Connection: Real-World Datasets**\n\nThe free embedding experiments provide empirical evidence that our theoretical results hold true.\nHowever, they still are abstract - what does this mean for real embedding models? In this section\nwe (1) draw connections from this theory to existing datasets and (2) create an trivially simple yet\nextremely difficult retrieval task for existing SOTA models.\n\n\n**5.1. Connection to Existing Datasets**\n\n\nExisting retrieval datasets typically use a static evaluation set with limited numbers of queries, as\nrelevance annotation is expensive to do for each query. This means practically that the space of\nqueries used for evaluation is a very small sample of the number of potential queries. For example, the\nQUEST dataset [Malaviya et al., 2023] has 325k documents and queries with 20 relevant documents\nper query, with a total of 3357 queries. The number of unique top-20 document sets that could\nbe returned with the QUEST corpus would be [\ufffd][325] 20 _[\ud835\udc58]_ - which is equal to 7.1e+91 (larger than the\nestimate of atoms in the observable universe, 10 [82] ). Thus, the 3k queries in QUEST can only cover an\ninfinitesimally small part of the qrel combination space.\n\n\n7In preliminary experiments, we found that InfoNCE performed best, beating MSE and Margin. As we are directly\n\n\n\n_",
      ". Metrics Measuring Qrel Graph Density**\n\n\nWe show two metrics that treat the qrel matrix as a graph and show that LIMIT has unique properties\ncompared to standard IR datasets (Table 1). We call these metrics Graph Density and Average Query\nStrength and describe them below.\n\n\n**Graph Density** We use the qrel matrix to construct the graph, where nodes are documents and an\nedge exists between two documents if they are both relevant to at least one common query.\n\n\nFor a given graph _\ud835\udc3a_ = ( _\ud835\udc49, \ud835\udc38_ ) with _\ud835\udc49_ being the set of nodes and _\ud835\udc38_ being the set of edges, the graph\ndensity is defined as the ratio of the number of edges in the graph to the maximum possible number\n\n- f edges. For an undirected graph, the maximum possible number of edges is [|] _[\ud835\udc49]_ [|] [(|] _[\ud835\udc49]_ 2 [|] [\u2212][1][)] . Thus, the\n\ndensity _\ud835\udf0c_ is calculated as:\n\n\n\n_\ud835\udc38_ | 2| _\ud835\udc38_ |\n\n=\n\n_\ud835\udc49_ | - 1) | _\ud835\udc49_ |(| _\ud835\udc49_ | \u2212 1)\n\n2\n\n\n\n| _\ud835\udc38_ |\n_\ud835\udf0c_ =\n\n\n\n| _\ud835\udc49_ | (| _\ud835\udc49_ | - 1)\n\n\n\nThis metric indicates how connected the graph is; a density of 1 signifies a complete graph (all\npossible edges exist), while a density close to 0 indicates a sparse graph. For a qrel dataset, the\n\n\n**Average Query Strength** In a query-query graph where nodes are queries and edges represent\nsimilarity between queries (e.g., Jaccard similarity of their relevant documents), the _strength_ - f a\nquery node _\ud835\udc56_, denoted _\ud835\udc60\ud835\udc56_, is defined as the sum of the weights of all edges incident to it. If _\ud835\udc64\ud835\udc56\ud835\udc57_ is the\nweight of the edge between query _\ud835\udc56_ and query _\ud835\udc57_, and _\ud835\udc41_ ( _\ud835\udc56_ ) is the set of neighbors of query _\ud835\udc56_, then the\nstrength is:\n\n\n_\ud835\udc60\ud835\udc56_ = \ufffd"
    ],
    "response": "The qrel matrix used in Yang et al.'s work, as referenced in the provided materials, serves to instantiate attributes by selecting a matrix with a high number of documents, aiming for over 1000 queries for a top- _\ud835\udc58_ (specifically _\ud835\udc58_ = 2). This results in 46 relevant documents, which corresponds to a selection that is just above 1000 queries, with 46 documents being specifically noted as relevant to one of the queries in their setup. The design of the qrel matrix is influenced by the need for interconnectedness among the documents, suggesting that a denser matrix (with all combinations included) would pose more challenges for models to represent effectively <document id=\"2508.21038_embedding_limit_weller:chunk_8550\">.",
    "reference": "The qrel matrix used in Yang et al.'s work for instantiating attributes has the highest number of documents for which all combinations would be just above 1000 queries for a top- _\ud835\udc58_ - f 2 (46 docs, since [\ufffd][46] 2 - is 1035, the smallest above 1k).",
    "answer_correctness": 0.8366765062,
    "context_precision": 0.7,
    "context_recall": 1.0
  },
  {
    "user_input": "How does MiniCPM-V 2.6 compare to VisRAG-Ret in terms of retrieval efficiency and performance on text retrieval benchmarks?",
    "retrieved_contexts": [
      "br>44.96 (56.1%)<br>80.19 (100%)|55.58 (85.1%)<br>58.99 (90.4%)<br>58.63 (89.8%)<br>65.29 (100%)|58.12 (80.0%)<br>58.32 (80.2%)<br>59.03 (81.0%)<br>73.35 (100%)|\n\n\n\nIn this section, we present supplementary evaluation results for both retrieval and generation on our\ndataset.\n\n\nTable 10 shows additional retrieval results obtained by applying reciprocal rank fusion (RRF) (Cormack et al., 2009) to combine the outputs of MiniCPM (OCR) and SigLIP. It is a straightforward\nmethod to integrate textual information extracted from the page with its visual clues. The results\nindicate that fusing text and image modalities provides a meaningful performance boost over individual modality baselines. However, this approach still falls short of the performance achieved\nby our VisRAG-Ret model (71.49 for out-of-domain, 77.91 for in-domain). This underscores the\nsuperior capability of VisRAG-Ret in understanding both modalities within a unified architecture.\n\n\nTable 11 provides additional generation results using top-6 and top-10 retrieved documents from\nVisRAG-Ret. For these experiments, we evaluate the performance of MiniCPM-V 2.0 using the\npage concatenation method and MiniCPM-V 2.6 with direct feeding. We also report the performance of another SOTA VLM, Qwen2-VL-7B-Instruct (Wang et al., 2024). The results indicate\nsignificant performance degradation when handling a larger number of retrieved pages, for both\npage concatenation (MiniCPM-V 2.0) and multi-page input (MiniCPM-V 2.6). MiniCPM-V 2.6\nexhibits greater robustness to increasing context compared to MiniCPM-V 2.0. Open-source VLMs\nstill face challenges in reasoning over multiple pages and extracting relevant information from noisy\nretrieved data. Results for Qwen2-VL demonstrate stronger document understanding capabilities,\n\n- utperforming MiniCPM-V 2.6 in these tasks.\n\n\nH RETRIEVAL EFFICIENCY",
      " pages and extracting relevant information from noisy\nretrieved data. Results for Qwen2-VL demonstrate stronger document understanding capabilities,\n\n- utperforming MiniCPM-V 2.6 in these tasks.\n\n\nH RETRIEVAL EFFICIENCY\n\n\nIn this experiment, we evaluate the retrieval efficiency of VisRAG-Ret and MiniCPM (OCR) by\nmeasuring two key components: offline document parsing and encoding latency, and online query\nencoding and search latency. Query and document encoding are conducted on an NVIDIA A100\n40G GPU with a batch size of 1, while document parsing is performed on a single core of an Intel\nXeon Platinum 8350C CPU. The reported latencies are averaged over the queries and documents\nfrom the PlotQA dataset. The results are summarized in Table 12.\n\n\nAs shown in the table, although VisRAG-Ret, a VLM-based model, requires more time for document\nencoding compared to MiniCPM (OCR), it bypasses the time-consuming parsing stage required by\n\n\n24\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 12: Retrieval efficiency (ms). We report offline latencies per document, including document\nparsing and encoding latencies, as well as online latencies per query, including query encoding and\nsearch latencies.\n\n|Col1|OfflineLatencyperDocument<br>Parsing Encoding Total|OnlineLatencyperQuery<br>Encoding Search Total|\n|---|---|---|\n|MiniCPM (OCR)<br>VisRAG-Ret|284<br>28<br>312<br>\u2013<br>121<br>121|28<br>26<br>54<br>28<br>26<br>54|\n\n\n\nMiniCPM (OCR). This leads to a 58% reduction in total document processing time for VisRAG-Ret.\nFor online query processing, the latencies of VisRAG-Ret and MiniCPM (OCR) are nearly identical,\nas the queries consist solely of textual inputs.\n\n\nI RETRIEVAL PERFORMANCE ON TEXT RETRIEVAL BENCHMARKS\n\n\nTable 13: Retrieval performance on subsets of the text retrieval benchmark BEIR (Thakur et al.,\n2021) in NDCG@10. VisRAG-Ret performs retrieval on rendered document screenshots.\n\n\n**Model** **SciFact** **NFCorpus** **Scidocs**\n\n\nMiniCP",
      " labels are available. We also report the performance\n\n     - f ColPali (Faysse et al., 2024) on our evaluation data. ColPali is a page embedding model\nthat encodes a screenshot of a page into multiple vectors. We train ColPali on our dataset\nusing the official code and hyper-parameters provided in its paper.\n\n\n**Generation Experiments.** To evaluate generation performance, we fix the retrieval model to\nVisRAG-Ret and report the performance of various generation models and methods. For VisRAGGen, we compare the performance of the single-image VLM MiniCPM-V 2.0, which only accepts a\nsingle image, against the multi-image VLM MiniCPM-V 2.6 (OpenBMB, 2024b; Yao et al., 2024)\nand GPT-4o (OpenAI, 2024). MiniCPM-V 2.6 is an upgrade of MiniCPM-V 2.0, incorporating\nQwen2-7B (Yang et al., 2024) as the language model and supporting multi-image input. We evaluate\nthe performance of page concatenation and weighted selection on the single-image VLM. Additionally, we report the performance of text-based generation baselines, including MiniCPM (OCR) and\nGPT-4o (OCR), where only extracted texts are used for generation. For all experiments, we report\nresults using the top-1, top-2, and top-3 retrieved documents, as well as an \u201cOracle\u201d condition where\nthe model is provided with only the positive document(s) to show the performance upper bound.\n\n\n**Implementation Details.** VisRAG-Ret is fine-tuned using in-batch negatives (Karpukhin et al.,\n2020) for one epoch with a batch size of 128 on 8 NVIDIA A100 80GB GPUs. The temperature\nparameter in Equation 2 is set to 0.02. Baseline retrievers are fine-tuned with the same hyperparameters, and textual baselines utilize extracted text data as document-side input. The generation\npart does not use any fine-tuning; we directly use off-the-shelf LLMs/VLMs for generation.\n\n\n5 EVALUATION RESULTS\n\n\nIn this section, we first present the overall performance of VisRAG (Sec. 5.1), followed by analyses",
      "1344x1344) at any aspect ratio. We use MiniCPM-V 2.0 to\nbuild VisRAG-Ret (Table 2) and VisRAG-Gen (Table 3(b)), as well as the document parsing model.\n\n\n**MiniCPM-V 2.6** (OpenBMB, 2024b; Yao et al., 2024) is an upgrade of MiniCPM-V 2.0 and\nMiniCPM-Llama3-V 2.5 (Yao et al., 2024). It is built upon SigLIP-400M and Qwen2-7B (Yang\net al., 2024) with a total of 8.5B parameters, exihibiting a significant performance improvement\n\n- ver MiniCPM-Llama3-V 2.5 (Yao et al., 2024). Different from previous models, MiniCPM-V\n2.6 can accept multiple images as the input and perform multi-modal in-context learning. It also\ndemonstrates stronger OCR capabilities. We use MiniCPM-V 2.6 to build VisRAG-Gen (Table 3)\nand a text-based generation baseline MiniCPM-V 2.6 (OCR) (Figure 3, Figure 5).\n\n\nNote that, MiniCPM-Llama3-V 2.5 (Yao et al., 2024) is not used in this paper.\n\n\n**GPT-4o** (OpenAI, 2024) is OpenAI\u2019s latest multi-modal model, capable of processing any combination of text, audio, image, and video inputs and generating outputs in text, audio, and image\nformats. We use GPT-4o to construct VisRAG-Gen (Table 3) and to synthesize training data.\n\n\nD RETRIEVAL PERFORMANCE IN RECALL@10\n\n\nTable 6 presents the retrieval performance in Recall@10.\n\n\nE PROMPTS FOR GENERATION\n\n\nWe present the prompts of VisRAG-Gen and TextRAG-Gen in Table 7.\n\n\n2https://huggingface.co/HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit\n\n\n20\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 6: Overall retrieval performance in Recall@10.\n\n\nTable 7: Prompt templates for generation. \u201c",
      "Gen models, which\n\n\n8\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nFigure 3: Pipeline performance of (a) TextRAG and (b) VisRAG on InfographicsVQA. We visualize\nthe portion of queries that have the positive document retrieved at the top-1 position (\u201cCorrect Retrieval\u201d), and that are answered correctly given the top-1 retrieved document (\u201cCorrect Generation\u201d).\n\n\nrely solely on extracted text. For instance, MiniCPM-V 2.0 achieves 30% higher performance than\nMiniCPM (OCR) when using ground-truth documents. This underscores the importance of visual\nclues in extracting answers from documents.\n\n\nIn practical scenarios where models receive the top-1 to 3 retrieved documents, which may include noise, VisRAG-Gen consistently outperforms TextRAG-Gen within the same model series.\nSpecifically, for MiniCPM-V 2.0, capable of processing only a single image, the weighted selection\napproach demonstrates better performance than page concatenation when handling 2 or 3 retrieved\ndocuments. However, neither method shows a performance improvement as the number of retrieved\ndocuments increases, a trend commonly observed in TextRAG pipelines (Zhu et al., 2024). In contrast, MiniCPM-V 2.6 and GPT-4o, both capable of processing multiple images as input, exhibit\na notable performance gain as the number of retrieved documents increases, suggesting that only\nVLMs pre-trained on multi-image data can effectively reason over multiple retrieved pages.\n\n\n**End-to-end Performance.** In this experiment, we study the effectiveness of the VisRAG _pipeline_,\nby comparing it with the TextRAG pipeline. We construct TextRAG using MiniCPM (OCR) and\nMiniCPM-V 2.6 (OCR) for retrieval and generation, respectively, and VisRAG using VisRAG-Ret\nfor retrieval and MiniCPM-V 2.6 for generation. The performance on InfographicsVQA is visually\nrepresented in Figure 3. Notebly, VisRAG achieves a higher rate of accurately retrieving documents than TextRAG, and demonstrates a significantly improved rate of correct answer generation\nfrom accurately retrieved documents. The cumulative improvements in both retrieval and generation phases result in an overall accuracy increment from 25% to 51%. Across the six evaluation\ndatasets, VisRAG shows"
    ],
    "response": "MiniCPM-V 2.6 is positioned as an upgrade over MiniCPM-V 2.0 and showcases enhanced capabilities, particularly in processing multiple images, while maintaining strong OCR capabilities. However, when comparing its performance and retrieval efficiency with VisRAG-Ret, there are notable differences.\n\nIn terms of **retrieval efficiency**, VisRAG-Ret exhibits a 58% reduction in total document processing time because it bypasses the parsing stage required by MiniCPM (OCR), albeit it has longer document encoding latencies. MiniCPM (OCR) has faster offline document processing but lacks the comprehensive processing power of VisRAG-Ret, which is designed to handle both textual and visual inputs effectively (<document 2410.10594_vision_rag_yu:chunk_25200>).\n\nRegarding **performance on text retrieval benchmarks**, VisRAG-Ret outperforms MiniCPM-V 2.6, particularly amidst multi-modal scenarios. The retrieval performance metrics show that fusing text and image modalities within VisRAG-Ret offers significant enhancements compared to the single-modality retrieval approaches (which MiniCPM-V 2.6 may rely on) (<document 2410.10594_vision_rag_yu:chunk_24750>).\n\nUltimately, while MiniCPM-V 2.6 is an advanced model with improved multi-image handling, VisRAG-Ret leads in both efficiency and benchmark performance, particularly in understanding and integrating multi-modal information.",
    "reference": "MiniCPM-V 2.6 outperforms VisRAG-Ret in retrieval efficiency, demonstrating stronger document understanding capabilities. VisRAG-Ret requires more time for document encoding compared to MiniCPM (OCR), but it bypasses the time-consuming parsing stage required by MiniCPM (OCR). This leads to a 58% reduction in total document processing time for VisRAG-Ret. On text retrieval benchmarks, VisRAG-Ret performs well on subsets of the text retrieval benchmark BEIR.",
    "answer_correctness": 0.5236736512,
    "context_precision": 1.0,
    "context_recall": 0.75
  },
  {
    "user_input": "How does the use of DeBERTa-v3-Large in evaluating RAG systems contribute to their accuracy?",
    "retrieved_contexts": [
      ", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation",
      " we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\n\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70.0%, 72.5%, 75.0%, ..., 90.0%).\nEach split also represents a different mock RAG\nsystem. Since we know the success percentages of\neach dataset split, we know the appropriate ranking of each mock RAG system. This allows us to\n\n\ntest ARES success at both scoring and ranking the\nmock RAG systems appropriately across the three\nevaluation criteria.\n\n\n**4.3** **Metrics**\n\n\nTo calculate the correlation between the correct\n\nranking and the ARES ranking, we use the Kendall\nrank correlation coefficient or Kendall\u2019s _\u03c4_ :\n\n\n_\u03c4_ = [(#][ of concordant pairs][)] _[ \u2212]_ [(#][ of discordant pairs][)]\n\n# of pairs total\n\n\nConcordant pairs are defined as two ordinal values in the ranking where the earlier value in the\nsequence is lower than the later value in the sequence. Discordant pairs are defined as two ordinal\nvalues in the ranking where the earlier value in the\nsequence is greater than or equal to the later value\nin the sequence. A Kendall\u2019s _\u03c4_ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document preprocessing. We want to make sure that ARES has satisfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall\u2019s _\u03c4_ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pairwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in\ninformation retrieval, allowing developers to evaluate ranking systems empirically. Therefore, we\nbelieve Kendall\u2019s tau and prediction accuracy provide meaningful metrics for testing the efficacy of\nARES as a RAG evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges",
      ".05 to 0.30, the ability to\ncheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of\nannotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we\ngenerate more GPT-4 generated labels. In the table, we define PPI range as the number of percentage points from\nthe lower number to the upper number of the PPI confidence bounding. Additionally, we use the fine-tuned LLM\njudge (DeBERTa-v3-Large) for evaluation.\n\n\n**ARES Ranking of Real RAG Systems**\n\n\nNQ WoW FEVER\n\n\nC.R. A.R. C.R. A.R. C.R. A.R.\n\n\nKendall\u2019s Tau for\n0.73 0.78 0.73 0.73 0.73 0.82\nSampled Annotations\n\nKendall\u2019s Tau\n0.82 0.82 0.73 0.82 0.73 0.87\nfor RAGAS\n\n\nKendall\u2019s Tau\n0.82 0.87 0.82 0.82 0.64 0.87\nfor GPT-3.5 Judge\n\nKendall\u2019s Tau\n0.91 **0.96** **0.91** **1.0** 0.73 0.87\nfor ARES LLM Judge\n\nKendall\u2019s Tau\n**1.0** **0.96** **0.91** **1.0** **0.82** **1.0**\nfor ARES\n\n\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\n\n\nTable 5: **ARES Ranking on Real-World RAG Systems** : For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against",
      " confidence set of the ML\nmodel\u2019s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence\ninterval for the performance of the evaluated RAG\nsystem (e.g. its context relevance, answer faithfulness, or answer relevance accuracy individually)\ncompared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the human preference validation set with the much larger\nset of datapoints with ML predictions, PPI can develop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\n\n\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confidence bounds for the success and failure rates of the\n\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (probability) for our confidence interval.\n\n\nWith the accuracy confidence interval for each\ncomponent of the RAG, we find the midpoint of\neach confidence interval and use the midpoints to\nrank the RAG systems. With our ranking, we can\ncompare different RAG systems, as well as different configurations of the same RAG system, to find\nthe best-performing approach for a given domain.\n\n\n**4** **Experiments**\n\n\n**4.1** **Models**\n\n\nFor our fine-tuned judges, ARES relies on generating cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use OpenAI\u2019s _gpt-3.5-turbo-16k_, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similarity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand Open",
      "## **ARES: An Automated Evaluation Framework for Retrieval-Augmented** **Generation Systems**\n\n\n\n**Jon Saad-Falcon**\nStanford University _[\u2217]_\n\njonsaadfalcon@stanford.edu\n\n\n**Christopher Potts**\nStanford University\ncgpotts@stanford.edu\n\n\n**Abstract**\n\n\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an _Automated RAG Evaluation_\n_System_, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By creating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or documents used in the evaluated RAG systems. We\nmake our code and datasets publicly available\n\n[on Github.](https://github.com/stanford-futuredata/ARES)\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to generate a response. This formulation admits a multitude\n\n- f choices: what retrieval model to use, how to di\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab"
    ],
    "response": "The use of DeBERTa-v3-Large in evaluating Retrieval-Augmented Generation (RAG) systems enhances their accuracy by serving as a fine-tuned judge within the ARES framework. ARES leverages this model to assess the quality of individual RAG components, specifically focusing on context relevance, answer faithfulness, and answer relevance. The evaluation is based on generating synthetic datasets, which allows for high-quality assessments without relying on external APIs.\n\nIn practical applications, DeBERTa-v3-Large contributes to a more effective ranking of RAG systems across various domains. For example, in evaluations, the ARES judges utilizing DeBERTa-v3-Large demonstrated a Kendall's Tau correlation coefficient of up to 1.0, indicating a perfect ranking accuracy. This performance reflects the model's ability to measure distinctions between RAG systems reliably, thus improving overall evaluation outcomes compared to traditional methods that rely on hand annotations (<document 2311.09476_ares_saad_falcon:chunk_4950>, <document 2311.09476_ares_saad_falcon:chunk_15300>, <document 2311.09476_ares_saad_falcon:chunk_0>). \n\nOverall, integrating DeBERTa-v3-Large into the evaluation pipeline of RAG systems allows for cost-effective, robust, and accurate assessments, significantly enhancing the performance of these systems in multi-faceted tasks (<document 2311.09476_ares_saad_falcon:chunk_3600>).",
    "reference": "The use of DeBERTa-v3-Large as the pretrained basis for the judges in evaluating RAG systems contributes to their accuracy by providing a fine-tuned language model that enhances the system's ability to score context relevance and answer relevance. This leads to higher Kendall\u2019s Tau values, indicating successful pairwise comparisons across various performance gaps between RAG systems.",
    "answer_correctness": 0.8247045107,
    "context_precision": 1.0,
    "context_recall": 1.0
  }
]