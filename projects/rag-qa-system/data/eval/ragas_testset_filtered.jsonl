{"user_input": "Who are the authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering'?", "reference_contexts": ["## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice ["], "reference": "The authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering' are Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.", "persona_name": "Research Scientist in Information Retrieval", "query_style": "WEB_SEARCH_LIKE", "query_length": "LONG", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the margin value used in triplet loss?", "reference_contexts": ["A** **Distant Supervision**\n\n\nWhen training our final DPR model using Natural\nQuestions, we use the passages in our collection\nthat best match the gold context as the positive\npassages. As some QA datasets contain only the\nquestion and answer pairs, it is thus interesting\nto see when using the passages that contain the\nanswers as positives (i.e., the distant supervision\nsetting), whether there is a significant performance\ndegradation. Using the question and answer together as the query, we run Lucene-BM25 and pick\nthe top passage that contains the answer as the positive passage. Table 5 shows the performance of\nDPR when trained using the original setting and\nthe distant supervision setting.\n\n\n**B** **Alternative Similarity Functions &**\n**Triplet Loss**\n\n\nIn addition to dot product (DP) and negative loglikelihood based on softmax (NLL), we also experiment with Euclidean distance (L2) and the triplet\nloss. We negate L2 similarity scores before applying softmax and change signs of question-topositive and question-to-negative similarities when\napplying the triplet loss on dot product scores. The\nmargin value of the triplet loss is set to 1. Table 6 summarizes the results. All these additional\n\nexperiments are conducted using the same hyperparameters tuned for the baseline (DP, NLL).\nNote that the retrieval accuracy for our \u201cbaseline\u201d\nsettings reported in Table 5 (Gold) and Table 6\n(DP, NLL) is slightly better than those reported in\nTable 3. This is due to a better hyper-parameter\nsetting used in these analysis experiments, which\nis documented in our code release.\n\n\n**C** **Qualitative Analysis**\n\n\nAlthough DPR performs better than BM25 in general, the retrieved passages of these two retrievers\nactually differ qualitatively. Methods like BM25\nare sensitive to highly selective keywords and\nphrases, but cannot capture lexical variations or semantic relationships well. In contrast, DPR excels\nat semantic representation, but might lack sufficient\ncapacity to represent salient phrases which appear\nrarely. Table 7 illustrates this phenomenon with\ntwo examples. In the first example, the top scoring passage from BM25 is irrelevant, even though\nkeywords such as _England_ and _Ireland_ appear multiple times. In comparison, DPR is able to return\n\n\n\n**Top-1** **Top-5** **Top-20"], "reference": "The margin value of the triplet loss is set to 1.", "persona_name": "Research Scientist in Information Retrieval", "query_style": "MISSPELLED", "query_length": "SHORT", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What are the potential reasons for lower performance on SQuAD?", "reference_contexts": ["-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\n\n- nly 500+ Wikipedia articles and thus the distribution of training examples is extremely biased, as\nargued previously by Lee et al. (2019).\n\n\n**5.2** **Ablation Study on Model Training**\n\n\nTo understand further how different model training\n\n- ptions affect the results, we conduct several additional experiments and discuss our findings below.\n\n\n\n\n\n\n**Sample efficiency** We explore how many training examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top- _k_\nretrieval accuracy with respect to different numbers of training examples, measured on the devel\n- pment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 examples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question\u2013passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently.\n\n\n**In-batch negative training** We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3.\nThe top block is the standard 1-of- _N_ training setting, where each question in the batch is paired\nwith a positive passage and its own set of _n_ negative passages (Eq. (2)). We find that the choice\n\n- f negatives \u2014 random, BM25 or gold passages\n(positive passages from other questions) \u2014 does\nnot impact the top- _k_ accuracy much in this setting\nwhen _k \u2265_ 20.\n\nThe middle bock is the in-batch negative training\n(Section 3.2) setting. We find that using a similar\nconfiguration (7 gold negative passages), in-batch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\n\n- r from the whole training set. Effectively, in-batch\nnegative training is an easy and memory"], "reference": "The lower performance on SQuAD is due to two main reasons. First, annotators wrote questions after seeing the passage, leading to high lexical overlap between passages and questions, which gives BM25 an advantage. Second, the data was collected from only 500+ Wikipedia articles, resulting in a biased distribution of training examples.", "persona_name": "Research Scientist in Information Retrieval", "query_style": "WEB_SEARCH_LIKE", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is a corpus in the context of information retrieval?", "reference_contexts": [" are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\n\n- pen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve comparable or better results on multiple QA datasets in the\n\n- pen-retrieval setting, compared to several, much\ncomplicated systems.\n\n\n**2** **Background**\n\n\nThe problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \u201c _Who first voiced Meg on Family_\n_Guy?_ \u201d or \u201c _Where was the 8th Dalai Lama born?_ \u201d, a\nsystem is required to answer it using a large corpus\n\n- f diversified topics. More specifically, we assume\n\n\n\nthe extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more passages in the corpus. Assume that our collection\ncontains _D_ documents, _d_ 1 _, d_ 2 _, \u00b7 \u00b7 \u00b7, dD_ . We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units [3] and get _M_\ntotal passages in our corpus _C_ = _{p_ 1 _, p_ 2 _, . . ., pM_ _}_,\nwhere each passage _pi_ can be viewed as a sequence\n\n- f tokens _w_ [(] _[i]_ [)]\n1 _[, w]_ 2 [(] _[i]_ [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _|_ [(] _p_ _[i]_ [)] _i|_ [. Given a question] _[ q]_ [,]\n\nthe task is to find a span _ws_ [(] _[i]_ [)] _[, w]_ _s_ [(] _[i]_ +1 [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _e_ [(] _[i]_ [)] from\n\n- ne of the passages _pi_ that can"], "reference": "A corpus in the context of information retrieval refers to a large collection of diversified topics, which is used for training and evaluating algorithms, particularly in tasks like question answering.", "persona_name": "Research Scientist in Information Retrieval", "query_style": "WEB_SEARCH_LIKE", "query_length": "SHORT", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the main contribution of Facebook AI's research paper at ICLR 2021?", "reference_contexts": ["Published as a conference paper at ICLR 2021\n\n## ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS\n### WITH MULTI-HOP DENSE RETRIEVAL\n\n\n**Wenhan Xiong** [1] _[\u2217]_ **Xiang Lorraine Li** [2] _[\u2217]_ **Srinivasan Iyer** _[\u2021]_ **Jingfei Du** _[\u2021]_\n\n\n**Patrick Lewis** _[\u2021\u2020]_ **William Wang** [1] **Yashar Mehdad** _[\u2021]_ **Wen-tau Yih** _[\u2021]_\n\n\n**Sebastian Riedel** _[\u2021\u2020]_ **Douwe Kiela** _[\u2021]_ **Barlas O\u02d8guz** _[\u2021]_\n\n\n_\u2021_ Facebook AI\n1University of California, Santa Barbara\n2University of Massachusetts Amherst\n\n_\u2020_ University College London\n_{_ xwhan, william _}_ @cs.ucsb.edu, xiangl@cs.umass.edu,\n_{_ sviyer, jingfeidu, plewis, mehdad, scottyih, sriedel, dkiela, barlaso _}_ @fb.com\n\n\nABSTRACT\n\n\nWe propose a simple and efficient multi-hop dense retrieval approach for answering\ncomplex open-domain questions, which achieves state-of-the-art performance on\ntwo multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previ\n     - us work, our method does not require access to any corpus-specific information,\nsuch as inter-document hyperlinks or human-annotated entity markers, and can\nbe applied to any unstructured text corpus. Our system also yields a much better\nefficiency-accuracy trade-off, matching the best published accuracy on HotpotQA\nwhile being 10 times faster at inference time. [1]\n\n\n1 INTRODUCTION\n\n\n_Open domain question answering_ is a challenging task where the answer to a given question needs to\nbe extracted from a large pool of documents. The prevailing approach (Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set"], "reference": "The main contribution of Facebook AI's research paper at ICLR 2021 is proposing a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, their method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Their system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.", "persona_name": "Research Scientist", "query_style": "PERFECT_GRAMMAR", "query_length": "LONG", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What are the performance comparisons between BERT-wwm reader and ELECTRA reader on HotpotQA dev set?", "reference_contexts": ["2019) explicitly encode the hyperlink graph structure within their\nanswer prediction models. In fact, this particular inductive bias provides a perhaps unreasonably\nstrong advantage in the specific context of HotpotQA, which by construction guarantees groundtruth passage sequences to follow hyperlinks. Despite not using such prior knowledge, our model\n\n- utperforms all previous systems by large margins, especially on supporting fact prediction, which\nbenefits more directly from better retrieval.\n\n\n\n**Reader** **Model** **Variants** Results for\nreader model variants are shown in Table 6. [7]\n\nTable 6: Reader comparison on HotpotQA dev set.\n\nFirst, we see that the BERT-wwm reader\nis 1-2% worse than the ELECTRA reader Model Top k EM F1\nwhen using enough passages. However, ELECTRA Top 50 61.7 74.3\nit still outperforms the results in (Asai Extractive ELECTRA Top 250 63.4 76.2\net al., 2020) which also uses BERT-wwm BERT-wwm Top 250 61.5 74.7\nfor answer prediction. While RAG and Multi-hop RAG Top 4*4 51.2 63.9\n\nGenerative\n\nFiD have shown strong improvements over FiD Top 50 61.7 73.1\nextractive models on single-hop datasets\nsuch as NaturalQuestions (Kwiatkowski\net al., 2019), they do not show an advantage in the multi-hop case. Despite having twice as many\nparameters as ELECTRA, FiD fails to outperform it using the same amount of context (top 50). In\ncontrast, on NaturalQuestions, FiD is 4 points better than a similar extractive reader when using the\ntop 100 passages in both. [8] We hypothesize that the improved performance on single-hop questions is\ndue to the ability of larger pretrained models to more effectively memorize single-hop knowledge\nabout real-world entities. [9] Compared to multi-hop questions that involve multiple relations and\nmissing entities, simple questions usually only ask about a certain property of an entity. It is likely\nthat such simple entity-centric information is explicitly mentioned by a single text piece in the\npretraining corpus, while the evidence for multihop questions is typically dispersed, making the\n\n\n7"], "reference": "The BERT-wwm reader is 1-2% worse than the ELECTRA reader when using enough passages. However, ELECTRA still outperforms the results in (Asai et al., 2020) which also uses BERT-wwm for answer prediction.", "persona_name": "Research Scientist in Information Retrieval", "query_style": "WEB_SEARCH_LIKE", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the purpose of HotpotQA in information retrieval?", "reference_contexts": ["; and _comparison_\nquestions where two entities are mentioned simultaneously and compared in some way. In Figure 2,\nwe show the retrieval performance of both question\ntypes. The case of _comparison_ questions proves easier, since both entities needed for retrieval are present\nin the question.\n\n\nThis case appears almost solved, confirming recent\nwork demonstrating that dense retrieval is very effective at entity linking (Wu et al., 2019).\n\n\n\nFigure 2: The retrieval performance gap between comparison and bridge questions. Left:\nrecall of groundtruth passage sequences with\n- ut reranking. Right: Top-1 chain exact\nmatch after reranking.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the case of _bridge_ questions, we manually in\nrecall of groundtruth passage sequences with\nspect 50 randomly sampled erroneous examples after\n\n                          - ut reranking. Right: Top-1 chain exact\n\nreranking. We find that in half of these cases, our re\nmatch after reranking.\n\ntrieval model predicts an alternative passage sequence\nthat is also valid (see Appendix A.1 for examples).\nThis gives an estimated top-1 passage sequence accuracy of about 90%. Other remaining errors are\ndue to the dense method\u2019s inability to capture the exact n-gram match between the question and\npassages. This is a known issue (Lee et al., 2019; Karpukhin et al., 2020) of dense retrieval methods\nwhen dealing with questions that have high lexical overlap with the passages. To this end, a hybrid\nmulti-hop retrieval method with both term and dense index might be used to further improve the\nperformance on _bridge_ questions.\n\n\n\n**Retriever Ablation Study** In Table 3, we examine our model with different variations on HotpotQA\nto show the effectiveness of each proposed component. We see that further training with a memory\nbank results in modest gains, while using a shared encoder is crucial for the best performance.\nRespecting the ordering of passages in two hops is essential - training in an order-agnostic manner\nhardly works at all, and underperforms even the single-hop baseline. Finally, not using hyperlinked\nparagraphs from TF-IDF passages as additional negatives has only a minor impact on performance.\n\n\n**Question Decomposition for Retrieval** As multi-hop questions have more complex structures\nthan simple questions, recent studies (Min et al., 2019; Perez et al., 2020) propose to use"], "reference": "HotpotQA is used for developing advanced models to improve information retrieval and understanding.", "persona_name": "Research Scientist", "query_style": "MISSPELLED", "query_length": "SHORT", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the birthday of the author of 'She Walks These Hills'?", "reference_contexts": [" by the decomposed system. These errors\ncould be potentially avoided if the model has access to the full information in the original question or\nprevious hop results. The important clue for correctly retrieving the documents or avoiding errors is\nmarked in blue. Once decomposed, the marked information are not longer available in one of the\ndecomposed retrieval hop.\n\n\n**Multi-hop Question** : What is the birthday of the author of \u201dShe Walks These Hills\u201d?\n**Decomposed Questions** :\n1. Who is the author of She Walks These Hills?\n2. What is the birthday of Sharyn McCrumb?\n**Ground-truth SP Passages** :\nShe Walks These Hills: She Walks These Hills is a book written by Sharyn McCrumb and\npublished by Charles Scribner\u2019s Sons in 1994, which later went on to win the Anthony Award\nfor Best Novel in 1995.\nSharyn McCrumb: Sharyn McCrumb (born February 26, 1948) is an American writer whose\n~~books celebrate th~~ e history and folklore of Appalachia. McCrumb is the winner of numerous\nliterary awards...\n**Decomposed Error Case:**\n1. She Walks These Hills (\u2713)\n2. Tane McClure\u00b4 : Tan\u00b4e M. McClure (born June 8, 1958) is an American singer and actress.\n\n\n**Multi-hop Question:** When was the album with the song Unbelievable by American rapper\nThe Notorious B.I.G released?\n**Decomposed Questions:**\n1. What is the album with the song Unbelievable by American rapper The Notorious B.I.G?\n2. When was the album Ready to Die released?\n**Ground-truth SP Passages:**\nUnbelievable (The Notorious B.I.G. song): Unbelievable is a song by American rapper The\n~~Notorious B.I.G., recorded for his debut st~~ udio album Ready to Die...\nReady to Die: Ready to Die is the debut studio album by American rapper The Notorious B.I.G.;\n~~it was release~~ d on September 13, 1994, by Bad Boy Records and Arista Records...\n**Decomposed Error Case:**\n1. Unbelievable (The Notorious B.I.G. song) (\u2713)\n2. Ready to Die (The Stooges album"], "reference": "Sharyn McCrumb was born on February 26, 1948.", "persona_name": "AI Researcher", "query_style": "MISSPELLED", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the role of downstream models in the retrieval process described?", "reference_contexts": ["downstream models, our system can match the best published result while being 10x faster.\n\n\n2 METHOD\n\n\n2.1 PROBLEM DEFINITION\n\n\nThe retrieval task considered in this work can be described as follows (see also Figure 1). Given a\nmulti-hop question _q_ and a large text corpus _C_, the retrieval module needs to retrieve a sequence of\npassages _Pseq_ : _{p_ 1 _, p_ 2 _, ..., pn}_ that provide _sufficient_ information for answering _q_ . Practically, the\nretriever returns the _k_ best-scoring sequence candidates, _{Pseq_ [1] _[,][ P]_ _seq_ [2] _[, ...,][ P]_ _seq_ _[k]_ _[}]_ [ (] _[k][ \u226a|C|]_ [), with the]\nhope that at least one of them has the desired qualities. _k_ should be small enough for downstream\nmodules to process in a reasonable time while maintaining adequate recall. In general, retrieval also\nneeds to be efficient enough to handle real-world corpora containing millions of documents.\n\n\n2.2 MULTI-HOP DENSE RETRIEVAL\n\n\n**Model** Based on the sequential nature of the multi-hop retrieval problem, our system solves it in an\niterative fashion. We model the probability of selecting a certain passage sequence as follows:\n\n\n\n_P_ ( _Pseq|q_ ) =\n\n\n\n_n_\n\n- _P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) _,_\n\n\n_t_ =1\n\n\n2\n\n\nPublished as a conference paper at ICLR 2021\n\n\nwhere for _t_ = 1, we only condition on the original question for retrieval. At each retrieval step, we\nconstruct a new query representation based on previous results and the retrieval is implemented as\nmaximum inner product search over the dense representations of the whole corpus:\n\n\nexp ( _\u27e8_ _**p**_ _t,_ _**q**_ _t\u27e9_ )\n_P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) =\n\n~~\ufffd~~ _p\u2208C_ [exp (] _[\u27e8]_ _**[p]**_ _[,]_ _**[ q]**_ _[t][\u27e9]_ [)] _[,]_"], "reference": "Downstream models are used to process the sequence candidates retrieved by the retriever and determine which one has the desired qualities.", "persona_name": "AI Researcher", "query_style": "PERFECT_GRAMMAR", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What conference paper introduced WebQuestions?", "reference_contexts": ["Published as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto finetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved significant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\n\n- f questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model\u2019s hidden states. However, their method requires an initial round\n\n- f TF-IDF/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\n"], "reference": "WebQuestions was published as a conference paper at ICLR 2021.", "persona_name": "AI Researcher", "query_style": "WEB_SEARCH_LIKE", "query_length": "SHORT", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What are structured state space models (SSMs) and how do they address the limitations of Transformers?", "reference_contexts": ["## Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\nAlbert Gu [\u2217][1] and Tri Dao [\u2217][2]\n\n\n1\nMachine Learning Department, Carnegie Mellon University\n\n\n2\nDepartment of Computer Science, Princeton University\nagu@cs.cmu.edu, tri@tridao.me\n\n\n**Abstract**\n\n\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the\nTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,\ngated convolution and recurrent models, and structured state space models (SSMs) have been developed to address\nTransformers\u2019 computational inefficiency on long sequences, but they have not performed as well as attention on important\nmodalities such as language. We identify that a key weakness of such models is their inability to perform content-based\nreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses\ntheir weakness with discrete modalities, allowing the model to _selectively_ propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though this change prevents the use of efficient\nconvolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a\nsimplified end-to-end neural network architecture without attention or even MLP blocks ( **Mamba** ). Mamba enjoys fast\ninference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves\n\n    - n real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art\nperformance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model\n\n    - utperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream\nevaluation.\n\n### **1 Introduction**\n\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged\nas an effective paradigm in modern machine learning. The backbone of these FMs are often _sequence models_, operating on\narbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and\ngenomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al"], "reference": "Structured state space models (SSMs) are a type of architecture developed to improve information retrieval and understanding. They address the limitations of Transformers by allowing content-based reasoning, selectively propagating or forgetting information along the sequence length dimension depending on the current token.", "persona_name": "Research Scientist", "query_style": "PERFECT_GRAMMAR", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the ability of Mamba to solve the induction heads task?", "reference_contexts": [" \u201cPotter\u201d by copying from history.\n\n\n**Dataset.** We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is\ncomparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We additionally investigate\ngeneralization and extrapolation abilities by evaluating on a range of sequence lengths from 2 [6] = 64 up to 2 [20] = 1048576 at\ntest time.\n\n\n**Models.** Following established work on induction heads, we use 2 layer models, which allows attention to mechanistically\nsolve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads, with various positional\nencodings) and SSM variants. We use a model dimension _\ud835\udc37_ - f 64 for Mamba and 128 for the other models.\n\n\n**Results.** Table 2 shows that Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the task perfectly\nbecause of its ability to selectively remember the relevant token while ignoring everything else in between. **It generalizes**\n**perfectly to million-length sequences, or** 4000\u00d7 **longer than it saw during training**, while no other method goes\nbeyond 2\u00d7.\n\n\n10\n\n\nModel Arch. Layer Acc.\n\n\nS4 No gate S4 18.3\n\n  - No gate S6 **97.0**\n\n\nH3 H3 S4 57.0\n\nHyena H3 Hyena 30.1\n\n  - H3 S6 **99.7**\n\n\n  - Mamba S4 56.4\n\n  - Mamba Hyena 28.4\nMamba Mamba S6 **99.8**\n\n\nTable 1: ( **Selective Copying** .)\nAccuracy for combinations of architectures\nand inner sequence layers.\n\n\n\n\n\n\n\n\n\n|Induction Heads Extrapolation|Col2|Col3|nduction He|eads Extrapolation|n|\n|---|---|---|---|---|---|\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<"], "reference": "Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the task perfectly because of its ability to selectively remember the relevant token while ignoring everything else in between.", "persona_name": "Research Scientist in Information Retrieval", "query_style": "MISSPELLED", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What were the speed and memory benchmarks for the SSM scan operation in the context of the Mamba model?", "reference_contexts": ["<1-hop>\n\naca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryasov and Tastet, 2023) IF 10M-word BabyLM dataset GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S\nMiniLLM (Gu et al., 2024) IF Dolly Dataset GPT2 + OPT + LLaMA GPT2 + OPT + LLaMA Feature D&S\nSelf-Align (Sun et al., 2024b) IF Human-written Principles LLaMA LLaMA Expansion + Self-Knowledge SFT\nSelf-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL\nSTaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT\nLlama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT\nReflection-Tuning (Li et al., 2023e) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nSelective Reflection-Tuning (Li et al., 2024d) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nVicuna (Chiang et al., 2023) IF/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT\nKoala (Geng et al., 2023) IF/MD Human Conversation ChatGPT LLaMA Labeling SFT\nBaize (Xu et al., 2023b) IF/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT\nUltraChat (Ding et al., 2023b) IF/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT\nOrca (Mukherjee et al., 2023) IF/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al", "<2-hop>\n\nTest    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC09 Model Ablations** ) Models with 6M parameters. In\nSaShiMi\u2019s U-Net backbone, there are 8 center blocks operating on\nsequence length 1000, sandwiched on each side by 8 outer blocks on\nsequence length 4000, sandwiched by 8 outer blocks on sequence\nlength 16000 (40 blocks total). The architecture of the 8 center\nblocks are ablated independently of the rest. Note that Transformers\n(MHA+MLP) were not tested in the more important outer blocks\nbecause of efficiency constraints.\n\n\nOuter Center NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\n\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\n\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\n\nMamba MHA+MLP **1.850** 1.37 5.63 58.23 0.62\n\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\n\nMamba Mamba 1.852 **0.94** **6.26** **88.54** **0.52**\n\n\n\nWe benchmark the speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of\n(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6."], "reference": "The speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16) was faster than the best attention implementation that we know of (FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation in PyTorch. Mamba achieved 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the KV cache it can use much higher batch sizes.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "What is the role of FreshQA in evaluating time-sensitive information and how does it compare to HotPotQA and Musique?", "reference_contexts": ["<1-hop>\n\n|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|||||||||||||\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens||||||pot|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|\n\n\nFigure 2: We compare the % of instances that our autorater labels as sufficient across datasets,\neither with the first 10k, 6k, or 2k tokens of the provided sources. FreshQA has hand-curated URLs\nthat support the answers and exhibits high sufficient context. HotPotQA and Musique have lower\nsufficient context (and even lower with 2000 tokens). We use 6000 token contexts in the remainder.\n\n\nThis is not the focus of our work, as we wish to understand how models perform with or without\nsufficient context. Having a mix of both is inevitable in generic RAG systems.\n\n\n**Datasets.** We consider **FreshQA**, **Musique-Ans**, and **HotpotQA** as a representative spread of open\nbook QA datasets. FreshQA (Vu et al., 2023) evaluates time-sensitive information and has up-to-date\nURLs that should support an answer to the queries, which we use to construct the context (see\nAppendix A.3 for details on the retrieval). We use the \u2018True Premise\u2019 setting (452 instances), skipping\n\u2018False Premise\u2019 questions that", "<2-hop>\n\n model to offer feedback (Yuan et al., 2024a), a\nstrategy that merges _Self-Knowledge_ with _Feedback_ methods\n\n- f eliciting knowledge. We advocate for increasingly leveraging the student model itself to provide feedback, thereby\nenhancing self-alignment capabilities. This approach not\n\n- nly facilitates moving beyond traditional human/teacher\npreference-based rewards but also opens avenues for continual self-improvement and alignment.\n\n\n**7** **CONCLUSION AND DISCUSSION**\n\n\nThis survey has explored the diverse landscape of knowledge distillation for LLMs, highlighting key techniques,\napplications, and challenges. KD plays a crucial role in\ndemocratizing access to advanced LLM capabilities, providing cutting-edge advancements without the high costs\n\n\n\n28\n\n\n- f training and deployment. Our review emphasizes vari\n- us KD approaches, from algorithmic innovations to skill\nenhancement and vertical distillation. Notably, data augmentation and synthesis within KD emerge as vital tools\nfor improving distillation, revealing the powerful synergy\nbetween enriched training data and effective model distillation. As the AI landscape evolves, rapid advancements\nin model architectures and training methods present both\nchallenges and research opportunities for KD of LLMs.\nFuture innovation will need to focus on achieving efficiency,\ntransparency, and ethics while maintaining model trustworthiness. Furthermore, promising areas such as weakto-strong generalization, self-alignment, and multi-modal\nLLMs offer the potential to enhance the capabilities of\ndistilled models. In conclusion, the KD of LLMs is set to play\na pivotal role in the future of AI research. As highlighted\nin this survey, sustained research efforts will be critical in\ndeveloping accessible, efficient, and responsible AI for all.\nImportantly, when conducting KD of LLMs like ChatGPT\n\n- r Llama, it\u2019s essential to comply with the model providers\u2019\nterms [4], such as the restrictions on developing competitive\nproducts.\n\n\n**REFERENCES**\n\n\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray\n_et al._, \u201cTraining language models to follow instructions\nwith human feedback,\u201d _Advances in Neural Information_\n_Processing Systems_, vol. 35, pp. 27 730\u201327 744,"], "reference": "FreshQA evaluates time-sensitive information and has up-to-date URLs that should support an answer to the queries. It exhibits high sufficient context compared to HotPotQA and Musique, which have lower sufficient context even with 6000 tokens.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "What open-domain QA models use dense retrieval techniques and how do they compare to traditional IR methods?", "reference_contexts": ["<1-hop>\n\naca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryasov and Tastet, 2023) IF 10M-word BabyLM dataset GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S\nMiniLLM (Gu et al., 2024) IF Dolly Dataset GPT2 + OPT + LLaMA GPT2 + OPT + LLaMA Feature D&S\nSelf-Align (Sun et al., 2024b) IF Human-written Principles LLaMA LLaMA Expansion + Self-Knowledge SFT\nSelf-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL\nSTaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT\nLlama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT\nReflection-Tuning (Li et al., 2023e) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nSelective Reflection-Tuning (Li et al., 2024d) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nVicuna (Chiang et al., 2023) IF/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT\nKoala (Geng et al., 2023) IF/MD Human Conversation ChatGPT LLaMA Labeling SFT\nBaize (Xu et al., 2023b) IF/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT\nUltraChat (Ding et al., 2023b) IF/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT\nOrca (Mukherjee et al., 2023) IF/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al", "<2-hop>\n\nPublished as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto finetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved significant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\n\n- f questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model\u2019s hidden states. However, their method requires an initial round\n\n- f TF-IDF/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\n"], "reference": "Open-domain QA models that use dense retrieval techniques typically employ dense passage retrieval methods, which better capture semantic matching beyond simple n-gram overlaps. These methods either conduct large-scale pretraining with self-supervised tasks close to the underlying question-passage matching or directly finetune pretrained masked language models using human-labeled question-passage pairs. On single-hop information-seeking QA datasets like NaturalQuestions or WebQuestions, these dense methods have achieved significant improvements over traditional IR methods. Prior to these model-based methods, Das et al. (2019) used RNN encoders for generating dense representations of questions and passages, involving an iterative retrieval process and reformulating the query representation based on reader model's hidden states. However, their method required an initial round of TF-IDF/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Unlike Feldman & El-Yaniv (2019)'s bi-attentive reformulation component applied on token-level representations, our approach uses a straightforward query reformulation strategy by concatenating the original query and previous retrieval as inputs to the query encoder. Together with stronger pretrained encoders and more effective training methods, MDR doubles the accuracy of their system.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "What is the relationship between Selective State Space Models and linear attention approximations like SSMs?", "reference_contexts": ["<1-hop>\n\naca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryasov and Tastet, 2023) IF 10M-word BabyLM dataset GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S\nMiniLLM (Gu et al., 2024) IF Dolly Dataset GPT2 + OPT + LLaMA GPT2 + OPT + LLaMA Feature D&S\nSelf-Align (Sun et al., 2024b) IF Human-written Principles LLaMA LLaMA Expansion + Self-Knowledge SFT\nSelf-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL\nSTaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT\nLlama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT\nReflection-Tuning (Li et al., 2023e) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nSelective Reflection-Tuning (Li et al., 2024d) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nVicuna (Chiang et al., 2023) IF/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT\nKoala (Geng et al., 2023) IF/MD Human Conversation ChatGPT LLaMA Labeling SFT\nBaize (Xu et al., 2023b) IF/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT\nUltraChat (Ding et al., 2023b) IF/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT\nOrca (Mukherjee et al., 2023) IF/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al", "<2-hop>\n\nKatharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be\nviewed as a degenerate linear SSM.\n\n\n- H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM\nsandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a\nshift-SSM, before the main SSM layer.\n\n\n- Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global\nconvolution (Romero et al. 2021).\n\n\n- RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative\nparallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.\n\n\n- RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \u201cWKV\u201d mechanism involves LTI recurrences and\ncan be viewed as the ratio of two SSMs.\n\n\nOther closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight\nin particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which\nwe view as the most closely related methods to our core selective SSM.\n\n\n4\n\n\n### **3 Selective State Space Models**\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate\nthis mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting\na technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits\nthe memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or\neven MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3."], "reference": "Selective State Space Models (SSMs) are related to linear attention approximations such as SSMs through their ability to handle time-varying data efficiently. They can be viewed as an architecture with an SSM sandwiched by two gated connections, which allows for parallelizable computation paths and the use of simpler mechanisms like multi-head attention instead of convolutions.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "What were the speed and memory benchmarks for the Mamba model in comparison to other models?", "reference_contexts": ["<1-hop>\n\naca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryasov and Tastet, 2023) IF 10M-word BabyLM dataset GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S\nMiniLLM (Gu et al., 2024) IF Dolly Dataset GPT2 + OPT + LLaMA GPT2 + OPT + LLaMA Feature D&S\nSelf-Align (Sun et al., 2024b) IF Human-written Principles LLaMA LLaMA Expansion + Self-Knowledge SFT\nSelf-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL\nSTaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT\nLlama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT\nReflection-Tuning (Li et al., 2023e) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nSelective Reflection-Tuning (Li et al., 2024d) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nVicuna (Chiang et al., 2023) IF/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT\nKoala (Geng et al., 2023) IF/MD Human Conversation ChatGPT LLaMA Labeling SFT\nBaize (Xu et al., 2023b) IF/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT\nUltraChat (Ding et al., 2023b) IF/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT\nOrca (Mukherjee et al., 2023) IF/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al", "<2-hop>\n\nTest    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC09 Model Ablations** ) Models with 6M parameters. In\nSaShiMi\u2019s U-Net backbone, there are 8 center blocks operating on\nsequence length 1000, sandwiched on each side by 8 outer blocks on\nsequence length 4000, sandwiched by 8 outer blocks on sequence\nlength 16000 (40 blocks total). The architecture of the 8 center\nblocks are ablated independently of the rest. Note that Transformers\n(MHA+MLP) were not tested in the more important outer blocks\nbecause of efficiency constraints.\n\n\nOuter Center NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\n\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\n\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\n\nMamba MHA+MLP **1.850** 1.37 5.63 58.23 0.62\n\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\n\nMamba Mamba 1.852 **0.94** **6.26** **88.54** **0.52**\n\n\n\nWe benchmark the speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of\n(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6."], "reference": "The Mamba model achieved 4-5\u00d7 higher inference throughput than a Transformer of similar size, as it can use much higher batch sizes without the KV cache. It was faster than the best attention implementation that we know of (FlashAttention-2) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation in PyTorch.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "What is RAFT and how does it improve the performance of pre-trained Large Language Models on domain-specific RAG tasks?", "reference_contexts": ["<1-hop>\n\naca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryasov and Tastet, 2023) IF 10M-word BabyLM dataset GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S\nMiniLLM (Gu et al., 2024) IF Dolly Dataset GPT2 + OPT + LLaMA GPT2 + OPT + LLaMA Feature D&S\nSelf-Align (Sun et al., 2024b) IF Human-written Principles LLaMA LLaMA Expansion + Self-Knowledge SFT\nSelf-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL\nSTaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT\nLlama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT\nReflection-Tuning (Li et al., 2023e) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nSelective Reflection-Tuning (Li et al., 2024d) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nVicuna (Chiang et al., 2023) IF/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT\nKoala (Geng et al., 2023) IF/MD Human Conversation ChatGPT LLaMA Labeling SFT\nBaize (Xu et al., 2023b) IF/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT\nUltraChat (Ding et al., 2023b) IF/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT\nOrca (Mukherjee et al., 2023) IF/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al", "<2-hop>\n\nPreprint, Under Review\n\n## **RAFT: Adapting Language Model to Domain Specific RAG**\n\n\n\n**Tianjun Zhang** _[\u2217]_\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n{tianjunz}@berkeley.edu\n\n```\n\n\n**Shishir G. Patil, Naman Jain, Sheng Shen**\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\n\n```\n\n\n**Matei Zaharia, Ion Stoica, Joseph E. Gonzalez**\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n {matei,istoica,jegonzal}@berkeley.edu\n\n```\n\n**Abstract**\n\n\nPretraining Large Language Models (LLMs) on large corpora of textual\ndata is now a standard paradigm. When using these LLMs for many\ndownstream applications, it is common to additionally incorporate new information into the pretrained model either through RAG-based-prompting,\n\n    - r finetuning. However, the best methodology to incorporate information\nremains an open question. In this paper, we present Retrieval Augmented\nFine Tuning (RAFT), a training recipe which improves the model\u2019s ability\nto answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don\u2019t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT\u2019s chain-of-thought-style response helps improve\nthe model\u2019s ability to reason. In domain specific RAG, RAFT consistently\nimproves the model\u2019s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n\n\n**1** **Introduction**\n\n\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized\ndomains"], "reference": "RAFT (Retrieval Augmented Fine Tuning) is a training recipe that improves the model's ability to answer questions in 'open-book' in-domain settings. In training RAFT, given a question and a set of retrieved documents, the model is trained to ignore distractor documents and cite verbatim the right sequence from relevant documents to help answer the question. This method consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets when used in domain-specific RAG tasks.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How does the Flare framework utilize recursive retrieval to enhance information retrieval?", "reference_contexts": ["<1-hop>\n\n run experiments.\n\n\n**Long-form QA** For \u201cWhere do the Philadelphia\nEagles play their home games?\u201d, the output we\naim to generate is \u201cWe need to consider the different possible locations or venues that could be\nconsidered the home field of the Philadelphia Eagles. These include the city, the sports complex,\n\n- r the stadium. Therefore, this question has 3 interpretations and the answers are: (1) The city is\nPhiladelphia. (2) The sports complex is the South\nPhiladelphia Sports Complex. (3) The stadium is\nthe Lincoln Financial Field stadium.\u201d For both the\n\n- riginal setting (ASQA) and the setting with hints\n(ASQA-hint), we manually annotate 8 exemplars\n(Prompt D.6 and D.8), use BM25 on the Wikipedia\ncorpus, and 3 retrieved documents to run experi\nments.\n\n\n**Open-domain Summarization** The original\nWikiAsp dataset is designed for multi-document\nsummarization and provides a list of references to\nsystems. We converted it into the open-domain\nsetting by removing the associated references and\ninstead gathering information from the open web.\nFor \u201cGenerate a summary about Echo School (Ore\n\ngon) including the following aspects: academics,\nhistory.\u201d, the output we aim to generate is \u201c# Academics. In 2008, 91% of the school\u2019s seniors received their high school diploma... # History. The\nclass of 2008 was the 100th class in the school\u2019s\n\nhistory.\u201d where # is used to indicate aspects. We\nmanually annotate 4 exemplars (Prompt D.10), and\nuse the Bing search engine to retrieve 5 documents\nfrom the open web. To avoid leaking, we exclude\nseveral Wikipedia-related domains listed in Table 8\nfrom Bing\u2019s search results.\n\n\n**C** **Hyperparameters**\n\n\nHyperparameters of FLARE on different datasets\nare listed in Table 9.\n\n\n**D** **Prompts and Few-shot exemplars**\n\n\nThe prompt used to linearize multiple documents\nis shown in Prompt D.1. The prompt used in selfask (Press et al., 2022) is shown in Prompt D.2.\nPrompts and exemplars of different tasks/datasets\nare shown in Prompt D.3, D.4, D.5, D.6, D.8, and\nD.10", "<2-hop>\n\n for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n\n\n_B. Recursive Retrieval_\n\n\nRecursive retrieval is often used in information retrieval and\n\nNLP to improve the depth and relevance of search results.\nThe process involves iteratively refining search queries based\n\n- n the results obtained from previous searches. Recursive\nRetrieval aims to enhance the search experience by gradually converging on the most pertinent information through a\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\nthe retrieval process and refines the CoT with the obtained\nretrieval results. ToC [57] creates a clarification tree that\nsystematically optimizes the ambiguous parts in the Query. It\ncan be particularly useful in complex search scenarios where\nthe user\u2019s needs are not entirely clear from the outset or where\nthe information sought is highly specialized or nuanced. The\nrecursive nature of the process allows for continuous learning\nand adaptation to the user\u2019s requirements, often resulting in\nimproved satisfaction with the search outcomes.\nTo address specific data scenarios, recursive retrieval and\nmulti-hop retrieval techniques are utilized together. Recursive\n\n\n\nretrieval involves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\n\n\n_C. Adaptive Retrieval_\n\n\nAdaptive retrieval methods, exemplified by Flare [24] and\nSelf-RAG [25], refine the RAG framework by enabling LLMs\nto actively determine the optimal moments and content for\nretrieval, thus enhancing the efficiency and relevance of the\ninformation sourced.\n\nThese methods are part of a broader trend wherein\nLLMs employ active judgment in their operations, as seen\nin model agents like AutoGPT, Toolformer, and GraphToolformer [107]\u2013[109]. Graph-Toolformer, for instance, divides its retrieval process into distinct steps where LLMs\nproactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive\nstance allows LLMs to decide when to search"], "reference": "The Flare framework utilizes recursive retrieval by iteratively refining search queries based on previous results, aiming to enhance the depth and relevance of search outcomes through a feedback loop. This process involves summarizing sections of a document or lengthy PDF before performing a retrieval based on this summary, followed by a secondary refinement within the document.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What is an example of a multi-hop reasoning task in MMLU high-school Physics?", "reference_contexts": ["<1-hop>\n\n by the friction\nbetween the object and the air molecules. The greater the surface area of the\n\n           - bject, the greater the air resistance. The tennis ball has a large surface area, so\nit experiences a lot of air resistance. This air resistance slows down the tennis\nball and causes the difference between the predicted velocity and the velocity\nmeasured by the photogate.\nTherefore, the correct answer is 0) The ball changes its shape while falling.\n\n\nTable 27: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Reasoning Error** . In this case, the model was able to correctly identify air resistance is the cause of\nthe velocity difference, but made the error at the final reasoning step of deriving the right answer.\n\n\n31\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nMMLU-Physics **Original Question** : What happens to the pressure, P, of an ideal gas if\nthe temperature is increased by a factor of 2 and the volume is increased\nby a factor of 8 ?\n0) P decreases by a factor of 16.\n1) P decreases by a factor of 4.\n2) P decreases by a factor of 2.\n3) P increases by a factor of 4.\n**Original Answer [Incorrect]** : The correct answer is 0. The ideal gas\nlaw is PV = nRT. If the temperature is increased by a factor of 2 and the\nvolume is increased by a factor of 8, then the pressure will decrease by a\nfactor of 16.\n**Stepback Question** : What are the physics principles behind this\nquestion?\n**Principles** :\nIdeal gas law: PV = nRT, where P is the pressure, V is the volume, n is\nthe number of moles, R is the gas constant, and T is the temperature.\n**Final Answer** : Using the Principles of Ideal gas law, we can solve the\nproblem as following:\nStep 1: Apply Ideal gas law to find out the pressure of an ideal gas. The\nideal gas law states that PV = nRT, where P is the pressure, V is the\nvolume, n is the number of moles, R is the gas constant, and T is the\ntemperature.\n\n\nStep 2: Apply Ideal gas law to find out the pressure of", "<2-hop>\n\n as well as emergent abilities (Wei et al., 2022a) such as multi-step reasoning (Wei et al.,\n2022b; Zhou et al., 2022) and instruction following (Mishra et al., 2022b; Wei et al., 2021).\n\n\nFigure 1: Strong Performance of STEP-BACK PROMPTING: our proposed Abstraction-and-Reasoning\nscheme leads to a substantial improvement in a wide range of challenging tasks in STEM, Knowledge\nQA and Multi-Hop Reasoning requiring complex (often multi-hop) reasoning.\n\n\n_\u2217_ Equal Contribution\n\n\n1\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nDespite the great advancements, complex multi-step reasoning remains challenging for even the state\n- f-the-art LLMs. Lightman et al. (2023) show that process-supervision with step-by-step verification\nis a promising remedy to improve the correctness of intermediate reasoning steps. Techniques such\nas Chain-of-Thought (Wei et al., 2022b) were introduced to produce a coherent series of intermediate\nreasoning steps to increase the success rate of following the right decoding path. Inspired by the\nfact that when faced with challenging tasks humans often step back and do abstractions to arrive at\nhigh-level principles to guide the process, we propose STEP-BACK PROMPTING to ground reasoning\n\n- n abstractions to reduce the chance of making errors in the intermediate reasoning steps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of STEP-BACK PROMPTING with two steps of Abstraction and Reasoning\nguided by concepts and principles. _Top_ : an example of MMLU high-school physics (Hendrycks et al.,\n2020) where the first principle of Ideal Gas Law is retrieved via abstraction. _Bottom_ : an example\nfrom TimeQA (Chen et al., 2021) where the high-level concept of education history is a result of the\nabstraction. _Left_ : PaLM-2L (Anil et al., 2023) fails to answer the original question. Chain-of-Thought\nprompting (Wei et al., 2022b; Kojima et al., 2022) ran into errors during intermediate reasoning\nsteps (highlighted as red). _Right_ : PaLM-2L (Anil et al., 2023) successfully answers the question via"], "reference": "An example of a multi-hop reasoning task in MMLU high-school Physics is the original question about the pressure of an ideal gas when temperature and volume are changed. The correct answer requires applying the Ideal gas law twice, first to find the initial pressure and then to find the final pressure after the changes.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What are retrieval units in the context of open-domain QA?", "reference_contexts": ["<1-hop>\n\n are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\n\n- pen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve comparable or better results on multiple QA datasets in the\n\n- pen-retrieval setting, compared to several, much\ncomplicated systems.\n\n\n**2** **Background**\n\n\nThe problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \u201c _Who first voiced Meg on Family_\n_Guy?_ \u201d or \u201c _Where was the 8th Dalai Lama born?_ \u201d, a\nsystem is required to answer it using a large corpus\n\n- f diversified topics. More specifically, we assume\n\n\n\nthe extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more passages in the corpus. Assume that our collection\ncontains _D_ documents, _d_ 1 _, d_ 2 _, \u00b7 \u00b7 \u00b7, dD_ . We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units [3] and get _M_\ntotal passages in our corpus _C_ = _{p_ 1 _, p_ 2 _, . . ., pM_ _}_,\nwhere each passage _pi_ can be viewed as a sequence\n\n- f tokens _w_ [(] _[i]_ [)]\n1 _[, w]_ 2 [(] _[i]_ [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _|_ [(] _p_ _[i]_ [)] _i|_ [. Given a question] _[ q]_ [,]\n\nthe task is to find a span _ws_ [(] _[i]_ [)] _[, w]_ _s_ [(] _[i]_ +1 [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _e_ [(] _[i]_ [)] from\n\n- ne of the passages _pi_ that can"], "reference": "Retrieval units in the context of open-domain QA refer to text passages of equal lengths used as basic units for information retrieval. These passages are split from documents and form a corpus, which is then used to answer factoid questions by finding relevant spans within these passages.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "How do different contextual inputs affect the embeddings generated for documents in the TREC-Covid dataset?", "reference_contexts": ["<1-hop>\n\n. Section 10.5 displays a few qualitative examples. We generate embeddings for randomly\nsampled documents from the TREC-Covid dataset and visualize their embeddings with PCA, where\nunique document inputs with different contextual embeddings are visualized in the same color. By\nchanging only the conditioning we reshape the embedding space and our model produces different\nembedding for the same text. Note that although the embeddings are clearly moving in response to\nchanging the contextual inputs, they still remain closer to each other than to different documents.\n\n\n19\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|||||Rand<br>TSP|om||\n||||||||\n||||||||\n||||||||\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Pre-training with TSP vs. random batching across cluster sizes.\n\n\n\nFigure 15: Each color indicates a single document input _d_ . Different points represent different values _\u03d5_ ( _d_ ; _D_ ) for different contexts.\n\n\n\nFigure 16: Performance of CDE model as the\nnumber of contextual examples increases.\n\n\n\nWe also consider how additional context is improving our model. Because the model includes an\n\n- ptional null token, we can supply any number of contextual inputs. We plot our model\u2019s performance\nacross context sizes in Figure 10.5. We see that our model is able to utilize partial context window\nsizes, and even perform reasonably with no context (i.e. all null token inputs) but offers the best\nperformance given a full context window size.\n\n\n10.6 CLUSTER TEXT EXAMPLES\n\n\nWe include random examples from a cluster gathered from our supervised dataset, shown in Table 4.\nThis particular cluster appears to be a combination of documents about county populations in the\nUntied States (in Kentucky, Iowa, Pennsylvania, etc.) and documents about criminal trials (mentioning\nhearings, depositions, and courts).\n\n\n10.7 TASK PREFIXES\n\n\nPrefixes are hand-written for each dataset in both meta-training sets. We follow the same prefix\nselection procedure as Nussbaum et al. (2024), inspired by Reimers et al. (2023):\n\n\n   - search ~~q~~ uery\n\n   - search ~~d~~   - cument\n\n   - classification\n\n\n20\n\n\nquery document\n\n\npopulation of breckenridge mi breckenridge, michigan. breckenridge is"], "reference": "Different contextual inputs reshape the embedding space, causing the model to produce different embeddings for the same text. Although the embeddings move in response to changing contextual inputs, they remain closer to each other than to different documents.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What method did Wu et al. propose to enhance the diversity and coverage of instruction-following knowledge extracted from teacher LLMs?", "reference_contexts": ["<1-hop>\n\n\n\n\n\nInput Set\n\n\n\n\n\n\n\n\n\n\n|et|Col2|Col3|\n|---|---|---|\n|t|||\n|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|xtract|||\n|xtract|||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 5: An illustration of different knowledge elicitation methods from teacher LLMs. _Labeling_ : The teacher generates\nthe output from the input; _Expansion_ : The teacher generates samples similar to the given demonstrations through incontext learning; _Data Curation_ : The teacher synthesizes data according to meta-information, such as a topic or an entity;\n_Feature_ : Feed the data into the teacher and extract its internal knowledge, such as logits and features; _Feedback_ : The teacher\nprovides feedback on the student\u2019s generations, such as preferences, corrections, expansions of challenging samples, etc;\n_Self-Knowledge_ : The student first generates outputs, which is then filtered for high quality or evaluated by the student itself.\n\n\n\nIn this formulation, _x_ and _y_ represent the new input\n- utput pairs generated by the teacher LLM. The input _x_\nis generated based on a set of input-output demonstrations\n_c_ . The output _y_ is then generated in response to the new\ninput _x_ under the guidance of an instruction _I_ . Note that\nthe demonstrations could be predefined or dynamically\nupdated by adding the newly generated samples.\nExpansion techniques have been widely utilized to\nextract extensive instruction-following knowledge from\nteacher LLMs. Wang et al. (2022a) first introduces an iterative bootstrapping method, Self-Instruct, to utilize LLMs\nto generate a wide array of instructions based on several demonstrations sampled from 175 manually-written instructions. The newly generated instructions are then added\nback to the initial pool, benefiting subsequent expansion\niterations. Subsequently, Taori et al. (2023) applies this expansion method to a more powerful teacher LLM, textdavinci-003, to distill 52K high-quality data. To improve\nthe diversity and coverage during expansion, Wu et al.\n(2023c) and (Sun et"], "reference": "Wu et al. proposed a method to enhance the diversity and coverage of instruction-following knowledge extracted from teacher LLMs by dynamically updating the initial pool of demonstrations with newly generated samples during expansion iterations.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What specific research approach did Xu et al. propose to improve the performance of LLMs in handling factual errors?", "reference_contexts": ["<1-hop>\n\n<br>89<br>9<br>**8**<br>**7**<br>**57.14**|\n\n\nTable 7: The result of counterfactual robustness. ACC is the\naccuracy (%) of LLMs without external documents. ACCdoc\nis the accuracy (%) of LLMs with counterfactual documents.\nED and ED _[\u2217]_ are error detection rates evaluated by exact\nmatching and ChatGPT, respectively. CR is the error correction rate.\n\n\nthought approach to break down complex problems (Zhou\net al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). However, these methods slow down the inference speed and cannot provide timely responses.\n\n\n**Results on Counterfactual Robustness testbed**\n\nIn order to ensure that LLMs possess relevant knowledge,\nwe assess their performance by directly asking them questions. However, we found that most LLMs struggle to answer them correctly. To ensure a more reasonable evaluation, we only consider LLMs that have an accuracy rate of\n\n- ver 70% as this threshold is relatively high and encompasses more LLMs. The results are shown in Table 7. We\npresent the following metrics: accuracy without any documents, accuracy with counterfactual documents, error detection rates, and error correction rates. We can see that It\nis hard for LLMs to identify and correct factual errors in the\ndocuments. This suggests that the model can be easily misled by documents containing incorrect facts.\nIt is important to note that retrieval-augmented generation\nis not designed to automatically address factual errors within\na given context, as this contradicts the underlying assumption that the model lacks knowledge and relies on retrieved\ndocuments for additional information. However, this issue is\ncrucial in practical applications due to the abundance of fake\nnews on the internet. Existing LLMs do not have a safeguard\nto handle inaccurate responses caused by misinformation. In\nfact, they heavily depend on the information they retrieve.\nEven when LLMs contain the internal knowledge about the\nquestions, they often trust false information that is retrieved.\nThis presents significant a challenge for the future development of RAG in LLMs.\n\n\n**Conclusion**\n\nIn this paper, we evaluated four abilities of retrievalaugmented generation in LLMs: noise robustness, negative rejection, information integration, and counterfactual\nrobustness"], "reference": "Xu et al. proposed a thought approach to break down complex problems, which involves using methods like Zhou et al. (2023a), Xu et al. (2023b), and Drozdov et al. (2023). These methods aim to enhance the performance of LLMs by providing relevant knowledge and addressing issues such as slow inference speed and delayed responses.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What datasets did the researchers use to evaluate the performance of their models for Musique?", "reference_contexts": ["<1-hop>\n\n~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>", "<2-hop>\n\n we use an LLM with ground truth answers to classify\nresponses as Correct ( **%C** ), Abstention ( **%A** ), or Hallucination ( **%H** ).\n\n\n**Fine-tuning Results and Discussion.** Table 3 shows our experimental results. We verify that the FT\nvariants have a higher rate of generating correct answers ( **%C** ) compared to closed-book and Vanilla\nRAG for Musique but not for HotPotQA. On the other hand, refuting our hypothesis, Data Mix 2 and\n3 do not lead to more abstentions than Vanilla RAG. But, they do abstain more than with Data Mix 1,\nshowing the impact of adding \u201cI don\u2019t know\u201d in the training set. In general, FT models using RAG\n\n- utput incorrect answers ( **%H** ) much of the time, and often more than they abstain ( **%A** ).\n\n\nB.2 PERFORMANCE BREAKDOWN BY SUFFICIENT CONTEXT\n\n\nWe explore RAG performance by different models for various RAG benchmark datasets. Here,\nthe first column shows performance without RAG (closed-book) while the second column shows\nperformance with RAG (open-book). To better understand RAG performance, we use our sufficient\ncontext autorater to stratify the retrieval augmented generation (RAG) datasets into sufficient and\ninsufficient context. The third and fourth columns show the performance of the second column\nstratified by sufficient vs insufficient context respectively.\n\n\n17\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Correct, hallucination, and abstention fractions across models for dataset FreshQA, stratified\nby sufficient context. FreshQA includes hand-curated source URLs, which explains the larger\npercentage of sufficient context (77.4%). FreshQA also specifically explores questions with answers\nthat change based on the question\u2019s timestamp, which may explain the frequent abstentions without\nRAG (100% for Gemini 1.5 Pro).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Correct, hallucination, and abstention fractions across models for dataset HotpotQA,\nstratified by sufficient context. HotpotQA includes questions that are more likely to be answerable\nwithout context (e.g., yes or no questions, multiple choice questions, or questions with answers that\nmight be answerable due to pre-training). This explains the higher fraction of correct answers without"], "reference": "The researchers used FreshQA and HotpotQA datasets to evaluate the performance of their models for Musique.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What is the qrel matrix used in Yang et al.'s work for instantiating attributes?", "reference_contexts": ["<1-hop>\n\n for simplicity in instantiating and to mirror previous\nwork (i.e. NQ, HotpotQA, etc. [Kwiatkowski et al., 2019, Yang et al., 2018]).\n\n\nOur last step is to choose a qrel matrix to instantiate these attributes. Although we could not prove\nthe hardest qrel matrix definitively with theory (as the sign rank is notoriously hard to prove), we\nspeculate based on intuition that our theoretical results imply that the more interconnected the qrel\nmatrix is (e.g. dense with all combinations) the harder it would be for models to represent. [8] Following\nthis, we use the qrel matrix with the highest number of documents for which all combinations would\nbe just above 1000 queries for a top- _\ud835\udc58_ - f 2 (46 docs, since [\ufffd][46] 2 - is 1035, the smallest above 1k).\n\n\nWe then assign random natural language attributes to the queries, adding these attributes to their\nrespective relevant documents (c.f. Figure 1). We give each document a random first and last name\nfrom open-source lists of names. Finally, we randomly sample new attributes for each document until\nall documents have the same number of attributes. As this setup has many more documents than\nthose that are relevant to any query (46 relevant documents, 49.95k non-relevant to any query) we\nalso create a \u201csmall\u201d version with only the 46 documents that are relevant to one of the 1000 queries.\n\n\n8See Appendix 10 for specific metrics that show the difference between LIMIT and other IR datasets.\n\n\n8\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n\n\n\n\n\n\n\n\n|Reca|ll@2|\n|---|---|\n|||\n|||\n|||\n\n\n|Recal|l@10|\n|---|---|\n|||\n|||\n|||\n\n\n|Recal|l@100|\n|---|---|\n|||\n|||\n\n\n\n\n\n\n\n\n\n\n\nFigure 3 | Scores on the LIMIT task. Despite the simplicity of the task we see that SOTA models\nstruggle. We also see that the dimensionality of the model is a limiting factor and that as the\ndimension increases, so does performance. Even multi-vector models struggle. Lexical models like\nBM25 do very well due to their higher dimensionality. Stars indicate models trained with MRL.\n\n\n"], "reference": "The qrel matrix used in Yang et al.'s work for instantiating attributes has the highest number of documents for which all combinations would be just above 1000 queries for a top- _\ud835\udc58_ - f 2 (46 docs, since [\ufffd][46] 2 - is 1035, the smallest above 1k).", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "How does MiniCPM-V 2.6 compare to VisRAG-Ret in terms of retrieval efficiency and performance on text retrieval benchmarks?", "reference_contexts": ["<1-hop>\n\n pages and extracting relevant information from noisy\nretrieved data. Results for Qwen2-VL demonstrate stronger document understanding capabilities,\n\n- utperforming MiniCPM-V 2.6 in these tasks.\n\n\nH RETRIEVAL EFFICIENCY\n\n\nIn this experiment, we evaluate the retrieval efficiency of VisRAG-Ret and MiniCPM (OCR) by\nmeasuring two key components: offline document parsing and encoding latency, and online query\nencoding and search latency. Query and document encoding are conducted on an NVIDIA A100\n40G GPU with a batch size of 1, while document parsing is performed on a single core of an Intel\nXeon Platinum 8350C CPU. The reported latencies are averaged over the queries and documents\nfrom the PlotQA dataset. The results are summarized in Table 12.\n\n\nAs shown in the table, although VisRAG-Ret, a VLM-based model, requires more time for document\nencoding compared to MiniCPM (OCR), it bypasses the time-consuming parsing stage required by\n\n\n24\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 12: Retrieval efficiency (ms). We report offline latencies per document, including document\nparsing and encoding latencies, as well as online latencies per query, including query encoding and\nsearch latencies.\n\n|Col1|OfflineLatencyperDocument<br>Parsing Encoding Total|OnlineLatencyperQuery<br>Encoding Search Total|\n|---|---|---|\n|MiniCPM (OCR)<br>VisRAG-Ret|284<br>28<br>312<br>\u2013<br>121<br>121|28<br>26<br>54<br>28<br>26<br>54|\n\n\n\nMiniCPM (OCR). This leads to a 58% reduction in total document processing time for VisRAG-Ret.\nFor online query processing, the latencies of VisRAG-Ret and MiniCPM (OCR) are nearly identical,\nas the queries consist solely of textual inputs.\n\n\nI RETRIEVAL PERFORMANCE ON TEXT RETRIEVAL BENCHMARKS\n\n\nTable 13: Retrieval performance on subsets of the text retrieval benchmark BEIR (Thakur et al.,\n2021) in NDCG@10. VisRAG-Ret performs retrieval on rendered document screenshots.\n\n\n**Model** **SciFact** **NFCorpus** **Scidocs**\n\n\nMiniCP"], "reference": "MiniCPM-V 2.6 outperforms VisRAG-Ret in retrieval efficiency, demonstrating stronger document understanding capabilities. VisRAG-Ret requires more time for document encoding compared to MiniCPM (OCR), but it bypasses the time-consuming parsing stage required by MiniCPM (OCR). This leads to a 58% reduction in total document processing time for VisRAG-Ret. On text retrieval benchmarks, VisRAG-Ret performs well on subsets of the text retrieval benchmark BEIR.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "How does the use of DeBERTa-v3-Large in evaluating RAG systems contribute to their accuracy?", "reference_contexts": ["<1-hop>\n\n.05 to 0.30, the ability to\ncheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of\nannotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we\ngenerate more GPT-4 generated labels. In the table, we define PPI range as the number of percentage points from\nthe lower number to the upper number of the PPI confidence bounding. Additionally, we use the fine-tuned LLM\njudge (DeBERTa-v3-Large) for evaluation.\n\n\n**ARES Ranking of Real RAG Systems**\n\n\nNQ WoW FEVER\n\n\nC.R. A.R. C.R. A.R. C.R. A.R.\n\n\nKendall\u2019s Tau for\n0.73 0.78 0.73 0.73 0.73 0.82\nSampled Annotations\n\nKendall\u2019s Tau\n0.82 0.82 0.73 0.82 0.73 0.87\nfor RAGAS\n\n\nKendall\u2019s Tau\n0.82 0.87 0.82 0.82 0.64 0.87\nfor GPT-3.5 Judge\n\nKendall\u2019s Tau\n0.91 **0.96** **0.91** **1.0** 0.73 0.87\nfor ARES LLM Judge\n\nKendall\u2019s Tau\n**1.0** **0.96** **0.91** **1.0** **0.82** **1.0**\nfor ARES\n\n\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\n\n\nTable 5: **ARES Ranking on Real-World RAG Systems** : For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against", "<2-hop>\n\n we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\n\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70.0%, 72.5%, 75.0%, ..., 90.0%).\nEach split also represents a different mock RAG\nsystem. Since we know the success percentages of\neach dataset split, we know the appropriate ranking of each mock RAG system. This allows us to\n\n\ntest ARES success at both scoring and ranking the\nmock RAG systems appropriately across the three\nevaluation criteria.\n\n\n**4.3** **Metrics**\n\n\nTo calculate the correlation between the correct\n\nranking and the ARES ranking, we use the Kendall\nrank correlation coefficient or Kendall\u2019s _\u03c4_ :\n\n\n_\u03c4_ = [(#][ of concordant pairs][)] _[ \u2212]_ [(#][ of discordant pairs][)]\n\n# of pairs total\n\n\nConcordant pairs are defined as two ordinal values in the ranking where the earlier value in the\nsequence is lower than the later value in the sequence. Discordant pairs are defined as two ordinal\nvalues in the ranking where the earlier value in the\nsequence is greater than or equal to the later value\nin the sequence. A Kendall\u2019s _\u03c4_ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document preprocessing. We want to make sure that ARES has satisfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall\u2019s _\u03c4_ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pairwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in\ninformation retrieval, allowing developers to evaluate ranking systems empirically. Therefore, we\nbelieve Kendall\u2019s tau and prediction accuracy provide meaningful metrics for testing the efficacy of\nARES as a RAG evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges"], "reference": "The use of DeBERTa-v3-Large as the pretrained basis for the judges in evaluating RAG systems contributes to their accuracy by providing a fine-tuned language model that enhances the system's ability to score context relevance and answer relevance. This leads to higher Kendall\u2019s Tau values, indicating successful pairwise comparisons across various performance gaps between RAG systems.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
