# Week 4: Advanced RAG - Topic Coverage Check

**Date**: Day 22 (Week 4, Day 1)
**Purpose**: Comprehensive topic inventory before beginning Advanced RAG study
**Context**: Gap analysis showed RAG at 30% (Q177: FiD 0%, Q178: Internet-augmented 75%, Q179: Hybrid retrieval 30%). Target: 75% interview readiness by end of Week 4.

---

## Instructions

For each subtopic, mark your current knowledge level:
- âœ… **Know**: Can explain confidently in an interview (2-3 min explanation)
- ğŸŸ¡ **Unsure**: Heard of it, vague understanding, need review
- âŒ **Dunno**: No idea, never learned, or completely forgot

**Scoring**: After completing the assessment, calculate percentages for each category and overall.

---

## Category 1: Retrieval Techniques (11 topics)

**Cross-reference with gap analysis**: Q179 (Hybrid retrieval - 30%)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Dense retrieval basics | â˜ | x | â˜ | Vector embeddings, semantic search, cosine similarity |
| Sparse retrieval (BM25, TF-IDF) | â˜ | x | â˜ | Keyword-based methods, term frequency scoring |
| SPLADE (Learned Sparse) â­ | â˜ | â˜ | x | **2025 trend** - Solves vocabulary mismatch, keeps exact-match benefits |
| **Hybrid retrieval (sparse + dense)** â­ | â˜ | x | â˜ | **Gap Q179** - Combining keyword + semantic, fusion strategies |
| Reciprocal Rank Fusion (RRF) â­ | â˜ | â˜ | x | Score fusion algorithm: 1/(k + rank) - Must know formula! |
| Late interaction models (ColBERT) | â˜ | â˜ | x | Token-level similarity, MaxSim operation |
| DPR (Dense Passage Retrieval) â­ | â˜ | â˜ | x | Dual-encoder architecture, contrastive learning |
| Contrastive learning for retrieval | â˜ | x | â˜ | In-batch negatives, hard negative mining |
| Multi-vector retrieval | â˜ | â˜ | x | Multiple embeddings per document |
| Query expansion techniques | â˜ | â˜ | x | Query rewriting, pseudo-relevance feedback |
| Approximate Nearest Neighbor (ANN) | â˜ | â˜ | x | HNSW, IVF, Product Quantization for scale |

**Category 1 Score**: Know: _/11 | Unsure: _/11 | Dunno: _/11

---

## Category 2: Chunking Strategies & Document Processing (10 topics)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Fixed-size chunking â­ | x | â˜ | â˜ | Simple 256/512/1024 token chunks |
| Semantic chunking â­ | â˜ | x | â˜ | Split by paragraph, sentence boundaries |
| Propositional / Context-aware chunking | â˜ | x | â˜ | LLM-based chunking with document context |
| Recursive chunking | â˜ | x | â˜ | Hierarchical splitting with overlap |
| Chunk overlap strategies â­ | â˜ | x | â˜ | 10-20% overlap, sliding windows |
| Document structure preservation | â˜ | x | â˜ | Maintain headers, sections, metadata |
| Chunk size vs retrieval quality | â˜ | x | â˜ | Trade-offs: precision vs context |
| Multi-resolution chunking | â˜ | â˜ | x | Store chunks at multiple granularities |
| Token vs character-based chunking | â˜ | x | â˜ | Alignment with model tokenizer |
| **Complex PDF parsing / OCR** â­ | â˜ | â˜ | x | Real-world: tables, columns, unstructured.io, LlamaParse |

**Category 2 Score**: Know: _/10 | Unsure: _/10 | Dunno: _/10

---

## Category 3: Embedding Models & Optimization (9 topics)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Sentence-BERT / Sentence Transformers â­ | â˜ | â˜ | x | Siamese networks for sentence embeddings |
| OpenAI embeddings (ada-002, text-embedding-3) | â˜ | â˜ | x | Commercial embedding APIs |
| Domain-specific embedding fine-tuning | â˜ | â˜ | x | Contrastive fine-tuning on domain data |
| Embedding dimension trade-offs | â˜ | x | â˜ | 384 vs 768 vs 1536 dimensions |
| Matryoshka embeddings | â˜ | â˜ | x | Variable-dimension embeddings |
| Multilingual embeddings | â˜ | x | â˜ | Cross-lingual retrieval (mBERT, XLM-R) |
| Embedding compression | â˜ | â˜ | x | Quantization for storage/speed |
| Late interaction vs bi-encoders | â˜ | â˜ | x | ColBERT vs SBERT trade-offs |
| Contextualized embeddings | â˜ | â˜ | x | Query-aware vs static embeddings |

**Category 3 Score**: Know: _/9 | Unsure: _/9 | Dunno: _/9

---

## Category 4: Vector Databases & Storage (7 topics)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Vector database landscape | â˜ | â˜ | x | Pinecone, Weaviate, Qdrant, Milvus, Chroma |
| FAISS (Facebook AI Similarity Search) â­ | â˜ | â˜ | x | Open-source ANN library, index types |
| Index types | â˜ | â˜ | x | Flat, IVF, HNSW, PQ trade-offs |
| Metadata filtering | â˜ | â˜ | x | Pre-filter vs post-filter strategies |
| Hybrid search in vector DBs | â˜ | â˜ | x | Combining vector + keyword + filters |
| Sharding & replication | â˜ | x | â˜ | Scaling vector search horizontally |
| Vector DB vs traditional DB | â˜ | â˜ | x | When to use which |

**Category 4 Score**: Know: _/7 | Unsure: _/7 | Dunno: _/7

---

## Category 5: Query Processing (8 topics)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Query rewriting / reformulation â­ | â˜ | x | â˜ | LLM-based query enhancement |
| Query decomposition | â˜ | x | â˜ | Multi-hop â†’ sub-questions (related to FiD) |
| Query expansion | â˜ | x | â˜ | Adding synonyms, related terms |
| HyDE (Hypothetical Document Embeddings) â­ | â˜ | â˜ | x | Generate hypothetical answer, embed that |
| Step-back prompting | â˜ | â˜ | x | Abstract question before retrieval |
| Query intent classification | â˜ | x | â˜ | Factual vs conversational vs multi-hop |
| Query understanding with LLMs | â˜ | x | â˜ | Extract entities, intent, constraints |
| Multi-query retrieval | â˜ | â˜ | x | Generate multiple query variants |

**Category 5 Score**: Know: _/8 | Unsure: _/8 | Dunno: _/8

---

## Category 6: Reranking Techniques (7 topics)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Cross-encoder reranking â­ | â˜ | â˜ | x | BERT-based query-document scoring |
| Two-stage retrieval (retrieve â†’ rerank) | â˜ | x | â˜ | Fast bi-encoder â†’ slow cross-encoder |
| Reranking with LLMs | â˜ | x | â˜ | Use LLM to score relevance |
| Maximal Marginal Relevance (MMR) â­ | â˜ | â˜ | x | Diversity-aware reranking |
| Cohere Rerank API | â˜ | â˜ | x | Commercial reranking service |
| Lost-in-the-middle problem â­ | â˜ | â˜ | x | Position bias in long contexts |
| Relevance score calibration | â˜ | â˜ | x | Converting scores to probabilities |

**Category 6 Score**: Know: _/7 | Unsure: _/7 | Dunno: _/7

---

## Category 7: Context Management (6 topics)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Context window optimization â­ | â˜ | â˜ | x | Fitting retrieved docs in 4K/8K/32K tokens |
| Context compression | â˜ | x | â˜ | Removing redundant information |
| Context ranking & selection | â˜ | â˜ | x | Top-K selection strategies |
| Long-context models (Claude, GPT-4) | â˜ | x | â˜ | 100K+ token windows |
| Context stuffing vs summarization | â˜ | â˜ | x | Trade-offs for long docs |
| Prompt template design | â˜ | x | â˜ | System + context + query structure |

**Category 7 Score**: Know: _/6 | Unsure: _/6 | Dunno: _/6

---

## Category 8: RAG Evaluation (9 topics)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Retrieval metrics â­ | â˜ | x | â˜ | Recall@K, Precision@K, MRR, NDCG |
| Generation metrics | â˜ | x | â˜ | BLEU, ROUGE, BERTScore |
| End-to-end RAG metrics â­ | â˜ | x | â˜ | Faithfulness, answer relevance |
| RAGAs framework | â˜ | â˜ | x | Automated RAG evaluation |
| Human evaluation protocols | â˜ | x | â˜ | Annotation guidelines, inter-rater reliability |
| Failure mode analysis | â˜ | x | â˜ | No relevant docs, wrong docs, hallucination |
| Latency vs quality trade-offs | â˜ | x | â˜ | P50/P95/P99 latency budgets |
| A/B testing RAG systems | â˜ | â˜ | x | Online evaluation strategies |
| Groundedness / Attribution | â˜ | x | â˜ | Verifying citations, source attribution |

**Category 8 Score**: Know: _/9 | Unsure: _/9 | Dunno: _/9

---

## Category 9: Production RAG Architecture (11 topics)

**Cross-reference with gap analysis**: Q177 (Fusion-in-Decoder - 0%), Q178 (Internet-augmented LLMs - 75%)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Standard RAG pipeline â­ | â˜ | x | â˜ | Query â†’ Retrieve â†’ Augment â†’ Generate |
| **Fusion-in-Decoder (FiD)** â­ | â˜ | â˜ | x | **Gap Q177 (0%)** - Encode passages independently, decode jointly. **2025 note**: Discuss FiD vs Long Context trade-offs! |
| Self-RAG | â˜ | â˜ | x | Model decides when to retrieve |
| Corrective RAG (CRAG) | â˜ | â˜ | x | Verify retrieval quality, re-retrieve if needed |
| Adaptive RAG | â˜ | â˜ | x | Dynamic retrieval strategy based on query |
| **RAFT (Retrieval Augmented Fine Tuning)** â­ | â˜ | â˜ | x | **2025 hot topic** - Fine-tune model to be better at RAG (ignore distractors) |
| **Agentic RAG** â­ | â˜ | â˜ | x | **2025 trend** - Autonomous agents, multi-step reasoning |
| **GraphRAG** â­ | â˜ | â˜ | x | **2025 trend** - Knowledge graph-based retrieval |
| Streaming RAG | â˜ | â˜ | x | Real-time document updates |
| Multi-modal RAG â­ | â˜ | x | â˜ | Text + images + tables |
| RAG with function calling | â˜ | â˜ | x | LLM calls APIs during generation |

**Category 9 Score**: Know: _/11 | Unsure: _/11 | Dunno: _/11

---

## Category 10: Advanced Patterns & Optimization (8 topics)

| Subtopic | Know | Unsure | Dunno | Notes |
|----------|------|--------|-------|-------|
| Multi-hop retrieval â­ | â˜ | â˜ | x | Iterative retrieval (retrieve â†’ reason â†’ retrieve) |
| **Long RAG** | â˜ | â˜ | x | **2025 trend** - Handling full documents vs chunks |
| Parent document retrieval â­ | â˜ | â˜ | x | Retrieve small chunks, return full context |
| Recursive retrieval | â˜ | â˜ | x | Hierarchical document structure |
| Query routing | â˜ | â˜ | x | Route to different indexes/strategies |
| Caching strategies | â˜ | â˜ | x | Query cache, embedding cache, result cache |
| Prompt compression | â˜ | â˜ | x | LongLLMLingua, AutoCompressors |
| Cold start for new documents | â˜ | â˜ | x | Indexing latency, temporary boosting |

**Category 10 Score**: Know: _/8 | Unsure: _/8 | Dunno: _/8

---

## High-Priority Subset (28 Topics)

These topics are marked with â­ above and represent interview-critical knowledge.

### Recommended Study Order (Attack in 4 phases)

**Phase 1: "The Fix" - Hybrid Retrieval Foundations** (Must master first)
1. **Hybrid retrieval (sparse + dense)** - **Gap Q179 (30%)**
2. **Reciprocal Rank Fusion (RRF)** - Must know formula: Score = 1/(k + rank)
3. Sparse retrieval (BM25, TF-IDF) - When dense fails (part numbers, acronyms)
4. Dense retrieval basics - When sparse fails (synonyms, semantic concepts)
5. **SPLADE (Learned Sparse)** - Modern bridge between sparse and dense
6. DPR (Dense Passage Retrieval) - Foundation for dense retrieval

**Phase 2: "The Polish" - Reranking & Retrieval Quality** (Shows practical expertise)
7. **Cross-encoder reranking** - Two-stage retrieval pattern
8. Late interaction models (ColBERT) - Token-level similarity
9. **MMR (Maximal Marginal Relevance)** - Diversity-aware reranking
10. **Lost-in-the-middle problem** - Position bias in long contexts
11. Sentence-BERT embeddings - Bi-encoder foundation
12. FAISS & vector DB basics - Production deployment

**Phase 3: "The System" - Advanced Patterns** (Shows seniority, 2025 relevance)
13. **GraphRAG** - **2025 hot trend** - Knowledge graph-based retrieval
14. **Agentic RAG** - **2025 hot trend** - Autonomous multi-step reasoning
15. **RAFT (Retrieval Augmented Fine Tuning)** - **2025 hot trend** - RAG + fine-tuning hybrid
16. **Fusion-in-Decoder (FiD)** - **Gap Q177 (0%)** + discuss Long Context trade-offs
17. Multi-hop retrieval - Iterative retrieve â†’ reason â†’ retrieve
18. Parent document retrieval - Retrieve small chunks, return full context
19. **Complex PDF parsing / OCR** - Real-world interview question: "How do you handle tables?"
20. Multi-modal RAG - Text + images + tables

**Phase 4: "The Proof" - Evaluation & Production** (Shows you can measure success)
21. **Retrieval metrics** - Recall@K, Precision@K, NDCG - Must know formulas
22. **End-to-end RAG metrics** - Faithfulness, answer relevance (RAGAs)
23. Context window optimization - Fitting docs in 4K/8K/32K tokens
24. Fixed-size & semantic chunking - Foundational preprocessing
25. Chunk overlap strategies - 10-20% overlap, sliding windows
26. Query rewriting / HyDE - Generate hypothetical answer, embed that
27. Standard RAG pipeline - Query â†’ Retrieve â†’ Augment â†’ Generate
28. Self-RAG / Corrective RAG - Model decides when to retrieve

**High-Priority Score**: Know: _/28 | Unsure: _/28 | Dunno: _/28

---

## Overall Summary Scorecard

| Category | Know | Unsure | Dunno | % Know |
|----------|------|--------|-------|--------|
| 1. Retrieval Techniques (11) | _/11 | _/11 | _/11 | ___% |
| 2. Chunking & Doc Processing (10) | _/10 | _/10 | _/10 | ___% |
| 3. Embedding Models (9) | _/9 | _/9 | _/9 | ___% |
| 4. Vector Databases (7) | _/7 | _/7 | _/7 | ___% |
| 5. Query Processing (8) | _/8 | _/8 | _/8 | ___% |
| 6. Reranking (7) | _/7 | _/7 | _/7 | ___% |
| 7. Context Management (6) | _/6 | _/6 | _/6 | ___% |
| 8. RAG Evaluation (9) | _/9 | _/9 | _/9 | ___% |
| 9. Production Architecture (11) | _/11 | _/11 | _/11 | ___% |
| 10. Advanced Patterns (8) | _/8 | _/8 | _/8 | ___% |
| **TOTAL (76 topics)** | **_/76** | **_/76** | **_/76** | **___%** |
| **High-Priority (28 topics)** | **_/28** | **_/28** | **_/28** | **___%** |

**Overall Readiness**: ___% (Target: 75% by end of Week 4)

---

## Gap Analysis Cross-Reference

### From Day 6 Gap Analysis (ML-Theory-Questions.md Q177-179)

| Question | Topic | Pre-Study Score | Notes |
|----------|-------|-----------------|-------|
| Q177 | Fusion-in-Decoder (FiD) | 0% (Dunno) | Critical gap - encode passages independently, decode jointly |
| Q178 | Internet-augmented LLMs | 75% (Mostly correct) | Understand retrieval + generation, need architecture details |
| Q179 | Hybrid retrieval | 30% (Conceptual only) | Know it combines sparse + dense, need RRF/fusion strategies |

**Target Post-Study Scores**: Q177: 80%+, Q178: 90%+, Q179: 85%+

---

## Post-Study Validation

**To be completed after Week 4 Day 1-2 study session**

### Re-Assessment Results

| Category | Pre-Study % Know | Post-Study % Know | Improvement |
|----------|------------------|-------------------|-------------|
| Overall (72 topics) | ___% | ___% | +___% |
| High-Priority (24) | ___% | ___% | +___% |
| Category 1: Retrieval | ___% | ___% | +___% |
| Category 9: Production | ___% | ___% | +___% |

**Gap Closure**:
- Q177 (FiD): 0% â†’ ___%
- Q178 (Internet-augmented): 75% â†’ ___%
- Q179 (Hybrid retrieval): 30% â†’ ___%

**Target Achievement**: â˜ Met 75% overall readiness | â˜ All high-priority topics â‰¥70%

---

## Recommended Study Resources

### Essential Papers
1. **DPR**: "Dense Passage Retrieval for Open-Domain Question Answering" (Karpukhin et al., 2020)
2. **FiD**: "Leveraging Passage Retrieval with Generative Models for Open Domain QA" (Izacard & Grave, 2021) â† **Gap Q177**
3. **SPLADE**: "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking" (Formal et al., 2021)
4. **ColBERT**: "Contextualized Late Interaction over BERT" (Khattab & Zaharia, 2020)
5. **Self-RAG**: "Self-RAG: Learning to Retrieve, Generate, and Critique" (Asai et al., 2023)
6. **RAFT**: "RAFT: Adapting Language Model to Domain Specific RAG" (Zhang et al., 2024)
7. **GraphRAG**: "From Local to Global: A Graph RAG Approach" (Microsoft, 2024)

### Key Blog Posts / Tutorials
- LangChain RAG from Scratch: Multi-query, RAG-Fusion, CRAG, Self-RAG
- Pinecone Learning Center: Retrieval, reranking, evaluation
- LlamaIndex guides: Production RAG patterns, evaluation frameworks
- RAGAs documentation: Automated evaluation metrics

### Hands-On Practice (Prioritized)
1. **Phase 1 Focus**: Implement hybrid retrieval (BM25 + dense embeddings + RRF formula)
2. **Phase 2 Focus**: Build two-stage retrieval (bi-encoder â†’ cross-encoder reranking)
3. **Phase 3 Focus**: Compare FiD vs Long Context for multi-document QA
4. **Phase 4 Focus**: Benchmark retrieval metrics (Recall@K, NDCG) on your implementation

### Key Interview Answers to Prepare
- **When does dense fail?** Exact part numbers, acronyms, proper nouns, new terms
- **When does sparse fail?** Synonyms, semantic concepts, paraphrasing
- **RRF formula on whiteboard**: Score = 1/(k + rank), typical k=60
- **FiD vs Long Context trade-offs**: FiD cheaper/faster but harder to implement vs Gemini 1.5 Pro 1M token window
- **Real-world PDF problem**: "How do you handle tables?" â†’ unstructured.io, LlamaParse, layout-aware chunking

---

## Notes

**Format Reference**: This topic check follows the structure of `Week2-LLM-Systems-Topic-Check.md` (76 subtopics, 83% post-study achievement)

**Success Pattern from Week 2**: Pre-study assessment (82% dunno) â†’ focused study on weak areas â†’ post-study validation (83% target achieved) â†’ Week 2 LLM Systems: 30% â†’ 85%+

**Expected Outcome**: Similar pattern for Week 4 RAG: 30% baseline â†’ identify critical gaps (especially FiD, Hybrid retrieval, GraphRAG, RAFT, SPLADE) â†’ focused 2-3 hour study following 4-phase priority order â†’ 75%+ interview readiness

**Update vs Original Version**: Added 4 topics based on 2025 trends and practical gaps:
1. SPLADE (learned sparse retrieval) - bridges sparse/dense
2. Propositional/context-aware chunking - LLM-based chunking
3. RAFT (Retrieval Augmented Fine Tuning) - hot 2025 topic
4. Complex PDF parsing / OCR - real-world interview question

Total: 72 â†’ 76 topics, High-Priority: 24 â†’ 28 topics
